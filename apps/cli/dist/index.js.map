{"version":3,"sources":["../src/index.ts","../package.json","../src/lib/prompts.ts","../src/commands/init.ts","../src/analyzer/orm-detectors/base-detector.ts","../src/core/utils/file-utils.ts","../src/core/utils/config.ts","../src/analyzer/orm-detectors/prisma-detector.ts","../src/analyzer/orm-detectors/drizzle-detector.ts","../src/analyzer/orm-detectors/typeorm-detector.ts","../src/commands/sync.ts","../src/lib/config.ts","../src/enhancer/parsers/sql-parser.ts","../src/enhancer/parsers/migration-parser.ts","../src/enhancer/risk-detector.ts","../src/enhancer/strategy-generator.ts","../src/enhancer/enhancement-engine.ts","../src/commands/test.ts","../src/commands/apply.ts","../src/commands/back.ts"],"sourcesContent":["/**\n * @driftjs/flow - Enhanced database migration CLI tool\n * Core entry point with beautiful CLI interactions\n */\n\nimport { Command } from 'commander'\nimport { intro, outro, isCancel, cancel, log } from '@clack/prompts'\nimport { version } from '../package.json'\nimport { initCommand } from './commands/init.js'\nimport { syncCommand } from './commands/sync.js'\nimport { testCommand } from './commands/test.js'\nimport { applyCommand } from './commands/apply.js'\nimport { backCommand } from './commands/back.js'\n\nconst handleCommand = async (commandPromise: Promise<any>) => {\n  try {\n    await commandPromise;\n  } catch (error) {\n    if (isCancel(error)) {\n      cancel('Operation cancelled.');\n      process.exit(0);\n    }\n    log.error(error instanceof Error ? error.message : 'An unknown error occurred.');\n    process.exit(1);\n  }\n};\n\nconst program = new Command()\n\nprogram\n  .name('flow')\n  .description('Enhanced database migration CLI tool for production-safety')\n  .version(version, '-v, --version', 'Output the current version')\n  .option('-d, --debug', 'Enable verbose logging')\n  .option('-c, --config <path>', 'Path to flow.config.json', './flow.config.json')\n  .option('--dry-run', 'Show what would be done without executing')\n\n// flow init - Initialize flow configuration\nprogram\n  .command('init')\n  .description('Initialize flow configuration in current project')\n  .option('--project <path>', 'Path to the project directory')\n  .option('--env-name <name>', 'Name for the initial environment', 'development')\n  .option('--db-url <url>', 'Database connection string')\n  .option('--migrations-path <path>', 'Path to the migrations folder')\n  .option('-y, --yes', 'Skip interactive prompts and use default or provided values')\n  .action(async (options) => {\n    intro('üåä DriftJS Flow - Initialize')\n    await handleCommand(initCommand(options, program.opts() as any));\n    outro('‚úÖ Flow configuration initialized successfully!')\n  })\n\n// flow sync\nprogram\n  .command('sync')\n  .description('Detect ORM changes and create enhanced migration plan')\n  .option('--force', 'Force re-analysis of existing migrations')\n  .option('--orm <name>', 'Specify ORM (prisma, drizzle, typeorm, auto)', 'auto')\n  .option('--project <path>', 'Path to the project directory')\n  .option('-y, --yes', 'Skip interactive prompts and use default or provided values')\n  .action(async (options) => {\n    intro('üåä DriftJS Flow - Sync')\n    await handleCommand(syncCommand(options, program.opts() as any));\n    outro('‚úÖ Sync completed')\n  })\n\n// flow apply\nprogram\n  .command('apply')\n  .description('Apply pending migrations to the database')\n  .option('--migration <name>', 'Apply a specific migration by name')\n  .option('--target <name>', 'Apply migrations up to and including the target migration')\n  .option('--project <path>', 'Path to the project directory')\n  .option('-y, --yes', 'Skip interactive prompts and use default or provided values')\n  .action(async (options) => {\n    intro('üåä DriftJS Flow - Apply')\n    await handleCommand(applyCommand(options, program.opts() as any));\n    outro('‚úÖ Apply completed')\n  })\n\n// flow back\nprogram\n  .command('back')\n  .description('Rollback the latest migration batch')\n  .option('--steps <n>', 'Number of migrations to rollback', '1')\n  .option('--to <name>', 'Rollback to a specific migration (exclusive)')\n  .option('--project <path>', 'Path to the project directory')\n  .option('-y, --yes', 'Skip interactive prompts and use default or provided values')\n  .action(async (options) => {\n    intro('üåä DriftJS Flow - Rollback')\n    // Convert steps to number\n    if (options.steps) {\n      options.steps = parseInt(options.steps, 10)\n    }\n    await handleCommand(backCommand(options, program.opts() as any));\n    outro('‚úÖ Rollback completed')\n  })\n\n// flow test (internal)\nprogram\n  .command('test')\n  .description('Run internal diagnostics')\n  .option('--project <path>', 'Path to the project directory')\n  .action(testCommand as any)\n\n// Execute the CLI\nprogram.parse() ","{\n  \"name\": \"@driftjs/flow\",\n  \"version\": \"1.0.0\",\n  \"description\": \"The complete all-in-one CLI for safe schema evolution. Flow wraps your ORM (Prisma, Drizzle, TypeORM) to automate zero-downtime migrations and prevent production risks. Ultra-fast analysis with enterprise-grade safety features.\",\n  \"main\": \"dist/index.js\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"flow\": \"./dist/index.js\"\n  },\n  \"scripts\": {\n    \"release\": \"bun run scripts/release.ts && bun publish\",\n    \"build\": \"tsup\",\n    \"dev\": \"tsup --watch\",\n    \"check-types\": \"tsc --noEmit\",\n    \"test\": \"jest\",\n    \"test:watch\": \"jest --watch\"\n  },\n  \"keywords\": [\n    \"database\",\n    \"migration\",\n    \"orm\",\n    \"prisma\",\n    \"drizzle\",\n    \"typeorm\",\n    \"cli\",\n    \"production-safety\"\n  ],\n  \"author\": \"DriftJS Team\",\n  \"license\": \"MIT\",\n  \"publishConfig\": {\n    \"access\": \"public\"\n  },\n  \"dependencies\": {\n    \"@clack/prompts\": \"^0.7.0\",\n    \"commander\": \"^12.0.0\",\n    \"diff\": \"^5.2.0\",\n    \"dotenv\": \"^16.4.5\",\n    \"execa\": \"^9.6.0\",\n    \"fs-extra\": \"^11.2.0\",\n    \"ms\": \"^2.1.3\",\n    \"node-sql-parser\": \"^5.0.0\",\n    \"picocolors\": \"^1.0.1\",\n    \"prompts\": \"^2.4.2\"\n  },\n  \"optionalDependencies\": {\n    \"better-sqlite3\": \"^9.4.0\",\n    \"mysql2\": \"^3.10.1\",\n    \"pg\": \"^8.12.0\",\n    \"sqlite3\": \"^5.1.7\"\n  },\n  \"devDependencies\": {\n    \"@types/better-sqlite3\": \"^7.6.9\",\n    \"@types/diff\": \"^5.2.1\",\n    \"@types/fs-extra\": \"^11.0.4\",\n    \"@types/jest\": \"^29.5.0\",\n    \"@types/ms\": \"^0.7.34\",\n    \"@types/pg\": \"^8.11.0\",\n    \"@types/sqlite3\": \"^5.1.0\",\n    \"jest\": \"^29.7.0\",\n    \"ts-jest\": \"^29.1.0\",\n    \"tsup\": \"^8.0.0\",\n    \"typescript\": \"^5.3.0\",\n    \"@types/bun\": \"latest\"\n  },\n  \"files\": [\n    \"dist\"\n  ],\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  }\n}\n","/**\n * Reusable prompt components for beautiful CLI interactions\n */\n\nimport { confirm, select, multiselect, text, spinner, log } from '@clack/prompts'\nimport colors from 'picocolors'\n\nexport interface EnhancementOption {\n  value: string\n  label: string\n  hint?: string\n  danger?: boolean\n}\n\n/**\n * Confirmation prompt with enhanced styling\n */\nexport async function confirmAction(\n  message: string,\n  options: any = {}\n): Promise<boolean> {\n  return await confirm({\n    message: colors.cyan(message),\n    ...options\n  }) as boolean\n}\n\n/**\n * Selection prompt for single choice\n */\nexport async function selectOption<T extends string>(\n  message: string,\n  options: Array<{ value: T; label: string; hint?: string }>,\n  selectOptions: any = {}\n): Promise<T> {\n  return await select({\n    message: colors.cyan(message),\n    options: options.map(opt => ({\n      value: opt.value,\n      label: opt.label,\n      hint: opt.hint ? colors.dim(opt.hint) : undefined\n    })),\n    ...selectOptions\n  }) as T\n}\n\n/**\n * Multi-selection prompt for multiple choices\n */\nexport async function selectMultiple<T extends string>(\n  message: string,\n  options: Array<{ value: T; label: string; hint?: string; required?: boolean }>,\n  multiselectOptions: any = {}\n): Promise<T[]> {\n  return await multiselect({\n    message: colors.cyan(message),\n    options: options.map(opt => ({\n      value: opt.value,\n      label: opt.label,\n      hint: opt.hint ? colors.dim(opt.hint) : undefined,\n      required: opt.required\n    })),\n    ...multiselectOptions\n  }) as T[]\n}\n\n/**\n * Text input prompt with validation\n */\nexport async function textInput(\n  message: string,\n  options: any = {}\n): Promise<string> {\n  return await text({\n    message: colors.cyan(message),\n    ...options\n  }) as string\n}\n\n/**\n * Enhancement selection flow\n */\nexport async function selectEnhancements(\n  availableEnhancements: EnhancementOption[]\n): Promise<string[]> {\n  log.info(colors.blue('üîç Available migration enhancements:'))\n  \n  const options = availableEnhancements.map(enhancement => ({\n    value: enhancement.value,\n    label: enhancement.danger \n      ? colors.red(`‚ö†Ô∏è  ${enhancement.label}`)\n      : colors.green(`‚úÖ ${enhancement.label}`),\n    hint: enhancement.hint,\n    required: false\n  }))\n\n  return await selectMultiple(\n    'Select enhancements to apply:',\n    options,\n    {\n      required: false\n    }\n  )\n}\n\n/**\n * Database connection confirmation\n */\nexport async function confirmDatabaseConnection(\n  connectionString: string\n): Promise<boolean> {\n  log.info(colors.yellow('üîå Database connection details:'))\n  log.info(colors.dim(`   ${connectionString}`))\n  \n  return await confirmAction(\n    'Proceed with this database connection?',\n    {\n      initialValue: false\n    }\n  )\n}\n\n/**\n * Migration risk confirmation\n */\nexport async function confirmHighRiskOperation(\n  operationName: string,\n  risks: string[]\n): Promise<boolean> {\n  log.warn(colors.red(`‚ö†Ô∏è  HIGH RISK OPERATION: ${operationName}`))\n  \n  risks.forEach(risk => {\n    log.warn(colors.red(`   ‚Ä¢ ${risk}`))\n  })\n  \n  return await confirmAction(\n    colors.red('Do you understand the risks and want to proceed?'),\n    {\n      initialValue: false\n    }\n  )\n}\n\n/**\n * Progress spinner with custom styling\n */\nexport function createSpinner(message: string) {\n  const s = spinner()\n  s.start(colors.blue(`üåä ${message}`))\n  \n  return {\n    update: (newMessage: string) => s.message(colors.blue(`üåä ${newMessage}`)),\n    succeed: (message?: string) => s.stop(colors.green(`‚úÖ ${message || 'Complete'}`)),\n    fail: (message?: string) => s.stop(colors.red(`‚ùå ${message || 'Failed'}`)),\n    stop: () => s.stop()\n  }\n} ","/**\n * flow init - Initialize flow configuration\n */\n\nimport { createSpinner, textInput, confirmAction } from '../lib/prompts.js'\nimport { GlobalOptions } from '../lib/config.js'\nimport fsExtra from 'fs-extra'\nimport { resolve } from 'node:path'\nimport dotenv from 'dotenv'\nimport { PrismaDetector, DrizzleDetector, TypeORMDetector } from '../analyzer/index.js'\n// @ts-ignore ‚Äì optional type import not strictly needed for compilation\n// import type { FlowConfig } from '../../core/index.js'\n\nexport interface InitOptions {\n  force?: boolean\n  envName?: string\n  dbUrl?: string\n  migrationsPath?: string\n  yes?: boolean\n  project?: string\n}\n\nasync function findDatabaseUrl(envName: string, projectPath: string): Promise<string> {\n  const candidateFiles: string[] = []\n  // 1) current directory .env\n  candidateFiles.push(resolve(projectPath, '.env'))\n  // 2) parent directories up to repo root\n  const parts = projectPath.split('/')\n  for (let i = parts.length - 1; i > 0; i--) {\n    candidateFiles.push(parts.slice(0, i + 1).join('/') + '/.env')\n  }\n  // 3) common monorepo locations (apps/*/.env and packages/*/.env)\n  const appsDir = resolve(projectPath, 'apps')\n  const pkgsDir = resolve(projectPath, 'packages')\n  if (await fsExtra.pathExists(appsDir)) {\n    const sub = await fsExtra.readdir(appsDir)\n    sub.forEach((s) => candidateFiles.push(resolve(appsDir, s, '.env')))\n  }\n  if (await fsExtra.pathExists(pkgsDir)) {\n    const sub = await fsExtra.readdir(pkgsDir)\n    sub.forEach((s) => candidateFiles.push(resolve(pkgsDir, s, '.env')))\n  }\n\n  for (const file of candidateFiles) {\n    if (await fsExtra.pathExists(file)) {\n      try {\n        const envVars = dotenv.parse(await fsExtra.readFile(file))\n        const v =\n          envVars.DATABASE_URL || envVars[`DATABASE_URL_${envName.toUpperCase()}`]\n        if (v) return v\n      } catch {}\n    }\n  }\n  return ''\n}\n\nasync function detectMigrationsDir(projectPath: string): Promise<string | null> {\n  const candidates = [\n    'migrations',\n    'db/migrations',\n    'drizzle/migrations',\n    'prisma/migrations',\n    'database/migrations'\n  ]\n  // Parse drizzle.config.* for out path\n  const drizzleConfigFiles = ['drizzle.config.ts', 'drizzle.config.js', 'drizzle.config.mjs', 'drizzle.config.cjs']\n  for (const f of drizzleConfigFiles) {\n    if (await fsExtra.pathExists(resolve(projectPath, f))) {\n      const content = await fsExtra.readFile(resolve(projectPath, f), 'utf8')\n      const match = content.match(/out\\s*:\\s*[\"'`](.+?)[\"'`]/)\n      if (match) {\n        candidates.unshift(match[1])\n      }\n    }\n  }\n  for (const rel of candidates) {\n    if (await fsExtra.pathExists(resolve(projectPath, rel))) return rel\n  }\n  return null\n}\n\nexport async function initCommand(options: InitOptions, globalOptions: GlobalOptions): Promise<void> {\n  const projectPath = resolve(options.project || process.cwd())\n  const spinner = createSpinner('Collecting project information')\n\n  let envName: string, databaseUrl: string, migrationsPath: string;\n\n  if (options.yes) {\n    // Non-interactive mode\n    envName = options.envName || 'development';\n    const detectedDb = await findDatabaseUrl(envName, projectPath);\n    databaseUrl = options.dbUrl || detectedDb;\n\n    if (!databaseUrl) {\n      spinner.fail('Database connection string is required. Please provide it with --db-url.');\n      throw new Error('FLOW_MISSING_DB_NON_INTERACTIVE');\n    }\n    \n    const detectedPath = await detectMigrationsDir(projectPath);\n    migrationsPath = options.migrationsPath || detectedPath || './migrations';\n\n  } else {\n    // Interactive mode\n    const envNameInput = await textInput('Environment name', {\n      placeholder: 'development',\n      defaultValue: 'development'\n    });\n    envName = envNameInput?.trim() || 'development';\n\n    const defaultDb = await findDatabaseUrl(envName, projectPath);\n    const dbPrompt = 'Database connection string (e.g. postgresql://user:pass@localhost:5432/db)';\n    const dbInput = await textInput(dbPrompt, {\n      placeholder: defaultDb || 'postgresql://user:pass@localhost:5432/db',\n      defaultValue: defaultDb\n    });\n    databaseUrl = (dbInput?.trim() || defaultDb).trim();\n\n    if (!databaseUrl) {\n      spinner.fail('Database connection string is required');\n      throw new Error('FLOW_MISSING_DB');\n    }\n\n    const detectedPath = (await detectMigrationsDir(projectPath)) || './migrations';\n    const migInput = await textInput('Path to migrations folder', {\n      placeholder: detectedPath,\n      defaultValue: detectedPath\n    });\n    migrationsPath = migInput?.trim() || detectedPath;\n\n    const proceed = await confirmAction('Generate configuration with these values?');\n    if (!proceed) {\n      spinner.fail('User cancelled');\n      return;\n    }\n  }\n\n  spinner.update('Generating flow.config')\n\n  // Detect ORM\n  const detectors = [\n    { name: 'prisma', detector: new PrismaDetector() },\n    { name: 'drizzle', detector: new DrizzleDetector() },\n    { name: 'typeorm', detector: new TypeORMDetector() }\n  ]\n  let detectedORM: string | null = null;\n  for (const { name, detector } of detectors) {\n    const result = await detector.detect(projectPath)\n    if (result.found) {\n      detectedORM = name\n      break\n    }\n  }\n\n  // Build config object\n  const config: any = {\n    version: '0.1.0',\n    defaultEnvironment: envName,\n    ...(detectedORM && { orm: detectedORM }),\n    environments: {\n      [envName]: {\n        db_connection_string: databaseUrl,\n        migrationsPath: migrationsPath\n      }\n    },\n    safety: {\n      maxTableSizeMB: 1024,\n      maxLockTimeMs: 300_000\n    }\n  }\n\n  const configPath = resolve(projectPath, globalOptions.config || 'flow.config.json')\n\n  if (await fsExtra.pathExists(configPath) && !options.yes && !(await confirmAction(`Overwrite existing ${configPath}?`))) {\n    spinner.fail('Init aborted ‚Äì config exists')\n    return\n  }\n\n  if (!globalOptions.dryRun) {\n    await fsExtra.writeFile(configPath, JSON.stringify(config, null, 2))\n    spinner.succeed(`Configuration written to ${configPath}`)\n  } else {\n    spinner.succeed('Dry run complete ‚Äì configuration would be:')\n    console.log(JSON.stringify(config, null, 2))\n  }\n\n  // --- NEW: ensure package.json has a \"flow\" script for easy execution\n  try {\n    const pkgPath = resolve(projectPath, 'package.json')\n    if (await fsExtra.pathExists(pkgPath)) {\n      // Dynamic import to avoid increasing cold start\n      const fsmod: any = await import('fs-extra')\n      const fsDyn = (fsmod.default ?? fsmod) as typeof fsExtra\n      const pkg = await fsDyn.readJson(pkgPath)\n      pkg.scripts = pkg.scripts || {}\n      if (!pkg.scripts.flow) {\n        pkg.scripts.flow = 'flow'\n        await fsDyn.writeJson(pkgPath, pkg, { spaces: 2 })\n        spinner.update('Added \"flow\" script to package.json')\n      }\n    }\n  } catch (err) {\n    // Non-fatal; emit warning but continue\n    console.warn('‚ö†Ô∏è  Could not update package.json:', err instanceof Error ? err.message : err)\n  }\n} ","/**\n * Base ORM detector with common functionality\n */\n\nimport { join } from 'path'\nimport { ORMDetector, ORMConfig } from '../../core/index.js'\nimport { DatabaseConfig, DetectionResult, FilePath } from '../../core/index.js'\nimport { exists, createFilePath, readJsonFile, findFiles } from '../../core/index.js'\n\nexport abstract class BaseORMDetector implements ORMDetector {\n  abstract name: string\n  \n  /**\n   * Detect if this ORM is present in the project\n   */\n  abstract detect(projectPath: string): Promise<DetectionResult>\n  \n  /**\n   * Extract ORM-specific configuration\n   */\n  abstract extractConfig(projectPath: string): Promise<ORMConfig | null>\n  \n  /**\n   * Extract database configuration from ORM setup\n   */\n  abstract getDatabaseConfig(projectPath: string): Promise<DatabaseConfig | null>\n  \n  /**\n   * Common helper: Check if package.json contains specific dependencies\n   */\n  protected async checkPackageJsonDependencies(\n    projectPath: string,\n    dependencies: string[]\n  ): Promise<{ found: string[]; missing: string[] }> {\n    const packageJsonPath = join(projectPath, 'package.json')\n    const packageResult = await readJsonFile<{ dependencies?: Record<string, string>; devDependencies?: Record<string, string> }>(packageJsonPath)\n    \n    if (!packageResult.success) {\n      return { found: [], missing: dependencies }\n    }\n    \n    const allDeps = {\n      ...packageResult.data.dependencies,\n      ...packageResult.data.devDependencies\n    }\n    \n    const found = dependencies.filter(dep => dep in allDeps)\n    const missing = dependencies.filter(dep => !(dep in allDeps))\n    \n    return { found, missing }\n  }\n  \n  /**\n   * Common helper: Check if specific files exist\n   */\n  protected async checkFiles(\n    projectPath: string,\n    filePaths: string[]\n  ): Promise<{ existing: FilePath[]; missing: string[] }> {\n    const existing: FilePath[] = []\n    const missing: string[] = []\n    \n    for (const filePath of filePaths) {\n      const fullPath = join(projectPath, filePath)\n      const fileExists = await exists(fullPath)\n      \n      if (fileExists) {\n        existing.push(await createFilePath(filePath, projectPath))\n      } else {\n        missing.push(filePath)\n      }\n    }\n    \n    return { existing, missing }\n  }\n  \n  /**\n   * Common helper: Find files matching patterns\n   */\n  protected async findFilesByPattern(\n    projectPath: string,\n    patterns: RegExp[],\n    directories: string[] = ['.']\n  ): Promise<string[]> {\n    const allFiles: string[] = []\n    \n    for (const directory of directories) {\n      const fullDirectory = join(projectPath, directory)\n      \n      for (const pattern of patterns) {\n        const files = await findFiles(fullDirectory, pattern, true)\n        allFiles.push(...files)\n      }\n    }\n    \n    return allFiles\n  }\n  \n  /**\n   * Common helper: Calculate confidence score based on evidence\n   */\n  protected calculateConfidence(evidence: {\n    required: { found: number; total: number }\n    optional: { found: number; total: number }\n    negative: number\n  }): number {\n    if (evidence.required.total === 0) {\n      return 0\n    }\n    \n    const requiredScore = evidence.required.found / evidence.required.total\n    const optionalScore = evidence.optional.total > 0 \n      ? evidence.optional.found / evidence.optional.total \n      : 0\n    \n    // Base score from required items (70% weight)\n    const baseScore = requiredScore * 0.7\n    \n    // Bonus from optional items (30% weight)\n    const bonusScore = optionalScore * 0.3\n    \n    // Penalty for negative evidence\n    const penalty = Math.min(evidence.negative * 0.1, 0.5)\n    \n    return Math.max(0, Math.min(1, baseScore + bonusScore - penalty))\n  }\n  \n  /**\n   * Parse database URL into DatabaseConfig\n   */\n  protected parseDatabaseUrl(url: string): DatabaseConfig | null {\n    try {\n      const parsed = new URL(url)\n      \n      let type: DatabaseConfig['type']\n      \n      switch (parsed.protocol) {\n        case 'postgresql:':\n        case 'postgres:':\n          type = 'postgresql'\n          break\n        case 'mysql:':\n          type = 'mysql'\n          break\n        case 'sqlite:':\n          type = 'sqlite'\n          break\n        default:\n          return null\n      }\n      \n      return {\n        type,\n        host: parsed.hostname || undefined,\n        port: parsed.port ? parseInt(parsed.port) : undefined,\n        database: parsed.pathname.slice(1), // Remove leading slash\n        username: parsed.username || undefined,\n        password: parsed.password || undefined,\n        url\n      }\n    } catch {\n      return null\n    }\n  }\n} ","/**\n * File system utilities for DriftJS\n */\n\nimport fs from 'fs-extra'\nconst { readFile, writeFile, access, stat, readdir } = fs\nimport { join, resolve, relative, dirname } from 'path'\nimport { FilePath, Result } from '../types/common.js'\n\n/**\n * Check if a file or directory exists\n */\nexport async function exists(path: string): Promise<boolean> {\n  try {\n    await access(path)\n    return true\n  } catch {\n    return false\n  }\n}\n\n/**\n * Create a FilePath object with absolute and relative paths\n */\nexport async function createFilePath(path: string, basePath: string = process.cwd()): Promise<FilePath> {\n  const absolutePath = resolve(basePath, path)\n  const relativePath = relative(basePath, absolutePath)\n  const fileExists = await exists(absolutePath)\n  \n  return {\n    absolute: absolutePath,\n    relative: relativePath,\n    exists: fileExists\n  }\n}\n\n/**\n * Read file content with error handling\n */\nexport async function readFileContent(path: string): Promise<Result<string>> {\n  try {\n    const content = await readFile(path, 'utf-8')\n    return { success: true, data: content }\n  } catch (error) {\n    return { \n      success: false, \n      error: error instanceof Error ? error : new Error('Unknown error reading file')\n    }\n  }\n}\n\n/**\n * Write file content with error handling\n */\nexport async function writeFileContent(path: string, content: string): Promise<Result<void>> {\n  try {\n    await writeFile(path, content, 'utf-8')\n    return { success: true, data: undefined }\n  } catch (error) {\n    return { \n      success: false, \n      error: error instanceof Error ? error : new Error('Unknown error writing file')\n    }\n  }\n}\n\n/**\n * Get file statistics\n */\nexport async function getFileStats(path: string): Promise<Result<{ size: number; modified: Date; isDirectory: boolean }>> {\n  try {\n    const stats = await stat(path)\n    return {\n      success: true,\n      data: {\n        size: stats.size,\n        modified: stats.mtime,\n        isDirectory: stats.isDirectory()\n      }\n    }\n  } catch (error) {\n    return {\n      success: false,\n      error: error instanceof Error ? error : new Error('Unknown error getting file stats')\n    }\n  }\n}\n\n/**\n * Find files matching a pattern in a directory\n */\nexport async function findFiles(\n  directory: string,\n  pattern: RegExp,\n  recursive: boolean = true\n): Promise<string[]> {\n  const files: string[] = []\n  \n  try {\n    const items = await readdir(directory, { withFileTypes: true })\n    \n    for (const item of items) {\n      const fullPath = join(directory, item.name)\n      \n      if (item.isDirectory() && recursive) {\n        const subFiles = await findFiles(fullPath, pattern, recursive)\n        files.push(...subFiles)\n      } else if (item.isFile() && pattern.test(item.name)) {\n        files.push(fullPath)\n      }\n    }\n  } catch {\n    // Directory doesn't exist or can't be read\n  }\n  \n  return files\n}\n\n/**\n * Check if a directory contains any files matching a pattern\n */\nexport async function hasFilesMatching(directory: string, pattern: RegExp): Promise<boolean> {\n  const files = await findFiles(directory, pattern, false)\n  return files.length > 0\n}\n\n/**\n * Parse JSON file with error handling\n */\nexport async function readJsonFile<T = any>(path: string): Promise<Result<T>> {\n  const fileResult = await readFileContent(path)\n  \n  if (!fileResult.success) {\n    return fileResult\n  }\n  \n  try {\n    const data = JSON.parse(fileResult.data)\n    return { success: true, data }\n  } catch (error) {\n    return {\n      success: false,\n      error: new Error(`Invalid JSON in file ${path}: ${error instanceof Error ? error.message : 'Unknown error'}`)\n    }\n  }\n} ","import fs from 'fs-extra'\nimport path from 'node:path'\nimport { FlowConfig } from '../types/config.js'\nimport { validateDatabaseConfig } from './validation.js'\n\n/**\n * Default file names that Drift Flow will look for when no --config flag is passed.\n */\nconst DEFAULT_CONFIG_FILES = [\n  'flow.config.json',\n  'flow.config.js',\n  'flow.config.cjs',\n  'flow.config.mjs',\n  'flow.config.ts',\n]\n\n/**\n * Load the first configuration file found in the current working directory hierarchy.\n * @param cwd Directory to start the search from (defaults to process.cwd())\n * @param explicitPath Optional explicit path passed by CLI flag\n */\nexport async function loadFlowConfig(\n  cwd: string = process.cwd(),\n  explicitPath?: string,\n): Promise<FlowConfig> {\n  let configPath: string | undefined = explicitPath\n\n  if (!configPath) {\n    configPath = await findConfigFile(cwd)\n    if (!configPath) {\n      throw new Error('Unable to locate flow.config file in current directory tree.')\n    }\n  }\n\n  const ext = path.extname(configPath)\n  let raw: unknown\n\n  if (ext === '.json') {\n    raw = await fs.readJSON(configPath)\n  } else if (ext === '.js' || ext === '.cjs' || ext === '.mjs') {\n    // eslint-disable-next-line import/no-dynamic-require, @typescript-eslint/no-var-requires\n    raw = await import(configPath)\n    raw = (raw as any).default ?? raw\n  } else if (ext === '.ts') {\n    throw new Error('Loading TypeScript config files is not yet supported. Please use JSON or JavaScript.')\n  } else {\n    throw new Error(`Unsupported config file extension: ${ext}`)\n  }\n\n  const validated = validateFlowConfig(raw)\n  if (!validated.valid) {\n    throw new Error(`Invalid flow.config: \\n${validated.errors.join('\\n')}`)\n  }\n\n  return validated.config!\n}\n\n/** Find the nearest config file walking up the directory tree */\nasync function findConfigFile(startDir: string): Promise<string | undefined> {\n  let dir = startDir\n  while (path.dirname(dir) !== dir) {\n    for (const file of DEFAULT_CONFIG_FILES) {\n      const candidate = path.join(dir, file)\n      if (await fs.pathExists(candidate)) {\n        return candidate\n      }\n    }\n    dir = path.dirname(dir)\n  }\n  return undefined\n}\n\nexport interface ValidationResult {\n  valid: boolean\n  errors: string[]\n  config?: FlowConfig\n}\n\n/**\n * Perform structural validation of FlowConfig instance.\n * We purposefully avoid pulling in a JSON-schema validator to keep deps light.\n */\nexport function validateFlowConfig(input: unknown): ValidationResult {\n  if (typeof input !== 'object' || input === null) {\n    return { valid: false, errors: ['Configuration must be an object'] }\n  }\n  const cfg = input as Partial<FlowConfig>\n  const errors: string[] = []\n\n  if (!cfg.environments || Object.keys(cfg.environments).length === 0) {\n    errors.push('`environments` section is required and cannot be empty')\n  }\n  if (!cfg.defaultEnvironment) {\n    errors.push('`defaultEnvironment` is required')\n  } else if (cfg.environments && !(cfg.defaultEnvironment in cfg.environments)) {\n    errors.push(`defaultEnvironment \\`${cfg.defaultEnvironment}\\` not found in environments section`)\n  }\n\n  // Validate each environment\n  if (cfg.environments) {\n    for (const [envName, envCfg] of Object.entries(cfg.environments)) {\n      if (!envCfg.databaseUrl) {\n        errors.push(`environment[${envName}].databaseUrl is required`)\n      }\n      if (envCfg.migrationsPath && typeof envCfg.migrationsPath !== 'string') {\n        errors.push(`environment[${envName}].migrationsPath must be a string`)\n      }\n      if (envCfg.migrationsPath) {\n        const absPath = path.isAbsolute(envCfg.migrationsPath)\n          ? envCfg.migrationsPath\n          : path.join(process.cwd(), envCfg.migrationsPath)\n        if (!(fs.existsSync(absPath))) {\n          errors.push(`environment[${envName}].migrationsPath '${envCfg.migrationsPath}' does not exist`)\n        }\n      }\n      // pattern toggles validation\n      if (envCfg.patterns) {\n        if (typeof envCfg.patterns !== 'object') {\n          errors.push(`environment[${envName}].patterns must be an object`)\n        }\n      }\n    }\n  }\n\n  // Validate safety thresholds\n  if (cfg.safety) {\n    if (cfg.safety.maxLockTimeMs && cfg.safety.maxLockTimeMs < 0) {\n      errors.push('safety.maxLockTimeMs must be positive')\n    }\n    if (cfg.safety.maxTableSizeMB && cfg.safety.maxTableSizeMB < 0) {\n      errors.push('safety.maxTableSizeMB must be positive')\n    }\n  }\n\n  // Validate database optimisation settings\n  if (cfg.database) {\n    for (const [name, dbCfg] of Object.entries(cfg.database)) {\n      const { valid, errors: dbErrors } = validateDatabaseConfig(dbCfg as any)\n      if (!valid) {\n        errors.push(...dbErrors.map((e) => `database[${name}]: ${e}`))\n      }\n    }\n  }\n\n  return { valid: errors.length === 0, errors, config: cfg as FlowConfig }\n} ","/**\n * Prisma ORM detector implementation\n */\n\nimport { join } from 'path'\nimport { BaseORMDetector } from './base-detector.js'\nimport { PrismaConfig } from '../../core/index.js'\nimport { DatabaseConfig, DetectionResult } from '../../core/index.js'\nimport { readFileContent, createFilePath } from '../../core/index.js'\n\nexport class PrismaDetector extends BaseORMDetector {\n  name = 'prisma'\n  \n  /**\n   * Detect Prisma in the project\n   */\n  async detect(projectPath: string): Promise<DetectionResult> {\n    const evidence: string[] = []\n    const warnings: string[] = []\n    \n    // Check for Prisma dependencies\n    const { found: foundDeps, missing: missingDeps } = await this.checkPackageJsonDependencies(\n      projectPath,\n      ['prisma', '@prisma/client']\n    )\n    \n    evidence.push(...foundDeps.map(dep => `Found dependency: ${dep}`))\n    \n    // Check for Prisma schema file\n    const { existing: schemaFiles } = await this.checkFiles(projectPath, [\n      'prisma/schema.prisma',\n      'schema.prisma'\n    ])\n    \n    if (schemaFiles.length > 0) {\n      evidence.push(`Found schema file: ${schemaFiles[0].relative}`)\n    }\n    \n    // Check for migration directory\n    const { existing: migrationDirs } = await this.checkFiles(projectPath, [\n      'prisma/migrations'\n    ])\n    \n    if (migrationDirs.length > 0) {\n      evidence.push(`Found migrations directory: ${migrationDirs[0].relative}`)\n    }\n    \n    // Look for generated Prisma client\n    const generatedFiles = await this.findFilesByPattern(\n      projectPath,\n      [/node_modules\\/@prisma\\/client/],\n      ['node_modules']\n    )\n    \n    if (generatedFiles.length > 0) {\n      evidence.push('Found generated Prisma client')\n    }\n    \n    // Calculate confidence\n    const confidence = this.calculateConfidence({\n      required: { found: foundDeps.length, total: 2 }, // prisma + @prisma/client\n      optional: { found: schemaFiles.length + migrationDirs.length, total: 2 },\n      negative: 0\n    })\n    \n    // Add warnings for incomplete setup\n    if (foundDeps.length > 0 && schemaFiles.length === 0) {\n      warnings.push('Prisma dependency found but no schema.prisma file detected')\n    }\n    \n    if (schemaFiles.length > 0 && !foundDeps.includes('@prisma/client')) {\n      warnings.push('Schema file found but @prisma/client not installed')\n    }\n    \n    return {\n      found: confidence > 0.5,\n      confidence,\n      evidence,\n      warnings: warnings.length > 0 ? warnings : undefined\n    }\n  }\n  \n  /**\n   * Extract Prisma configuration\n   */\n  async extractConfig(projectPath: string): Promise<PrismaConfig | null> {\n    // Find schema file\n    const { existing: schemaFiles } = await this.checkFiles(projectPath, [\n      'prisma/schema.prisma',\n      'schema.prisma'\n    ])\n    \n    if (schemaFiles.length === 0) {\n      return null\n    }\n    \n    const schemaFile = schemaFiles[0]\n    const migrationDirectory = await createFilePath('prisma/migrations', projectPath)\n    \n    // Parse schema file for generator and database info\n    const schemaResult = await readFileContent(schemaFile.absolute)\n    if (!schemaResult.success) {\n      return null\n    }\n    \n    // Extract client generator config\n    let clientGenerator: PrismaConfig['clientGenerator']\n    const generatorMatch = schemaResult.data.match(/generator\\s+client\\s*{([^}]+)}/s)\n    if (generatorMatch) {\n      const generatorConfig = generatorMatch[1]\n      const providerMatch = generatorConfig.match(/provider\\s*=\\s*\"([^\"]+)\"/)\n      const outputMatch = generatorConfig.match(/output\\s*=\\s*\"([^\"]+)\"/)\n      \n      clientGenerator = {\n        provider: providerMatch?.[1] || 'prisma-client-js',\n        output: outputMatch?.[1]\n      }\n    }\n    \n    return {\n      type: 'prisma',\n      configFile: schemaFile,\n      migrationDirectory,\n      schemaFile,\n      dependencies: ['prisma', '@prisma/client'],\n      clientGenerator\n    }\n  }\n  \n  /**\n   * Extract database configuration from Prisma schema\n   */\n  async getDatabaseConfig(projectPath: string): Promise<DatabaseConfig | null> {\n    const config = await this.extractConfig(projectPath)\n    if (!config) {\n      return null\n    }\n    \n    // Read schema file\n    const schemaResult = await readFileContent(config.schemaFile.absolute)\n    if (!schemaResult.success) {\n      return null\n    }\n    \n    // Parse datasource block\n    const datasourceMatch = schemaResult.data.match(/datasource\\s+\\w+\\s*{([^}]+)}/s)\n    if (!datasourceMatch) {\n      return null\n    }\n    \n    const datasourceConfig = datasourceMatch[1]\n    \n    // Extract provider\n    const providerMatch = datasourceConfig.match(/provider\\s*=\\s*\"([^\"]+)\"/)\n    const provider = providerMatch?.[1]\n    \n    // Extract URL\n    const urlMatch = datasourceConfig.match(/url\\s*=\\s*env\\(\"([^\"]+)\"\\)/) ||\n                    datasourceConfig.match(/url\\s*=\\s*\"([^\"]+)\"/)\n    \n    if (!urlMatch) {\n      return null\n    }\n    \n    let databaseUrl: string\n    if (urlMatch[0].includes('env(')) {\n      // Environment variable reference\n      const envVar = urlMatch[1]\n      databaseUrl = process.env[envVar] || ''\n      \n      if (!databaseUrl) {\n        // Return basic config with type only\n        return {\n          type: this.mapPrismaProviderToType(provider),\n          database: 'unknown'\n        }\n      }\n    } else {\n      // Direct URL\n      databaseUrl = urlMatch[1]\n    }\n    \n    // Parse the database URL\n    const dbConfig = this.parseDatabaseUrl(databaseUrl)\n    if (dbConfig) {\n      return dbConfig\n    }\n    \n    // Fallback to provider-based detection\n    return {\n      type: this.mapPrismaProviderToType(provider),\n      database: 'unknown'\n    }\n  }\n  \n  /**\n   * Map Prisma provider to database type\n   */\n  private mapPrismaProviderToType(provider?: string): DatabaseConfig['type'] {\n    switch (provider) {\n      case 'postgresql':\n        return 'postgresql'\n      case 'mysql':\n        return 'mysql'\n      case 'sqlite':\n        return 'sqlite'\n      default:\n        return 'postgresql' // Default assumption\n    }\n  }\n} ","/**\n * Drizzle ORM detector implementation\n * Detects Drizzle projects by looking for drizzle.config.ts/js files and schema patterns\n */\n\nimport { BaseORMDetector } from './base-detector.js'\nimport { DrizzleConfig } from '../../core/index.js'\nimport { DatabaseConfig, DetectionResult } from '../../core/index.js'\nimport path from 'path'\nimport fs from 'fs-extra'\n\nexport class DrizzleDetector extends BaseORMDetector {\n  name = 'drizzle'\n  \n  async detect(projectPath: string): Promise<DetectionResult> {\n    const evidence: string[] = []\n    \n    try {\n      // Check for drizzle.config.ts/js files\n      const configFiles = [\n        'drizzle.config.ts',\n        'drizzle.config.js',\n        'drizzle.config.mjs'\n      ]\n      \n             const { existing: configFilesFound } = await this.checkFiles(projectPath, configFiles)\n       evidence.push(...configFilesFound.map(f => `Found config file: ${f.relative}`))\n       \n       // Check for package.json with drizzle dependencies\n       const deps = await this.checkPackageJsonDependencies(projectPath, ['drizzle-orm', 'drizzle-kit'])\n       evidence.push(...deps.found.map(dep => `Found dependency: ${dep}`))\n       \n       // Check for common schema file patterns\n       const schemaPatterns = [\n         'src/db/schema.ts',\n         'src/schema.ts',\n         'db/schema.ts',\n         'schema.ts',\n         'src/lib/db/schema.ts'\n       ]\n       \n       const { existing: schemaFiles } = await this.checkFiles(projectPath, schemaPatterns)\n       evidence.push(...schemaFiles.map(f => `Found schema file: ${f.relative}`))\n       \n       // Check for migrations directory\n       const migrationDirs = ['drizzle', 'migrations', 'drizzle/migrations']\n       const { existing: migrationDirsFound } = await this.checkFiles(projectPath, migrationDirs)\n       evidence.push(...migrationDirsFound.map(f => `Found migration directory: ${f.relative}`))\n      \n      // Calculate confidence using helper\n      const confidence = this.calculateConfidence({\n        required: { \n          found: deps.found.length > 0 ? 1 : 0, \n          total: 1 \n        },\n        optional: { \n          found: configFilesFound.length + schemaFiles.length + migrationDirsFound.length, \n          total: configFiles.length + schemaPatterns.length + migrationDirs.length \n        },\n        negative: 0\n      })\n      \n      return {\n        found: confidence > 0.3,\n        confidence: Math.round(confidence * 100),\n        evidence\n      }\n    } catch (error) {\n      return {\n        found: false,\n        confidence: 0,\n        evidence: [`Error detecting Drizzle: ${error}`]\n      }\n    }\n  }\n  \n  async extractConfig(projectPath: string): Promise<DrizzleConfig | null> {\n    try {\n      // Try to find drizzle config file\n      const configFiles = [\n        'drizzle.config.ts',\n        'drizzle.config.js',\n        'drizzle.config.mjs'\n      ]\n      \n      const { existing: configFilesFound } = await this.checkFiles(projectPath, configFiles)\n      if (configFilesFound.length === 0) {\n        return null\n      }\n      \n      const configFile = configFilesFound[0]\n      const configContent = await fs.readFile(configFile.absolute, 'utf-8')\n      \n      // Basic config parsing - in production, we'd use a proper TS/JS parser\n      const driver = this.extractConfigValue(configContent, 'dialect') || 'pg'\n      const validDrivers = ['pg', 'mysql2', 'better-sqlite3', 'sqlite'] as const\n      const mappedDriver = validDrivers.includes(driver as any) ? driver as typeof validDrivers[number] : 'pg'\n      const outDir = this.extractConfigValue(configContent, 'out') || './drizzle'\n      const migrationDirAbsolute = path.resolve(projectPath, outDir)\n      \n      const config: DrizzleConfig = {\n        type: 'drizzle',\n        configFile,\n        driver: mappedDriver,\n        schemaPath: this.extractConfigValue(configContent, 'schema') || './src/db/schema.ts',\n        outDir: outDir,\n        migrationDirectory: {\n          absolute: migrationDirAbsolute,\n          relative: outDir,\n          exists: await fs.pathExists(migrationDirAbsolute)\n        },\n        dependencies: ['drizzle-orm', 'drizzle-kit']\n      }\n      \n      return config\n    } catch (error) {\n      console.warn(`Failed to extract Drizzle config: ${error}`)\n      return null\n    }\n  }\n  \n  async getDatabaseConfig(projectPath: string): Promise<DatabaseConfig | null> {\n    try {\n      // Look for database URL in various places\n      const envFiles = ['.env', '.env.local', '.env.development']\n      const { existing: envFilesFound } = await this.checkFiles(projectPath, envFiles)\n      \n      for (const envFile of envFilesFound) {\n        const envContent = await fs.readFile(envFile.absolute, 'utf-8')\n        const dbUrl = this.extractEnvValue(envContent, 'DATABASE_URL')\n        if (dbUrl) {\n          const parsed = this.parseDatabaseUrl(dbUrl)\n          if (parsed) return parsed\n        }\n      }\n      \n      // Fallback: try to extract from drizzle config\n      const drizzleConfig = await this.extractConfig(projectPath)\n      if (!drizzleConfig) return null\n      \n      // Map Drizzle driver to database type\n      const driverMap: Record<string, 'postgresql' | 'mysql' | 'sqlite'> = {\n        'pg': 'postgresql',\n        'mysql2': 'mysql',\n        'better-sqlite3': 'sqlite'\n      }\n      \n      const dbType = driverMap[drizzleConfig.driver] || 'postgresql'\n      \n      return {\n        type: dbType,\n        host: 'localhost',\n        port: dbType === 'postgresql' ? 5432 : dbType === 'mysql' ? 3306 : undefined,\n        database: 'main'\n      }\n    } catch (error) {\n      console.warn(`Failed to extract database config: ${error}`)\n      return null\n    }\n  }\n  \n  private extractConfigValue(content: string, key: string): string | undefined {\n    // Simple regex-based extraction - in production, use proper AST parsing\n    const regex = new RegExp(`${key}:\\\\s*['\"]([^'\"]+)['\"]`)\n    const match = content.match(regex)\n    return match?.[1]\n  }\n  \n  private extractEnvValue(content: string, key: string): string | undefined {\n    const regex = new RegExp(`^${key}\\\\s*=\\\\s*(.+)$`, 'm')\n    const match = content.match(regex)\n    return match?.[1]?.replace(/['\"]/g, '').trim()\n  }\n} ","/**\n * TypeORM detector implementation\n * Detects TypeORM projects by looking for ormconfig files, entity patterns, and migrations\n */\n\nimport { BaseORMDetector } from './base-detector.js'\nimport { TypeORMConfig } from '../../core/index.js'\nimport { DatabaseConfig, DetectionResult } from '../../core/index.js'\nimport path from 'path'\nimport fs from 'fs/promises'\n\nexport class TypeORMDetector extends BaseORMDetector {\n  name = 'typeorm'\n  \n  async detect(projectPath: string): Promise<DetectionResult> {\n    const evidence: string[] = []\n    \n    try {\n      // Check for TypeORM config files\n      const configFiles = [\n        'ormconfig.ts',\n        'ormconfig.js',\n        'ormconfig.json',\n        'typeorm.config.ts',\n        'typeorm.config.js',\n        'src/data-source.ts',\n        'src/data-source.js'\n      ]\n      \n      const { existing: configFilesFound } = await this.checkFiles(projectPath, configFiles)\n      evidence.push(...configFilesFound.map(f => `Found config file: ${f.relative}`))\n      \n      // Check for package.json with TypeORM dependencies\n      const deps = await this.checkPackageJsonDependencies(projectPath, ['typeorm', '@nestjs/typeorm'])\n      evidence.push(...deps.found.map(dep => `Found dependency: ${dep}`))\n      \n      // Check for common entity file patterns\n      const entityPatterns = await this.findFilesByPattern(\n        projectPath,\n        [/\\.entity\\.(ts|js)$/, /@Entity\\(/],\n        ['src', 'entities', 'entity']\n      )\n      if (entityPatterns.length > 0) {\n        evidence.push(`Found ${entityPatterns.length} entity files`)\n      }\n      \n      // Check for migrations directory\n      const migrationDirs = ['src/migrations', 'migrations', 'database/migrations']\n      const { existing: migrationDirsFound } = await this.checkFiles(projectPath, migrationDirs)\n      evidence.push(...migrationDirsFound.map(f => `Found migration directory: ${f.relative}`))\n      \n      // Check for migration files\n      const migrationPatterns = await this.findFilesByPattern(\n        projectPath,\n        [/\\d+.*\\.(ts|js)$/],\n        ['src/migrations', 'migrations', 'database/migrations']\n      )\n      if (migrationPatterns.length > 0) {\n        evidence.push(`Found ${migrationPatterns.length} migration files`)\n      }\n      \n      // Calculate confidence using helper\n      const confidence = this.calculateConfidence({\n        required: { \n          found: deps.found.length > 0 ? 1 : 0, \n          total: 1 \n        },\n        optional: { \n          found: configFilesFound.length + (entityPatterns.length > 0 ? 1 : 0) + migrationDirsFound.length + (migrationPatterns.length > 0 ? 1 : 0), \n          total: 4 \n        },\n        negative: 0\n      })\n      \n      return {\n        found: confidence > 0.3,\n        confidence: Math.round(confidence * 100),\n        evidence\n      }\n    } catch (error) {\n      return {\n        found: false,\n        confidence: 0,\n        evidence: [`Error detecting TypeORM: ${error}`]\n      }\n    }\n  }\n  \n  async extractConfig(projectPath: string): Promise<TypeORMConfig | null> {\n    try {\n      // Try to find TypeORM config file\n      const configFiles = [\n        'ormconfig.ts',\n        'ormconfig.js',\n        'ormconfig.json',\n        'typeorm.config.ts',\n        'typeorm.config.js',\n        'src/data-source.ts',\n        'src/data-source.js'\n      ]\n      \n      const { existing: configFilesFound } = await this.checkFiles(projectPath, configFiles)\n      if (configFilesFound.length === 0) {\n        return null\n      }\n      \n      const configFile = configFilesFound[0]\n      let entities: string[] = []\n      let migrations: string[] = []\n      \n      if (configFile.relative.endsWith('.json')) {\n        // Parse JSON config\n        const configContent = await fs.readFile(configFile.absolute, 'utf-8')\n        const jsonConfig = JSON.parse(configContent)\n        entities = Array.isArray(jsonConfig.entities) ? jsonConfig.entities : ['src/**/*.entity.{ts,js}']\n        migrations = Array.isArray(jsonConfig.migrations) ? jsonConfig.migrations : ['src/migrations/*.{ts,js}']\n      } else {\n        // Parse TypeScript/JavaScript config (basic parsing)\n        const configContent = await fs.readFile(configFile.absolute, 'utf-8')\n        entities = this.extractArrayValue(configContent, 'entities') || ['src/**/*.entity.{ts,js}']\n        migrations = this.extractArrayValue(configContent, 'migrations') || ['src/migrations/*.{ts,js}']\n      }\n      \n      const config: TypeORMConfig = {\n        type: 'typeorm',\n        configFile,\n        entities,\n        migrations,\n        migrationDirectory: configFile, // Will be updated with proper migration directory  \n        dependencies: ['typeorm'],\n        cli: {\n          migrationsDir: 'src/migrations',\n          entitiesDir: 'src/entities'\n        }\n      }\n      \n      return config\n    } catch (error) {\n      console.warn(`Failed to extract TypeORM config: ${error}`)\n      return null\n    }\n  }\n  \n  async getDatabaseConfig(projectPath: string): Promise<DatabaseConfig | null> {\n    try {\n      // Look for database URL in various places\n      const envFiles = ['.env', '.env.local', '.env.development']\n      const { existing: envFilesFound } = await this.checkFiles(projectPath, envFiles)\n      \n      for (const envFile of envFilesFound) {\n        const envContent = await fs.readFile(envFile.absolute, 'utf-8')\n        const dbUrl = this.extractEnvValue(envContent, 'DATABASE_URL') || \n                      this.extractEnvValue(envContent, 'DB_URL') ||\n                      this.extractEnvValue(envContent, 'TYPEORM_URL')\n        if (dbUrl) {\n          const parsed = this.parseDatabaseUrl(dbUrl)\n          if (parsed) return parsed\n        }\n      }\n      \n      // Try to extract from TypeORM config\n      const typeormConfig = await this.extractConfig(projectPath)\n      if (typeormConfig?.configFile) {\n        const configContent = await fs.readFile(typeormConfig.configFile.absolute, 'utf-8')\n        \n        // Extract database configuration from config file\n        const type = this.extractConfigValue(configContent, 'type')\n        const host = this.extractConfigValue(configContent, 'host')\n        const port = this.extractConfigValue(configContent, 'port')\n        const database = this.extractConfigValue(configContent, 'database')\n        const username = this.extractConfigValue(configContent, 'username')\n        const password = this.extractConfigValue(configContent, 'password')\n        \n        if (type && database) {\n          const dbTypeMap: Record<string, 'postgresql' | 'mysql' | 'sqlite'> = {\n            'postgres': 'postgresql',\n            'postgresql': 'postgresql',\n            'mysql': 'mysql',\n            'mariadb': 'mysql',\n            'sqlite': 'sqlite'\n          }\n          \n          const mappedType = dbTypeMap[type] || 'postgresql'\n          \n          return {\n            type: mappedType,\n            host: host || 'localhost',\n            port: port ? parseInt(port) : (mappedType === 'postgresql' ? 5432 : mappedType === 'mysql' ? 3306 : undefined),\n            database,\n            username,\n            password\n          }\n        }\n      }\n      \n      // Fallback defaults\n      return {\n        type: 'postgresql',\n        host: 'localhost',\n        port: 5432,\n        database: 'main'\n      }\n    } catch (error) {\n      console.warn(`Failed to extract database config: ${error}`)\n      return null\n    }\n  }\n  \n  private extractConfigValue(content: string, key: string): string | undefined {\n    // Simple regex-based extraction - in production, use proper AST parsing\n    const regex = new RegExp(`${key}:\\\\s*['\"]([^'\"]+)['\"]`)\n    const match = content.match(regex)\n    return match?.[1]\n  }\n  \n  private extractArrayValue(content: string, key: string): string[] | undefined {\n    // Basic array extraction - in production, use proper AST parsing\n    const regex = new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]+)\\\\]`)\n    const match = content.match(regex)\n    if (!match) return undefined\n    \n    return match[1]\n      .split(',')\n      .map(item => item.trim().replace(/['\"]/g, ''))\n      .filter(item => item.length > 0)\n  }\n  \n  private extractEnvValue(content: string, key: string): string | undefined {\n    const regex = new RegExp(`^${key}\\\\s*=\\\\s*(.+)$`, 'm')\n    const match = content.match(regex)\n    return match?.[1]?.replace(/['\"]/g, '').trim()\n  }\n} ","/**\n * flow sync - Detect ORM changes and create enhanced migration plan\n */\n\nimport { confirm, select } from '@clack/prompts'\nimport { getFlowConfig, GlobalOptions } from '../lib/config.js'\nimport { createSpinner } from '../lib/prompts.js'\nimport { PrismaDetector, DrizzleDetector, TypeORMDetector } from '../analyzer/index.js'\nimport { EnhancementEngine } from '../enhancer/index.js'\nimport fs from 'fs-extra'\nimport path from 'node:path'\nimport { diffChars } from 'diff'\nimport pc from 'picocolors'\nimport { execa } from 'execa'\n\nexport interface SyncOptions {\n  force?: boolean\n  orm?: 'prisma' | 'drizzle' | 'typeorm' | 'auto'\n  project?: string\n  yes?: boolean\n}\n\nexport async function syncCommand(options: SyncOptions, globalOptions: GlobalOptions): Promise<void> {\n  const spinner = createSpinner('Detecting ORM setup and analyzing schema changes...')\n  \n  const projectPath = options.project ? path.resolve(options.project) : process.cwd()\n  const cfg = await getFlowConfig(globalOptions, projectPath)\n  const envCfg = cfg.environments[cfg.defaultEnvironment]\n  \n  let detectedORM: string | null = null\n  const detectors = [\n    { name: 'prisma', detector: new PrismaDetector() },\n    { name: 'drizzle', detector: new DrizzleDetector() },\n    { name: 'typeorm', detector: new TypeORMDetector() }\n  ]\n\n  if (options.orm && options.orm !== 'auto') {\n    detectedORM = options.orm\n  } else {\n    for (const { name, detector } of detectors) {\n      const result = await detector.detect(projectPath)\n      if (result.found) {\n        detectedORM = name\n        break\n      }\n    }\n  }\n\n  if (!detectedORM) {\n    spinner.fail('ORM detection failed')\n    console.log(pc.red('No supported ORM detected. Make sure you have Prisma, Drizzle, or TypeORM configured.'))\n    return\n  }\n\n  spinner.update(`Detected ${detectedORM.toUpperCase()} - analyzing schema changes...`)\n\n  const detector = detectors.find(d => d.name === detectedORM)?.detector\n  if (!detector) {\n    spinner.fail('Detector not found')\n    return\n  }\n\n  const ormConfig = await detector.extractConfig(projectPath)\n  \n  const hasChanges = await checkForSchemaChanges(detectedORM, ormConfig, projectPath)\n  \n  if (!hasChanges && !options.force) {\n    spinner.succeed('Schema analysis completed')\n    console.log(pc.green('üéâ No pending schema changes detected. Your migrations are up to date.'))\n    console.log(pc.gray('Use --force to re-analyze existing migrations for enhancements.'))\n    return\n  }\n\n  const migrationsDir = envCfg.migrationsPath || (ormConfig?.migrationDirectory?.relative) || './migrations'\n  const absoluteMigrationsDir = path.resolve(projectPath, migrationsDir)\n\n  if (hasChanges) {\n    spinner.update('Generating migration plan for schema changes...')\n    await handleSchemaChanges(detectedORM, ormConfig, absoluteMigrationsDir, globalOptions, projectPath, options)\n  } else {\n    spinner.update('Analyzing existing migrations for enhancements...')\n    await enhanceExistingMigrations(absoluteMigrationsDir, globalOptions, options)\n  }\n\n  spinner.succeed('Sync completed')\n}\n\nasync function checkForSchemaChanges(orm: string, config: any, projectPath: string): Promise<boolean> {\n  switch (orm) {\n    case 'prisma':\n      return await checkPrismaChanges(config, projectPath)\n    case 'drizzle':\n      return await checkDrizzleChanges(config, projectPath)\n    case 'typeorm':\n      return await checkTypeORMChanges(config, projectPath)\n    default:\n      return false\n  }\n}\n\nasync function checkPrismaChanges(config: any, projectPath: string): Promise<boolean> {\n  try {\n    const schemaPath = path.join(projectPath, 'prisma', 'schema.prisma')\n    const migrationsPath = path.join(projectPath, 'prisma', 'migrations')\n    \n    if (!await fs.pathExists(schemaPath)) return false\n    if (!await fs.pathExists(migrationsPath)) return true\n    \n    try {\n      await execa('npx prisma migrate status', { cwd: projectPath })\n      try {\n        const { stdout } = await execa('npx prisma migrate diff --from-migrations ./prisma/migrations --to-schema-datamodel ./prisma/schema.prisma', { cwd: projectPath })\n        return stdout.trim().length > 0\n      } catch {\n        return true\n      }\n    } catch {\n      const schemaStats = await fs.stat(schemaPath)\n      const migrationFiles = await fs.readdir(migrationsPath)\n      if (migrationFiles.length === 0) return true\n      const migrationDirs = migrationFiles.filter(file => file.match(/^\\d{14}_/))\n      if (migrationDirs.length === 0) return true\n      const latestMigration = migrationDirs.sort().pop()\n      const latestMigrationPath = path.join(migrationsPath, latestMigration!)\n      const migrationStats = await fs.stat(latestMigrationPath)\n      return schemaStats.mtime > migrationStats.mtime\n    }\n  } catch (error) {\n    console.warn('Error checking Prisma changes:', error)\n    return false\n  }\n}\n\nasync function checkDrizzleChanges(config: any, projectPath: string): Promise<boolean> {\n  // FIXME: This is a temporary hack to bypass the unreliable drizzle-kit check\n  return true\n}\n\nasync function checkTypeORMChanges(config: any, projectPath: string): Promise<boolean> {\n  try {\n    try {\n      const { stdout } = await execa('npx typeorm migration:show', { cwd: projectPath })\n      return !stdout.includes('No migrations are pending')\n    } catch {\n      const entityDirs = ['src/entities', 'src/entity', 'entities']\n      const migrationsDir = config?.migrationDirectory || './src/migrations'\n      const migrationsDirPath = path.join(projectPath, migrationsDir)\n      \n      if (!await fs.pathExists(migrationsDirPath)) return true\n      \n      for (const entityDir of entityDirs) {\n        const entityDirPath = path.join(projectPath, entityDir)\n        if (await fs.pathExists(entityDirPath)) {\n          const entityFiles = await fs.readdir(entityDirPath)\n          const tsFiles = entityFiles.filter(f => f.endsWith('.ts') || f.endsWith('.js'))\n          \n          for (const entityFile of tsFiles) {\n            const entityPath = path.join(entityDirPath, entityFile)\n            const entityStats = await fs.stat(entityPath)\n            const migrationFiles = await fs.readdir(migrationsDirPath)\n            if (migrationFiles.length === 0) return true\n            const latestMigration = migrationFiles.sort().pop()\n            const latestMigrationPath = path.join(migrationsDirPath, latestMigration!)\n            const migrationStats = await fs.stat(latestMigrationPath)\n            if (entityStats.mtime > migrationStats.mtime) {\n              return true\n            }\n          }\n        }\n      }\n      return false\n    }\n  } catch (error) {\n    console.warn('Error checking TypeORM changes:', error)\n    return false\n  }\n}\n\nasync function handleSchemaChanges(\n  orm: string,\n  config: any,\n  migrationsDir: string,\n  globalOptions: GlobalOptions,\n  projectPath: string,\n  options: SyncOptions,\n): Promise<void> {\n  const migrationName = `flow_change_${Date.now()}`\n  let generateCmd = ''\n\n  switch (orm) {\n    case 'prisma':\n      generateCmd = `npx prisma migrate dev --name ${migrationName}`\n      break\n    case 'drizzle':\n      // The --dialect flag conflicts with the --config flag which drizzle-kit uses implicitly.\n      // Drizzle Kit will detect the dialect from the config file.\n      generateCmd = `npx drizzle-kit generate`\n      break\n    case 'typeorm':\n      const migPath = path.join(migrationsDir, migrationName)\n      generateCmd = `npx typeorm migration:generate ${migPath}`\n      break\n  }\n\n  const spinner = createSpinner(`Running ${orm} to generate migration...`)\n  try {\n    const { stdout, stderr } = await execa(generateCmd, { cwd: projectPath, shell: true })\n    if (globalOptions.debug) {\n      console.log(stdout)\n      if (stderr) console.error(pc.yellow(stderr))\n    }\n    spinner.succeed('ORM migration generated successfully.')\n    await enhanceExistingMigrations(migrationsDir, globalOptions, options)\n  } catch (error: any) {\n    spinner.fail('Migration generation failed.')\n    console.error(pc.red(error.stderr || error.message))\n    console.log(pc.yellow(`Could not automatically generate migration. Please run the following command manually:\\n${generateCmd}`))\n  }\n}\n\nasync function enhanceExistingMigrations(\n  migrationsDir: string,\n  globalOptions: GlobalOptions,\n  options: SyncOptions,\n): Promise<void> {\n  const spinner = createSpinner('Analyzing migrations for enhancements...')\n  if (!(await fs.pathExists(migrationsDir))) {\n    spinner.fail(`Migrations directory not found: ${migrationsDir}`)\n    return\n  }\n\n  const files = await fs.readdir(migrationsDir)\n  const migrationFiles = files.filter(file => file.endsWith('.sql') || file.endsWith('.ts') || file.endsWith('.js'));\n\n  if (migrationFiles.length === 0) {\n    spinner.succeed('No migration files found to analyze.')\n    return\n  }\n  \n  spinner.update(`Found ${migrationFiles.length} migration(s) to analyze for enhancements.`)\n\n  // Ultra-fast parallel processing\n  const engine = new EnhancementEngine();\n  \n  // Step 1: Read all files in parallel (ultra-fast I/O)\n  spinner.update('Reading migration files...')\n  const fileReads = await Promise.all(\n    migrationFiles.map(async (file) => {\n      const filePath = path.join(migrationsDir, file);\n      const content = await fs.readFile(filePath, 'utf-8');\n      const sql = extractSQLFromMigrationFile(content);\n      return {\n        file,\n        filePath,\n        content,\n        sql,\n        migrationFile: {\n          path: filePath,\n          name: file,\n          up: sql,\n          down: '',\n          timestamp: new Date(),\n          operations: [],\n          checksum: '',\n        }\n      };\n    })\n  );\n\n  // Step 2: Analyze all migrations in parallel (ultra-fast processing)\n  spinner.update('Analyzing migrations in parallel...')\n  const analyses = await Promise.all(\n    fileReads.map(async ({ file, filePath, content, sql, migrationFile }) => {\n      const enhanced = await engine.enhance(migrationFile);\n      return {\n        file,\n        filePath,\n        content,\n        sql,\n        enhanced,\n        hasChanges: enhanced.original.up !== enhanced.enhanced.up\n      };\n    })\n  );\n\n  // Step 3: Present results and apply changes\n  let changesApplied = 0;\n  for (const analysis of analyses) {\n    if (analysis.hasChanges) {\n      const originalColor = (text: string) => pc.red(`- ${text}`);\n      const enhancedColor = (text: string) => pc.green(`+ ${text}`);\n      const diff = diffChars(analysis.enhanced.original.up, analysis.enhanced.enhanced.up);\n      let diffOutput = '';\n      diff.forEach(part => {\n        const color = part.added ? enhancedColor : part.removed ? originalColor : pc.gray;\n        diffOutput += color(part.value);\n      });\n      console.log(pc.bold(`\\nEnhancements for ${analysis.file}:`))\n      console.log(diffOutput)\n\n      const proceed = options.yes ? true : await confirm({ message: `Apply these enhancements to ${analysis.file}?` })\n\n      if (proceed) {\n        try {\n          const newContent = await replaceEnhancedSQLInMigrationFile(analysis.filePath, analysis.enhanced.enhanced.up, analysis.enhanced.enhanced.down);\n          await fs.writeFile(analysis.filePath, newContent, 'utf-8');\n          console.log(pc.green(`‚úÖ Updated ${analysis.file}`))\n          changesApplied++;\n        } catch (error) {\n          console.log(pc.yellow(`‚ö†Ô∏è  Could not automatically update ${analysis.file}: ${error}`))\n          console.log(pc.gray('Enhanced UP SQL:'))\n          console.log(analysis.enhanced.enhanced.up)\n          if (analysis.enhanced.enhanced.down) {\n            console.log(pc.gray('Enhanced DOWN SQL:'))\n            console.log(analysis.enhanced.enhanced.down)\n          }\n        }\n      } else {\n        console.log(pc.gray(`Skipped ${analysis.file}`))\n      }\n    }\n  }\n\n  if (changesApplied === 0 && analyses.every(a => !a.hasChanges)) {\n    console.log(pc.gray('No enhancements needed for any migration files.'))\n  }\n  spinner.succeed('Enhancement analysis completed.')\n}\n\nfunction extractSQLFromMigrationFile(content: string): string {\n  const sqlPatterns = [\n    /queryRunner\\.query\\s*\\(\\s*[`\"']([^`\"']+)[`\"']/g,\n    /sql\\s*`([^`]+)`/g,\n    /\"((?:CREATE|ALTER|DROP|INSERT|UPDATE|DELETE)[^\"]+)\"/gi\n  ];\n  let extractedSQL = '';\n  for (const pattern of sqlPatterns) {\n    let match;\n    while ((match = pattern.exec(content)) !== null) {\n      extractedSQL += match[1] + ';\\n';\n    }\n  }\n  return extractedSQL || content;\n}\n\nasync function replaceEnhancedSQLInMigrationFile(filePath: string, upSQL: string, downSQL?: string): Promise<string> {\n  const content = await fs.readFile(filePath, 'utf-8');\n  let updatedContent = content;\n\n  // Generate enhancement comment header\n  const fileName = path.basename(filePath);\n  const enhancementComment = generateEnhancementComment(fileName, upSQL);\n\n  // Add comment at the top of the file if it doesn't already exist\n  if (!updatedContent.includes('Enhanced by DriftJS')) {\n    const lines = updatedContent.split('\\n');\n    let insertIndex = 0;\n    \n    // Skip existing comments and imports to find insertion point\n    for (let i = 0; i < lines.length; i++) {\n      const line = lines[i].trim();\n      if (line && !line.startsWith('//') && !line.startsWith('/*') && !line.startsWith('*') && !line.startsWith('import') && !line.startsWith('export')) {\n        insertIndex = i;\n        break;\n      }\n    }\n    \n    lines.splice(insertIndex, 0, enhancementComment, '');\n    updatedContent = lines.join('\\n');\n  }\n\n  updatedContent = updatedContent.replace(/queryRunner\\.query\\s*\\(\\s*[`\"']([^`\"']+)[`\"']/g, `queryRunner.query(\\`${upSQL.trim()}\\`)`);\n  updatedContent = updatedContent.replace(/sql\\s*`([^`]+)`/g, `sql\\`${upSQL.trim()}\\``);\n  updatedContent = updatedContent.replace(/\"((?:CREATE|ALTER|DROP|INSERT|UPDATE|DELETE)[^\"]+)\"/gi, `\"${upSQL.trim()}\"`);\n\n  if (downSQL) {\n    updatedContent = updatedContent.replace(/(public async down\\(.*?\\): Promise<void> \\{[\\s\\S]*?queryRunner\\.query\\s*\\(\\s*[`\"'])([^`\"']+)([`\"'])/, `$1${downSQL.trim()}$3`);\n  }\n\n  return updatedContent;\n}\n\nfunction generateEnhancementComment(fileName: string, enhancedSQL: string): string {\n  const timestamp = new Date().toISOString().split('T')[0];\n  const operations = analyzeEnhancements(enhancedSQL);\n  \n  const header = `/*\n * Migration Enhanced by DriftJS.com\n * File: ${fileName}\n * Enhanced: ${timestamp}\n * \n * This migration has been optimized for production safety and performance.\n * Learn more at https://driftjs.com\n */`;\n\n  if (operations.length === 1) {\n    return `${header.slice(0, -2)}\n * \n * Enhancement: ${operations[0]}\n */`;\n  } else if (operations.length > 1) {\n    const enhancementList = operations.map(op => ` * - ${op}`).join('\\n');\n    return `${header.slice(0, -2)}\n * \n * Enhancements Applied:\n${enhancementList}\n */`;\n  }\n  \n  return header;\n}\n\nfunction analyzeEnhancements(sql: string): string[] {\n  const enhancements: string[] = [];\n  const sqlLower = sql.toLowerCase();\n  \n  // Detect common enhancement patterns\n  if (sqlLower.includes('add constraint') && sqlLower.includes('not valid')) {\n    enhancements.push('Safe constraint addition with NOT VALID optimization');\n  }\n  if (sqlLower.includes('concurrently')) {\n    enhancements.push('Non-blocking concurrent index creation');\n  }\n  if (sqlLower.includes('backup') || sqlLower.includes('copy')) {\n    enhancements.push('Data backup created before destructive operations');\n  }\n  if (sqlLower.includes('alter table') && sqlLower.includes('add column') && !sqlLower.includes('not null')) {\n    enhancements.push('Nullable column added first for safe NOT NULL migration');\n  }\n  if (sqlLower.includes('validate constraint')) {\n    enhancements.push('Constraint validation separated for reduced downtime');\n  }\n  if (sqlLower.includes('lock timeout') || sqlLower.includes('statement_timeout')) {\n    enhancements.push('Query timeouts configured to prevent long locks');\n  }\n  if (sqlLower.includes('begin;') && sqlLower.includes('commit;')) {\n    enhancements.push('Transaction boundaries optimized for safety');\n  }\n  \n  // Default if no specific enhancements detected\n  if (enhancements.length === 0) {\n    enhancements.push('Production-ready migration optimizations applied');\n  }\n  \n  return enhancements;\n}\n","export interface GlobalOptions {\n  debug?: boolean\n  config?: string\n  dryRun?: boolean\n}\n\nimport fsExtra from 'fs-extra'\nimport { resolve, dirname } from 'node:path'\n\n/**\n * Locate and parse flow.config.json.\n * If --config is supplied use that path, otherwise walk parent directories.\n */\nexport async function getFlowConfig(global: GlobalOptions, projectPath?: string) {\n  const configPath = await findConfigFile(projectPath || process.cwd(), global.config)\n  return JSON.parse(await fsExtra.readFile(configPath, 'utf8'))\n}\n\nasync function findConfigFile(startDir: string, explicit?: string): Promise<string> {\n  if (explicit) {\n    const p = resolve(explicit)\n    if (await fsExtra.pathExists(p)) return p\n    throw new Error(`Config file not found at ${p}`)\n  }\n\n  let dir = startDir\n  while (true) {\n    const candidate = resolve(dir, 'flow.config.json')\n    if (await fsExtra.pathExists(candidate)) return candidate\n    const parent = dirname(dir)\n    if (parent === dir) break\n    dir = parent\n  }\n  throw new Error('flow.config.json not found')\n} ","import pkg from 'node-sql-parser';\nconst { Parser } = pkg;\nimport type { AST, Parser as ParserType } from 'node-sql-parser';\n\n/**\n * SQL operation types that can be parsed and analyzed\n */\nexport enum SqlOperationType {\n  CREATE_TABLE = 'CREATE_TABLE',\n  ALTER_TABLE = 'ALTER_TABLE',\n  DROP_TABLE = 'DROP_TABLE',\n  CREATE_INDEX = 'CREATE_INDEX',\n  DROP_INDEX = 'DROP_INDEX',\n  INSERT = 'INSERT',\n  UPDATE = 'UPDATE',\n  DELETE = 'DELETE',\n  SELECT = 'SELECT',\n  UNKNOWN = 'UNKNOWN'\n}\n\n/**\n * Parsed SQL operation with metadata\n */\nexport interface ParsedSqlOperation {\n  type: SqlOperationType;\n  sql: string;\n  ast: AST | null;\n  tableName?: string;\n  columnName?: string;\n  indexName?: string;\n  operation?: string;\n  metadata: {\n    isBlocking: boolean;\n    isDestructive: boolean;\n    affectsData: boolean;\n    requiresLock: boolean;\n    estimatedDuration: 'fast' | 'medium' | 'slow';\n  };\n}\n\n/**\n * SQL parsing result with all operations found\n */\nexport interface SqlParseResult {\n  operations: ParsedSqlOperation[];\n  errors: string[];\n  warnings: string[];\n  totalOperations: number;\n  destructiveOperations: number;\n  blockingOperations: number;\n}\n\n/**\n * SQL Parser class for analyzing migration SQL statements\n */\nexport class SqlParser {\n  private parser: ParserType;\n\n  constructor() {\n    // Initialize node-sql-parser with multiple database support\n    this.parser = new Parser();\n  }\n\n  /**\n   * Parse a single SQL statement and return operation details\n   */\n  public parseSql(sql: string, database: 'postgresql' | 'mysql' | 'sqlite' = 'postgresql'): ParsedSqlOperation {\n    try {\n      // Clean and normalize the SQL\n      const cleanSql = this.cleanSql(sql);\n      \n      // Parse the SQL into AST\n      const ast = this.parser.astify(cleanSql, { database });\n      \n      // Extract operation details\n      const operation = this.extractOperationFromAst(ast, cleanSql);\n      \n      return operation;\n    } catch (error) {\n      // If parsing fails, try to determine operation type from SQL text\n      const fallbackOperation = this.fallbackParse(sql);\n      return {\n        ...fallbackOperation,\n        ast: null,\n        metadata: {\n          ...fallbackOperation.metadata,\n          estimatedDuration: 'medium' // Default to medium for unparseable operations\n        }\n      };\n    }\n  }\n\n  /**\n   * Parse multiple SQL statements (common in migration files)\n   */\n  public parseMultipleSql(sqlStatements: string[], database: 'postgresql' | 'mysql' | 'sqlite' = 'postgresql'): SqlParseResult {\n    const operations: ParsedSqlOperation[] = [];\n    const errors: string[] = [];\n    const warnings: string[] = [];\n\n    for (const sql of sqlStatements) {\n      try {\n        const operation = this.parseSql(sql, database);\n        operations.push(operation);\n\n        // Add warnings for risky operations\n        if (operation.metadata.isDestructive) {\n          warnings.push(`Destructive operation detected: ${operation.type} on ${operation.tableName || 'unknown table'}`);\n        }\n        if (operation.metadata.isBlocking) {\n          warnings.push(`Blocking operation detected: ${operation.type} - may cause downtime`);\n        }\n      } catch (error) {\n        errors.push(`Failed to parse SQL: ${sql.substring(0, 50)}... - ${error}`);\n      }\n    }\n\n    return {\n      operations,\n      errors,\n      warnings,\n      totalOperations: operations.length,\n      destructiveOperations: operations.filter(op => op.metadata.isDestructive).length,\n      blockingOperations: operations.filter(op => op.metadata.isBlocking).length\n    };\n  }\n\n  /**\n   * Clean and normalize SQL for better parsing\n   */\n  private cleanSql(sql: string): string {\n    return sql\n      .trim()\n      .replace(/\\s+/g, ' ') // Normalize whitespace\n      .replace(/;\\s*$/, ''); // Remove trailing semicolon\n  }\n\n  /**\n   * Extract operation details from parsed AST\n   */\n  private extractOperationFromAst(ast: AST | AST[], sql: string): ParsedSqlOperation {\n    // Handle array of AST nodes\n    if (Array.isArray(ast)) {\n      ast = ast[0]; // Take the first statement\n    }\n\n    const operation: ParsedSqlOperation = {\n      type: SqlOperationType.UNKNOWN,\n      sql,\n      ast,\n      metadata: {\n        isBlocking: false,\n        isDestructive: false,\n        affectsData: false,\n        requiresLock: false,\n        estimatedDuration: 'fast'\n      }\n    };\n\n    // Cast to any to handle different AST node types\n    const astNode = ast as any;\n\n    switch (astNode.type?.toLowerCase()) {\n      case 'create':\n        if (astNode.keyword === 'table') {\n          operation.type = SqlOperationType.CREATE_TABLE;\n          operation.tableName = this.extractTableName(astNode);\n          operation.metadata = {\n            isBlocking: true,\n            isDestructive: false,\n            affectsData: false,\n            requiresLock: true,\n            estimatedDuration: 'medium'\n          };\n        } else if (astNode.keyword === 'index') {\n          operation.type = SqlOperationType.CREATE_INDEX;\n          operation.indexName = astNode.index;\n          operation.tableName = this.extractTableName(astNode);\n          operation.metadata = {\n            isBlocking: true,\n            isDestructive: false,\n            affectsData: false,\n            requiresLock: true,\n            estimatedDuration: 'slow'\n          };\n        }\n        break;\n\n      case 'alter':\n        operation.type = SqlOperationType.ALTER_TABLE;\n        operation.tableName = this.extractTableName(astNode);\n        operation.operation = astNode.expr?.type || 'unknown';\n        \n        // Analyze the specific ALTER operation\n        const alterMetadata = this.analyzeAlterOperation(astNode);\n        operation.metadata = alterMetadata;\n        break;\n\n      case 'drop':\n        if (astNode.keyword === 'table') {\n          operation.type = SqlOperationType.DROP_TABLE;\n          operation.tableName = this.extractTableName(astNode);\n          operation.metadata = {\n            isBlocking: true,\n            isDestructive: true,\n            affectsData: true,\n            requiresLock: true,\n            estimatedDuration: 'fast'\n          };\n        } else if (astNode.keyword === 'index') {\n          operation.type = SqlOperationType.DROP_INDEX;\n          operation.indexName = astNode.name;\n          operation.metadata = {\n            isBlocking: false,\n            isDestructive: true,\n            affectsData: false,\n            requiresLock: false,\n            estimatedDuration: 'fast'\n          };\n        }\n        break;\n\n      case 'insert':\n        operation.type = SqlOperationType.INSERT;\n        operation.tableName = this.extractTableName(astNode);\n        operation.metadata = {\n          isBlocking: false,\n          isDestructive: false,\n          affectsData: true,\n          requiresLock: false,\n          estimatedDuration: 'fast'\n        };\n        break;\n\n      case 'update':\n        operation.type = SqlOperationType.UPDATE;\n        operation.tableName = this.extractTableName(astNode);\n        operation.metadata = {\n          isBlocking: false,\n          isDestructive: false,\n          affectsData: true,\n          requiresLock: false,\n          estimatedDuration: 'medium'\n        };\n        break;\n\n      case 'delete':\n        operation.type = SqlOperationType.DELETE;\n        operation.tableName = this.extractTableName(astNode);\n        operation.metadata = {\n          isBlocking: false,\n          isDestructive: true,\n          affectsData: true,\n          requiresLock: false,\n          estimatedDuration: 'medium'\n        };\n        break;\n\n      case 'select':\n        operation.type = SqlOperationType.SELECT;\n        operation.tableName = this.extractTableName(astNode);\n        operation.metadata = {\n          isBlocking: false,\n          isDestructive: false,\n          affectsData: false,\n          requiresLock: false,\n          estimatedDuration: 'fast'\n        };\n        break;\n    }\n\n    return operation;\n  }\n\n  /**\n   * Analyze ALTER TABLE operations for specific risk assessment\n   */\n  private analyzeAlterOperation(ast: any): ParsedSqlOperation['metadata'] {\n    const baseMetadata = {\n      isBlocking: true,\n      isDestructive: false,\n      affectsData: false,\n      requiresLock: true,\n      estimatedDuration: 'medium' as const\n    };\n\n    // Check for specific ALTER operations\n    if (ast.expr) {\n      const action = ast.expr.action?.toLowerCase();\n      \n      switch (action) {\n        case 'add':\n          if (ast.expr.resource === 'column') {\n            // Adding NOT NULL column is blocking and potentially destructive\n            const hasNotNull = ast.expr.definition?.nullable === false;\n            const hasDefault = ast.expr.definition?.defaultValue !== undefined;\n            \n            return {\n              ...baseMetadata,\n              isDestructive: hasNotNull && !hasDefault,\n              estimatedDuration: hasNotNull ? 'slow' : 'medium'\n            };\n          }\n          break;\n\n        case 'drop':\n          return {\n            ...baseMetadata,\n            isDestructive: true,\n            affectsData: true,\n            estimatedDuration: 'fast'\n          };\n\n        case 'modify':\n        case 'change':\n          return {\n            ...baseMetadata,\n            isDestructive: true,\n            affectsData: true,\n            estimatedDuration: 'slow'\n          };\n\n        case 'rename':\n          return {\n            ...baseMetadata,\n            isDestructive: false,\n            affectsData: false,\n            estimatedDuration: 'fast'\n          };\n      }\n    }\n\n    return baseMetadata;\n  }\n\n  /**\n   * Extract table name from AST node\n   */\n  private extractTableName(ast: any): string | undefined {\n    if (ast.table) {\n      return typeof ast.table === 'string' ? ast.table : ast.table.table;\n    }\n    if (ast.name) {\n      return typeof ast.name === 'string' ? ast.name : ast.name.table;\n    }\n    if (ast.from && ast.from.length > 0) {\n      const fromTable = ast.from[0];\n      return typeof fromTable.table === 'string' ? fromTable.table : fromTable.table?.table;\n    }\n    return undefined;\n  }\n\n  /**\n   * Fallback parsing when AST parsing fails\n   */\n  private fallbackParse(sql: string): Omit<ParsedSqlOperation, 'ast'> {\n    const upperSql = sql.toUpperCase().trim();\n    \n    // Extract operation type from SQL text\n    let type = SqlOperationType.UNKNOWN;\n    let tableName: string | undefined;\n    let isBlocking = false;\n    let isDestructive = false;\n    let affectsData = false;\n    let requiresLock = false;\n    let estimatedDuration: 'fast' | 'medium' | 'slow' = 'medium';\n\n    if (upperSql.startsWith('CREATE TABLE')) {\n      type = SqlOperationType.CREATE_TABLE;\n      tableName = this.extractTableNameFromText(sql, 'CREATE TABLE');\n      isBlocking = true;\n      requiresLock = true;\n    } else if (upperSql.startsWith('CREATE INDEX')) {\n      type = SqlOperationType.CREATE_INDEX;\n      isBlocking = true;\n      requiresLock = true;\n      estimatedDuration = 'slow';\n    } else if (upperSql.startsWith('ALTER TABLE')) {\n      type = SqlOperationType.ALTER_TABLE;\n      tableName = this.extractTableNameFromText(sql, 'ALTER TABLE');\n      isBlocking = true;\n      requiresLock = true;\n      \n      // Check for destructive ALTER operations\n      if (upperSql.includes('DROP COLUMN') || upperSql.includes('DROP CONSTRAINT')) {\n        isDestructive = true;\n        affectsData = true;\n      }\n    } else if (upperSql.startsWith('DROP TABLE')) {\n      type = SqlOperationType.DROP_TABLE;\n      tableName = this.extractTableNameFromText(sql, 'DROP TABLE');\n      isBlocking = true;\n      isDestructive = true;\n      affectsData = true;\n      requiresLock = true;\n      estimatedDuration = 'fast';\n    } else if (upperSql.startsWith('DROP INDEX')) {\n      type = SqlOperationType.DROP_INDEX;\n      isDestructive = true;\n      estimatedDuration = 'fast';\n    } else if (upperSql.startsWith('INSERT')) {\n      type = SqlOperationType.INSERT;\n      affectsData = true;\n      estimatedDuration = 'fast';\n    } else if (upperSql.startsWith('UPDATE')) {\n      type = SqlOperationType.UPDATE;\n      affectsData = true;\n    } else if (upperSql.startsWith('DELETE')) {\n      type = SqlOperationType.DELETE;\n      isDestructive = true;\n      affectsData = true;\n    } else if (upperSql.startsWith('SELECT')) {\n      type = SqlOperationType.SELECT;\n      estimatedDuration = 'fast';\n    }\n\n    return {\n      type,\n      sql,\n      tableName,\n      metadata: {\n        isBlocking,\n        isDestructive,\n        affectsData,\n        requiresLock,\n        estimatedDuration\n      }\n    };\n  }\n\n  /**\n   * Extract table name from SQL text using regex\n   */\n  private extractTableNameFromText(sql: string, operation: string): string | undefined {\n    const regex = new RegExp(`${operation}\\\\s+(?:IF\\\\s+(?:NOT\\\\s+)?EXISTS\\\\s+)?(?:\\`|\\\"|\\\\[)?([\\\\w_]+)(?:\\`|\\\"|\\\\])?`, 'i');\n    const match = sql.match(regex);\n    return match ? match[1] : undefined;\n  }\n} ","import * as fs from 'fs/promises';\nimport * as path from 'path';\nimport { SqlParser, type ParsedSqlOperation } from './sql-parser.js';\n\n/**\n * Migration file types supported\n */\nexport enum MigrationType {\n  PRISMA = 'PRISMA',\n  DRIZZLE = 'DRIZZLE',\n  TYPEORM = 'TYPEORM',\n  PLAIN_SQL = 'PLAIN_SQL'\n}\n\n/**\n * Parsed migration file result\n */\nexport interface ParsedMigration {\n  filePath: string;\n  type: MigrationType;\n  name: string;\n  timestamp?: Date;\n  upOperations: ParsedSqlOperation[];\n  downOperations: ParsedSqlOperation[];\n  metadata: {\n    ormVersion?: string;\n    database: 'postgresql' | 'mysql' | 'sqlite';\n    hasUpMigration: boolean;\n    hasDownMigration: boolean;\n    totalOperations: number;\n    destructiveOperations: number;\n    blockingOperations: number;\n  };\n  errors: string[];\n  warnings: string[];\n}\n\n/**\n * Migration parsing result for multiple files\n */\nexport interface MigrationParseResult {\n  migrations: ParsedMigration[];\n  totalMigrations: number;\n  errors: string[];\n  warnings: string[];\n  summary: {\n    byType: Record<MigrationType, number>;\n    totalOperations: number;\n    destructiveOperations: number;\n    blockingOperations: number;\n  };\n}\n\n/**\n * Migration Parser for different ORM formats\n */\nexport class MigrationParser {\n  private sqlParser: SqlParser;\n\n  constructor() {\n    this.sqlParser = new SqlParser();\n  }\n\n  /**\n   * Parse a single migration file\n   */\n  public async parseMigrationFile(filePath: string, type?: MigrationType): Promise<ParsedMigration> {\n    const content = await fs.readFile(filePath, 'utf-8');\n    const detectedType = type || this.detectMigrationType(filePath, content);\n    \n    switch (detectedType) {\n      case MigrationType.PRISMA:\n        return this.parsePrismaMigration(filePath, content);\n      case MigrationType.DRIZZLE:\n        return this.parseDrizzleMigration(filePath, content);\n      case MigrationType.TYPEORM:\n        return this.parseTypeOrmMigration(filePath, content);\n      case MigrationType.PLAIN_SQL:\n        return this.parsePlainSqlMigration(filePath, content);\n      default:\n        throw new Error(`Unsupported migration type: ${detectedType}`);\n    }\n  }\n\n  /**\n   * Parse multiple migration files from a directory\n   */\n  public async parseMigrationDirectory(dirPath: string): Promise<MigrationParseResult> {\n    const files = await this.findMigrationFiles(dirPath);\n    const migrations: ParsedMigration[] = [];\n    const errors: string[] = [];\n    const warnings: string[] = [];\n\n    for (const file of files) {\n      try {\n        const migration = await this.parseMigrationFile(file);\n        migrations.push(migration);\n        errors.push(...migration.errors);\n        warnings.push(...migration.warnings);\n      } catch (error) {\n        errors.push(`Failed to parse ${file}: ${error}`);\n      }\n    }\n\n    const summary = this.calculateSummary(migrations);\n\n    return {\n      migrations,\n      totalMigrations: migrations.length,\n      errors,\n      warnings,\n      summary\n    };\n  }\n\n  /**\n   * Detect migration type from file path and content\n   */\n  private detectMigrationType(filePath: string, content: string): MigrationType {\n    const fileName = path.basename(filePath);\n    const extension = path.extname(filePath);\n\n    // Check for Prisma migration (usually in prisma/migrations/)\n    if (filePath.includes('prisma/migrations') || fileName === 'migration.sql') {\n      return MigrationType.PRISMA;\n    }\n\n    // Check for Drizzle migration\n    if (extension === '.ts' && (\n      content.includes('import { sql }') ||\n      content.includes('drizzle-orm') ||\n      fileName.includes('drizzle')\n    )) {\n      return MigrationType.DRIZZLE;\n    }\n\n    // Check for TypeORM migration\n    if (extension === '.ts' && (\n      content.includes('import { MigrationInterface') ||\n      content.includes('QueryRunner') ||\n      fileName.match(/^\\d{13}-.*\\.ts$/)\n    )) {\n      return MigrationType.TYPEORM;\n    }\n\n    // Default to plain SQL\n    if (extension === '.sql') {\n      return MigrationType.PLAIN_SQL;\n    }\n\n    // If TypeScript but couldn't determine, try to guess from content\n    if (extension === '.ts') {\n      return MigrationType.TYPEORM; // Default for TS files\n    }\n\n    return MigrationType.PLAIN_SQL;\n  }\n\n  /**\n   * Parse Prisma migration file\n   */\n  private async parsePrismaMigration(filePath: string, content: string): Promise<ParsedMigration> {\n    const fileName = path.basename(filePath);\n    const timestamp = this.extractTimestampFromPrismaFile(filePath);\n    \n    // Prisma migrations are usually just SQL files\n    const sqlStatements = this.extractSqlStatements(content);\n    const database = this.detectDatabaseFromSql(content);\n    \n    const parseResult = this.sqlParser.parseMultipleSql(sqlStatements, database);\n\n    return {\n      filePath,\n      type: MigrationType.PRISMA,\n      name: fileName,\n      timestamp,\n      upOperations: parseResult.operations,\n      downOperations: [], // Prisma doesn't typically have down migrations\n      metadata: {\n        database,\n        hasUpMigration: parseResult.operations.length > 0,\n        hasDownMigration: false,\n        totalOperations: parseResult.operations.length,\n        destructiveOperations: parseResult.destructiveOperations,\n        blockingOperations: parseResult.blockingOperations\n      },\n      errors: parseResult.errors,\n      warnings: parseResult.warnings\n    };\n  }\n\n  /**\n   * Parse Drizzle migration file\n   */\n  private async parseDrizzleMigration(filePath: string, content: string): Promise<ParsedMigration> {\n    const fileName = path.basename(filePath, '.ts');\n    const timestamp = this.extractTimestampFromDrizzleFile(fileName);\n\n    // Extract SQL from Drizzle TypeScript file\n    const sqlStatements = this.extractSqlFromDrizzleTs(content);\n    const database = this.detectDatabaseFromContent(content);\n    \n    const parseResult = this.sqlParser.parseMultipleSql(sqlStatements, database);\n\n    return {\n      filePath,\n      type: MigrationType.DRIZZLE,\n      name: fileName,\n      timestamp,\n      upOperations: parseResult.operations,\n      downOperations: [], // Drizzle typically doesn't have down migrations in the same file\n      metadata: {\n        database,\n        hasUpMigration: parseResult.operations.length > 0,\n        hasDownMigration: false,\n        totalOperations: parseResult.operations.length,\n        destructiveOperations: parseResult.destructiveOperations,\n        blockingOperations: parseResult.blockingOperations\n      },\n      errors: parseResult.errors,\n      warnings: parseResult.warnings\n    };\n  }\n\n  /**\n   * Parse TypeORM migration file\n   */\n  private async parseTypeOrmMigration(filePath: string, content: string): Promise<ParsedMigration> {\n    const fileName = path.basename(filePath, '.ts');\n    const timestamp = this.extractTimestampFromTypeOrmFile(fileName);\n\n    // Extract SQL from up() and down() methods\n    const upSql = this.extractSqlFromTypeOrmMethod(content, 'up');\n    const downSql = this.extractSqlFromTypeOrmMethod(content, 'down');\n    const database = this.detectDatabaseFromContent(content);\n\n    const upParseResult = this.sqlParser.parseMultipleSql(upSql, database);\n    const downParseResult = this.sqlParser.parseMultipleSql(downSql, database);\n\n    return {\n      filePath,\n      type: MigrationType.TYPEORM,\n      name: fileName,\n      timestamp,\n      upOperations: upParseResult.operations,\n      downOperations: downParseResult.operations,\n      metadata: {\n        database,\n        hasUpMigration: upParseResult.operations.length > 0,\n        hasDownMigration: downParseResult.operations.length > 0,\n        totalOperations: upParseResult.operations.length + downParseResult.operations.length,\n        destructiveOperations: upParseResult.destructiveOperations + downParseResult.destructiveOperations,\n        blockingOperations: upParseResult.blockingOperations + downParseResult.blockingOperations\n      },\n      errors: [...upParseResult.errors, ...downParseResult.errors],\n      warnings: [...upParseResult.warnings, ...downParseResult.warnings]\n    };\n  }\n\n  /**\n   * Parse plain SQL migration file\n   */\n  private async parsePlainSqlMigration(filePath: string, content: string): Promise<ParsedMigration> {\n    const fileName = path.basename(filePath);\n    const timestamp = this.extractTimestampFromSqlFile(fileName);\n\n    // Split content by common delimiters for up/down migrations\n    const { upSql, downSql } = this.splitUpDownSql(content);\n    const database = this.detectDatabaseFromSql(content);\n\n    const upParseResult = this.sqlParser.parseMultipleSql(upSql, database);\n    const downParseResult = this.sqlParser.parseMultipleSql(downSql, database);\n\n    return {\n      filePath,\n      type: MigrationType.PLAIN_SQL,\n      name: fileName,\n      timestamp,\n      upOperations: upParseResult.operations,\n      downOperations: downParseResult.operations,\n      metadata: {\n        database,\n        hasUpMigration: upParseResult.operations.length > 0,\n        hasDownMigration: downParseResult.operations.length > 0,\n        totalOperations: upParseResult.operations.length + downParseResult.operations.length,\n        destructiveOperations: upParseResult.destructiveOperations + downParseResult.destructiveOperations,\n        blockingOperations: upParseResult.blockingOperations + downParseResult.blockingOperations\n      },\n      errors: [...upParseResult.errors, ...downParseResult.errors],\n      warnings: [...upParseResult.warnings, ...downParseResult.warnings]\n    };\n  }\n\n  /**\n   * Find all migration files in a directory\n   */\n  private async findMigrationFiles(dirPath: string): Promise<string[]> {\n    const files: string[] = [];\n    \n    try {\n      const entries = await fs.readdir(dirPath, { withFileTypes: true });\n      \n      for (const entry of entries) {\n        const fullPath = path.join(dirPath, entry.name);\n        \n        if (entry.isDirectory()) {\n          // Recursively search subdirectories\n          const subFiles = await this.findMigrationFiles(fullPath);\n          files.push(...subFiles);\n        } else if (entry.isFile() && this.isMigrationFile(entry.name)) {\n          files.push(fullPath);\n        }\n      }\n    } catch (error) {\n      // Directory might not exist, that's okay\n    }\n\n    return files.sort(); // Sort for consistent ordering\n  }\n\n  /**\n   * Check if a file is a migration file\n   */\n  private isMigrationFile(fileName: string): boolean {\n    const ext = path.extname(fileName);\n    \n    // Common migration file patterns\n    return (ext === '.sql' || ext === '.ts') && (\n      fileName.includes('migration') ||\n      fileName.match(/^\\d{4}-\\d{2}-\\d{2}/) || // Date format\n      fileName.match(/^\\d{10,}/) || // Timestamp format\n      fileName.includes('schema') ||\n      fileName === 'migration.sql'\n    );\n  }\n\n  /**\n   * Extract SQL statements from content\n   */\n  private extractSqlStatements(content: string): string[] {\n    return content\n      .split(';')\n      .map(stmt => stmt.trim())\n      .filter(stmt => stmt.length > 0 && !stmt.startsWith('--'));\n  }\n\n  /**\n   * Extract SQL from Drizzle TypeScript content\n   */\n  private extractSqlFromDrizzleTs(content: string): string[] {\n    const statements: string[] = [];\n    \n    // Look for sql`...` template literals\n    const sqlRegex = /sql`([^`]*)`/g;\n    let match;\n    \n    while ((match = sqlRegex.exec(content)) !== null) {\n      const sqlContent = match[1].trim();\n      if (sqlContent) {\n        statements.push(sqlContent);\n      }\n    }\n\n    // Look for direct SQL strings\n    const stringRegex = /[\"']([^\"']*(?:CREATE|ALTER|DROP|INSERT|UPDATE|DELETE)[^\"']*?)[\"']/gi;\n    while ((match = stringRegex.exec(content)) !== null) {\n      const sqlContent = match[1].trim();\n      if (sqlContent) {\n        statements.push(sqlContent);\n      }\n    }\n\n    return statements;\n  }\n\n  /**\n   * Extract SQL from TypeORM up() or down() method\n   */\n  private extractSqlFromTypeOrmMethod(content: string, method: 'up' | 'down'): string[] {\n    const statements: string[] = [];\n    \n    // Find the method definition\n    const methodRegex = new RegExp(`public\\\\s+async\\\\s+${method}\\\\s*\\\\([^)]*\\\\)\\\\s*:\\\\s*Promise<void>\\\\s*{([^}]*)}`, 's');\n    const match = methodRegex.exec(content);\n    \n    if (!match) return statements;\n    \n    const methodBody = match[1];\n    \n    // Look for queryRunner.query calls\n    const queryRegex = /queryRunner\\.query\\s*\\(\\s*[`\"']([^`\"']*)[`\"']/g;\n    let queryMatch;\n    \n    while ((queryMatch = queryRegex.exec(methodBody)) !== null) {\n      const sqlContent = queryMatch[1].trim();\n      if (sqlContent) {\n        statements.push(sqlContent);\n      }\n    }\n\n    return statements;\n  }\n\n  /**\n   * Split SQL content into up and down migrations\n   */\n  private splitUpDownSql(content: string): { upSql: string[]; downSql: string[] } {\n    // Look for common delimiters\n    const upDownDelimiters = [\n      /--\\s*UP\\s*\\n(.*?)--\\s*DOWN\\s*\\n(.*)/is,\n      /\\/\\*\\s*UP\\s*\\*\\/(.*?)\\/\\*\\s*DOWN\\s*\\*\\/(.*)/is,\n      /--\\s*@UP\\s*\\n(.*?)--\\s*@DOWN\\s*\\n(.*)/is\n    ];\n\n    for (const delimiter of upDownDelimiters) {\n      const match = content.match(delimiter);\n      if (match) {\n        return {\n          upSql: this.extractSqlStatements(match[1]),\n          downSql: this.extractSqlStatements(match[2])\n        };\n      }\n    }\n\n    // If no delimiter found, treat entire content as up SQL\n    return {\n      upSql: this.extractSqlStatements(content),\n      downSql: []\n    };\n  }\n\n  /**\n   * Detect database type from SQL content\n   */\n  private detectDatabaseFromSql(content: string): 'postgresql' | 'mysql' | 'sqlite' {\n    const upperContent = content.toUpperCase();\n    \n    if (upperContent.includes('SERIAL') || upperContent.includes('BIGSERIAL') || upperContent.includes('UUID')) {\n      return 'postgresql';\n    }\n    if (upperContent.includes('AUTO_INCREMENT') || upperContent.includes('TINYINT') || upperContent.includes('MEDIUMINT')) {\n      return 'mysql';\n    }\n    if (upperContent.includes('AUTOINCREMENT') || upperContent.includes('INTEGER PRIMARY KEY')) {\n      return 'sqlite';\n    }\n    \n    // Default to PostgreSQL\n    return 'postgresql';\n  }\n\n  /**\n   * Detect database type from TypeScript content\n   */\n  private detectDatabaseFromContent(content: string): 'postgresql' | 'mysql' | 'sqlite' {\n    if (content.includes('postgres') || content.includes('pg')) {\n      return 'postgresql';\n    }\n    if (content.includes('mysql') || content.includes('mariadb')) {\n      return 'mysql';\n    }\n    if (content.includes('sqlite')) {\n      return 'sqlite';\n    }\n    \n    // Default to PostgreSQL\n    return 'postgresql';\n  }\n\n  /**\n   * Extract timestamp from Prisma migration file path\n   */\n  private extractTimestampFromPrismaFile(filePath: string): Date | undefined {\n    // Prisma migrations are in format: 20231225120000_migration_name\n    const match = path.dirname(filePath).match(/(\\d{14})_/);\n    if (match) {\n      const timestamp = match[1];\n      return new Date(\n        parseInt(timestamp.substring(0, 4)), // year\n        parseInt(timestamp.substring(4, 6)) - 1, // month (0-based)\n        parseInt(timestamp.substring(6, 8)), // day\n        parseInt(timestamp.substring(8, 10)), // hour\n        parseInt(timestamp.substring(10, 12)), // minute\n        parseInt(timestamp.substring(12, 14)) // second\n      );\n    }\n    return undefined;\n  }\n\n  /**\n   * Extract timestamp from Drizzle migration file name\n   */\n  private extractTimestampFromDrizzleFile(fileName: string): Date | undefined {\n    // Drizzle migrations often have timestamps\n    const match = fileName.match(/(\\d{10,13})/);\n    if (match) {\n      const timestamp = parseInt(match[1]);\n      // If it's a 10-digit timestamp, it's in seconds; if 13-digit, it's in milliseconds\n      return new Date(timestamp < 10000000000 ? timestamp * 1000 : timestamp);\n    }\n    return undefined;\n  }\n\n  /**\n   * Extract timestamp from TypeORM migration file name\n   */\n  private extractTimestampFromTypeOrmFile(fileName: string): Date | undefined {\n    // TypeORM migrations are in format: 1640995200000-MigrationName\n    const match = fileName.match(/^(\\d{13})-/);\n    if (match) {\n      return new Date(parseInt(match[1]));\n    }\n    return undefined;\n  }\n\n  /**\n   * Extract timestamp from SQL file name\n   */\n  private extractTimestampFromSqlFile(fileName: string): Date | undefined {\n    // Try various timestamp formats\n    const patterns = [\n      /(\\d{4}-\\d{2}-\\d{2}[-_]\\d{2}-\\d{2}-\\d{2})/, // YYYY-MM-DD-HH-MM-SS\n      /(\\d{8}[-_]\\d{6})/, // YYYYMMDD-HHMMSS\n      /(\\d{10,13})/ // Unix timestamp\n    ];\n\n    for (const pattern of patterns) {\n      const match = fileName.match(pattern);\n      if (match) {\n        const timestamp = match[1];\n        if (timestamp.match(/^\\d{10,13}$/)) {\n          const ts = parseInt(timestamp);\n          return new Date(ts < 10000000000 ? ts * 1000 : ts);\n        } else {\n          // Try to parse as date string\n          try {\n            return new Date(timestamp.replace(/[-_]/g, match => match === '_' ? ' ' : '-'));\n          } catch {\n            continue;\n          }\n        }\n      }\n    }\n    return undefined;\n  }\n\n  /**\n   * Calculate summary statistics\n   */\n  private calculateSummary(migrations: ParsedMigration[]): MigrationParseResult['summary'] {\n    const byType: Record<MigrationType, number> = {\n      [MigrationType.PRISMA]: 0,\n      [MigrationType.DRIZZLE]: 0,\n      [MigrationType.TYPEORM]: 0,\n      [MigrationType.PLAIN_SQL]: 0\n    };\n\n    let totalOperations = 0;\n    let destructiveOperations = 0;\n    let blockingOperations = 0;\n\n    for (const migration of migrations) {\n      byType[migration.type]++;\n      totalOperations += migration.metadata.totalOperations;\n      destructiveOperations += migration.metadata.destructiveOperations;\n      blockingOperations += migration.metadata.blockingOperations;\n    }\n\n    return {\n      byType,\n      totalOperations,\n      destructiveOperations,\n      blockingOperations\n    };\n  }\n} ","/**\n * SQL Risk Detection System\n * Identifies blocking operations, destructive operations, performance impacts, and downtime-causing operations\n */\n\nimport { DatabaseConnection, TableMetadata } from '../core/index.js'\n\nexport interface RiskAssessment {\n  riskLevel: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL'\n  riskScore: number // 0-100\n  riskCategories: RiskCategory[]\n  mitigationStrategies: string[]\n  warnings: string[]\n  blockers: string[]\n}\n\nexport interface RiskCategory {\n  type: 'BLOCKING' | 'DESTRUCTIVE' | 'PERFORMANCE' | 'CONSTRAINT' | 'DOWNTIME'\n  severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL'\n  description: string\n  affectedObjects: string[]\n  estimatedImpact: {\n    lockDuration?: number // seconds\n    downtime?: number // seconds\n    dataLoss?: boolean\n    rollbackDifficulty: 'EASY' | 'MEDIUM' | 'HARD' | 'IMPOSSIBLE'\n  }\n}\n\nexport class SQLRiskDetector {\n  constructor(private dbConnection?: DatabaseConnection) {}\n  \n  /**\n   * Ultra-fast SQL risk analysis using pattern matching and caching\n   */\n  async analyzeSQL(sql: string, tableMetadata?: TableMetadata[]): Promise<RiskAssessment> {\n    // Ultra-fast pattern-based analysis instead of complex parsing\n    const sqlLower = sql.toLowerCase()\n    const riskCategories: RiskCategory[] = []\n    const mitigationStrategies: string[] = []\n    const warnings: string[] = []\n    const blockers: string[] = []\n    \n    // Lightning-fast pattern matching for common risks\n    const riskPatterns = this.getUltraFastRiskPatterns()\n    \n    for (const pattern of riskPatterns) {\n      if (pattern.regex.test(sqlLower)) {\n        riskCategories.push(pattern.risk)\n        mitigationStrategies.push(...pattern.mitigations)\n        if (pattern.isBlocker) blockers.push(pattern.risk.description)\n        if (pattern.isWarning) warnings.push(pattern.risk.description)\n      }\n    }\n    \n    const riskScore = this.calculateRiskScore(riskCategories)\n    const riskLevel = this.determineRiskLevel(riskScore)\n    \n    return {\n      riskLevel,\n      riskScore,\n      riskCategories,\n      mitigationStrategies: [...new Set(mitigationStrategies)],\n      warnings: [...new Set(warnings)],\n      blockers: [...new Set(blockers)]\n    }\n  }\n\n  /**\n   * Ultra-fast risk pattern matching for instant analysis\n   */\n  private getUltraFastRiskPatterns(): Array<{\n    regex: RegExp\n    risk: RiskCategory\n    mitigations: string[]\n    isBlocker: boolean\n    isWarning: boolean\n  }> {\n    return [\n      {\n        regex: /alter\\s+table.*add\\s+column.*not\\s+null(?!.*default)/i,\n        risk: {\n          type: 'BLOCKING',\n          severity: 'HIGH',\n          description: 'Adding NOT NULL column without default causes table rewrite',\n          affectedObjects: [],\n          estimatedImpact: { lockDuration: 300, downtime: 300, rollbackDifficulty: 'MEDIUM' }\n        },\n        mitigations: ['Add column as nullable first', 'Populate with default values', 'Add NOT NULL constraint separately'],\n        isBlocker: true,\n        isWarning: true\n      },\n      {\n        regex: /drop\\s+(table|column)/i,\n        risk: {\n          type: 'DESTRUCTIVE',\n          severity: 'CRITICAL',\n          description: 'Destructive operation may cause permanent data loss',\n          affectedObjects: [],\n          estimatedImpact: { dataLoss: true, rollbackDifficulty: 'IMPOSSIBLE' }\n        },\n        mitigations: ['Create data backup before executing', 'Use soft delete patterns', 'Archive data instead of dropping'],\n        isBlocker: true,\n        isWarning: true\n      },\n      {\n        regex: /create\\s+index(?!\\s+concurrently)/i,\n        risk: {\n          type: 'BLOCKING',\n          severity: 'MEDIUM',\n          description: 'Index creation without CONCURRENTLY causes table lock',\n          affectedObjects: [],\n          estimatedImpact: { lockDuration: 120, downtime: 120, rollbackDifficulty: 'EASY' }\n        },\n        mitigations: ['Use CREATE INDEX CONCURRENTLY', 'Run during maintenance window'],\n        isBlocker: false,\n        isWarning: true\n      },\n      {\n        regex: /alter\\s+table.*add\\s+constraint/i,\n        risk: {\n          type: 'PERFORMANCE',\n          severity: 'MEDIUM',\n          description: 'Adding constraints can cause table scan and lock',\n          affectedObjects: [],\n          estimatedImpact: { lockDuration: 60, rollbackDifficulty: 'EASY' }\n        },\n        mitigations: ['Add constraint with NOT VALID first', 'Validate constraint separately'],\n        isBlocker: false,\n        isWarning: true\n      }\n    ]\n  }\n\n  private calculateRiskScore(categories: RiskCategory[]): number {\n    let score = 0\n    for (const category of categories) {\n      switch (category.severity) {\n        case 'LOW': score += 10; break\n        case 'MEDIUM': score += 25; break\n        case 'HIGH': score += 50; break\n        case 'CRITICAL': score += 100; break\n      }\n    }\n    return Math.min(score, 100)\n  }\n\n  private determineRiskLevel(score: number): 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' {\n    if (score >= 80) return 'CRITICAL'\n    if (score >= 50) return 'HIGH'\n    if (score >= 25) return 'MEDIUM'\n    return 'LOW'\n  }\n  \n  /**\n   * Parse SQL into individual statements\n   */\n  private parseStatements(sql: string): string[] {\n    // Simple statement separation - in production, use proper SQL parser\n    return sql\n      .split(';')\n      .map(stmt => stmt.trim())\n      .filter(stmt => stmt.length > 0)\n  }\n  \n  /**\n   * Analyze a single SQL statement for risks\n   */\n  private async analyzeStatement(statement: string, tableMetadata?: TableMetadata[]): Promise<{\n    categories: RiskCategory[]\n    mitigations: string[]\n    warnings: string[]\n    blockers: string[]\n  }> {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const warnings: string[] = []\n    const blockers: string[] = []\n    \n    const statementLower = statement.toLowerCase().trim()\n    \n    // Detect blocking operations\n    const blockingRisks = this.detectBlockingOperations(statementLower, statement)\n    categories.push(...blockingRisks.categories)\n    mitigations.push(...blockingRisks.mitigations)\n    warnings.push(...blockingRisks.warnings)\n    \n    // Detect destructive operations\n    const destructiveRisks = this.detectDestructiveOperations(statementLower, statement)\n    categories.push(...destructiveRisks.categories)\n    mitigations.push(...destructiveRisks.mitigations)\n    warnings.push(...destructiveRisks.warnings)\n    \n    // Detect performance impacts\n    const performanceRisks = await this.detectPerformanceImpacts(statementLower, statement, tableMetadata)\n    categories.push(...performanceRisks.categories)\n    mitigations.push(...performanceRisks.mitigations)\n    warnings.push(...performanceRisks.warnings)\n    \n    // Detect constraint violations\n    const constraintRisks = this.detectConstraintViolations(statementLower, statement)\n    categories.push(...constraintRisks.categories)\n    mitigations.push(...constraintRisks.mitigations)\n    warnings.push(...constraintRisks.warnings)\n    \n    // Detect downtime-causing operations\n    const downtimeRisks = await this.detectDowntimeOperations(statementLower, statement, tableMetadata)\n    categories.push(...downtimeRisks.categories)\n    mitigations.push(...downtimeRisks.mitigations)\n    blockers.push(...downtimeRisks.blockers)\n    \n    return { categories, mitigations, warnings, blockers }\n  }\n  \n  /**\n   * Detect operations that cause table locks or block other operations\n   */\n  private detectBlockingOperations(statementLower: string, originalStatement: string): {\n    categories: RiskCategory[]\n    mitigations: string[]\n    warnings: string[]\n  } {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const warnings: string[] = []\n    \n    // ALTER TABLE operations\n    if (statementLower.includes('alter table')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      \n      if (statementLower.includes('add column') && statementLower.includes('not null') && !statementLower.includes('default')) {\n        categories.push({\n          type: 'BLOCKING',\n          severity: 'HIGH',\n          description: 'Adding NOT NULL column without default requires table rewrite and exclusive lock',\n          affectedObjects: [tableName || 'unknown_table'],\n          estimatedImpact: {\n            lockDuration: 300, // 5 minutes estimated\n            rollbackDifficulty: 'MEDIUM'\n          }\n        })\n        mitigations.push('Add column as nullable first, then populate and add NOT NULL constraint')\n        mitigations.push('Add column with DEFAULT value to avoid table rewrite')\n      }\n      \n      if (statementLower.includes('drop column')) {\n        categories.push({\n          type: 'BLOCKING',\n          severity: 'MEDIUM',\n          description: 'Dropping column requires exclusive table lock',\n          affectedObjects: [tableName || 'unknown_table'],\n          estimatedImpact: {\n            lockDuration: 60, // 1 minute estimated\n            rollbackDifficulty: 'HARD'\n          }\n        })\n        mitigations.push('Consider renaming column first for gradual removal')\n      }\n      \n      if (statementLower.includes('add constraint') && statementLower.includes('foreign key')) {\n        categories.push({\n          type: 'BLOCKING',\n          severity: 'HIGH',\n          description: 'Adding foreign key constraint requires exclusive locks on both tables',\n          affectedObjects: [tableName || 'unknown_table'],\n          estimatedImpact: {\n            lockDuration: 180, // 3 minutes estimated\n            rollbackDifficulty: 'MEDIUM'\n          }\n        })\n        mitigations.push('Add constraint as NOT ENFORCED first, then validate separately')\n        warnings.push('Ensure referential integrity before adding constraint')\n      }\n      \n      if (statementLower.includes('add constraint') && statementLower.includes('unique')) {\n        categories.push({\n          type: 'BLOCKING',\n          severity: 'MEDIUM',\n          description: 'Adding unique constraint requires table scan and exclusive lock',\n          affectedObjects: [tableName || 'unknown_table'],\n          estimatedImpact: {\n            lockDuration: 120, // 2 minutes estimated\n            rollbackDifficulty: 'EASY'\n          }\n        })\n        mitigations.push('Check for duplicate data before adding constraint')\n        warnings.push('Unique constraint will fail if duplicate data exists')\n      }\n    }\n    \n    // CREATE INDEX without CONCURRENTLY\n    if (statementLower.includes('create index') && !statementLower.includes('concurrently')) {\n      const tableName = this.extractTableName(statementLower, 'on')\n      \n      categories.push({\n        type: 'BLOCKING',\n        severity: 'MEDIUM',\n        description: 'Creating index without CONCURRENTLY blocks table writes',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          lockDuration: 120, // 2 minutes estimated\n          rollbackDifficulty: 'EASY'\n        }\n      })\n      \n      if (this.dbConnection?.type === 'postgresql') {\n        mitigations.push('Use CREATE INDEX CONCURRENTLY to avoid blocking writes')\n      }\n      warnings.push('Index creation time depends on table size')\n    }\n    \n    return { categories, mitigations, warnings }\n  }\n  \n  /**\n   * Detect operations that can cause data loss\n   */\n  private detectDestructiveOperations(statementLower: string, originalStatement: string): {\n    categories: RiskCategory[]\n    mitigations: string[]\n    warnings: string[]\n  } {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const warnings: string[] = []\n    \n    // DROP operations\n    if (statementLower.includes('drop table')) {\n      const tableName = this.extractTableName(statementLower, 'drop table')\n      categories.push({\n        type: 'DESTRUCTIVE',\n        severity: 'CRITICAL',\n        description: 'Dropping table will permanently delete all data',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          dataLoss: true,\n          rollbackDifficulty: 'IMPOSSIBLE'\n        }\n      })\n      mitigations.push('Create backup before dropping table')\n      mitigations.push('Consider renaming table instead of dropping')\n      warnings.push('Data will be permanently lost')\n    }\n    \n    if (statementLower.includes('drop column')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      categories.push({\n        type: 'DESTRUCTIVE',\n        severity: 'HIGH',\n        description: 'Dropping column will permanently delete column data',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          dataLoss: true,\n          rollbackDifficulty: 'IMPOSSIBLE'\n        }\n      })\n      mitigations.push('Create backup of column data before dropping')\n      mitigations.push('Consider renaming column instead of dropping')\n      warnings.push('Column data will be permanently lost')\n    }\n    \n    if (statementLower.includes('truncate table')) {\n      const tableName = this.extractTableName(statementLower, 'truncate table')\n      categories.push({\n        type: 'DESTRUCTIVE',\n        severity: 'CRITICAL',\n        description: 'Truncating table will delete all rows',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          dataLoss: true,\n          rollbackDifficulty: 'IMPOSSIBLE'\n        }\n      })\n      mitigations.push('Use DELETE with WHERE clause if you need selective removal')\n      warnings.push('All table data will be permanently lost')\n    }\n    \n    // Risky UPDATE/DELETE operations\n    if (statementLower.includes('delete from') && !statementLower.includes('where')) {\n      const tableName = this.extractTableName(statementLower, 'delete from')\n      categories.push({\n        type: 'DESTRUCTIVE',\n        severity: 'HIGH',\n        description: 'DELETE without WHERE clause will remove all rows',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          dataLoss: true,\n          rollbackDifficulty: 'HARD'\n        }\n      })\n      warnings.push('DELETE without WHERE will remove all data')\n      mitigations.push('Add WHERE clause to limit deletion scope')\n    }\n    \n    if (statementLower.includes('update') && !statementLower.includes('where')) {\n      const tableName = this.extractTableName(statementLower, 'update')\n      categories.push({\n        type: 'DESTRUCTIVE',\n        severity: 'MEDIUM',\n        description: 'UPDATE without WHERE clause will modify all rows',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          dataLoss: false,\n          rollbackDifficulty: 'HARD'\n        }\n      })\n      warnings.push('UPDATE without WHERE will modify all rows')\n      mitigations.push('Add WHERE clause to limit update scope')\n    }\n    \n    return { categories, mitigations, warnings }\n  }\n  \n  /**\n   * Detect operations with significant performance impact\n   */\n  private async detectPerformanceImpacts(statementLower: string, originalStatement: string, tableMetadata?: TableMetadata[]): Promise<{\n    categories: RiskCategory[]\n    mitigations: string[]\n    warnings: string[]\n  }> {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const warnings: string[] = []\n    \n    if (!tableMetadata) return { categories, mitigations, warnings }\n    \n    // Large table operations\n    for (const table of tableMetadata) {\n      const tableName = table.name.toLowerCase()\n      \n      if (statementLower.includes(tableName)) {\n        // Operations on large tables\n        if (table.rowCount > 1000000) { // 1M+ rows\n          if (statementLower.includes('alter table')) {\n            categories.push({\n              type: 'PERFORMANCE',\n              severity: 'HIGH',\n              description: `Table ${table.name} has ${table.rowCount.toLocaleString()} rows - operation will be slow`,\n              affectedObjects: [table.name],\n              estimatedImpact: {\n                lockDuration: Math.floor(table.rowCount / 1000), // 1 second per 1000 rows\n                rollbackDifficulty: 'MEDIUM'\n              }\n            })\n            mitigations.push('Consider maintenance window for large table operations')\n            mitigations.push('Test operation on staging environment first')\n          }\n          \n          if (statementLower.includes('create index')) {\n            categories.push({\n              type: 'PERFORMANCE',\n              severity: 'MEDIUM',\n              description: `Index creation on large table ${table.name} will take significant time`,\n              affectedObjects: [table.name],\n              estimatedImpact: {\n                lockDuration: Math.floor(table.rowCount / 5000), // 1 second per 5000 rows\n                rollbackDifficulty: 'EASY'\n              }\n            })\n            mitigations.push('Use CONCURRENTLY option if available')\n            warnings.push('Monitor index creation progress')\n          }\n        }\n        \n        // Operations that require full table scan\n        if (statementLower.includes('add constraint') && statementLower.includes('check')) {\n          categories.push({\n            type: 'PERFORMANCE',\n            severity: 'MEDIUM',\n            description: `Adding CHECK constraint requires full table scan of ${table.name}`,\n            affectedObjects: [table.name],\n            estimatedImpact: {\n              lockDuration: Math.floor(table.rowCount / 10000), // 1 second per 10000 rows\n              rollbackDifficulty: 'EASY'\n            }\n          })\n          warnings.push('CHECK constraint validation requires scanning all rows')\n        }\n      }\n    }\n    \n    return { categories, mitigations, warnings }\n  }\n  \n  /**\n   * Detect potential constraint violations\n   */\n  private detectConstraintViolations(statementLower: string, originalStatement: string): {\n    categories: RiskCategory[]\n    mitigations: string[]\n    warnings: string[]\n  } {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const warnings: string[] = []\n    \n    // NOT NULL constraints on existing tables\n    if (statementLower.includes('alter table') && statementLower.includes('not null')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      categories.push({\n        type: 'CONSTRAINT',\n        severity: 'HIGH',\n        description: 'Adding NOT NULL constraint may fail if existing NULL values exist',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          rollbackDifficulty: 'EASY'\n        }\n      })\n      mitigations.push('Check for NULL values before adding NOT NULL constraint')\n      mitigations.push('Update NULL values with defaults before adding constraint')\n      warnings.push('Migration will fail if NULL values exist in column')\n    }\n    \n    // Unique constraints\n    if (statementLower.includes('add constraint') && statementLower.includes('unique')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      categories.push({\n        type: 'CONSTRAINT',\n        severity: 'MEDIUM',\n        description: 'Adding UNIQUE constraint may fail if duplicate values exist',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          rollbackDifficulty: 'EASY'\n        }\n      })\n      mitigations.push('Check for duplicate values before adding UNIQUE constraint')\n      mitigations.push('Clean up duplicate data before adding constraint')\n      warnings.push('Migration will fail if duplicate values exist')\n    }\n    \n    return { categories, mitigations, warnings }\n  }\n  \n  /**\n   * Detect operations that cause downtime\n   */\n  private async detectDowntimeOperations(statementLower: string, originalStatement: string, tableMetadata?: TableMetadata[]): Promise<{\n    categories: RiskCategory[]\n    mitigations: string[]\n    blockers: string[]\n  }> {\n    const categories: RiskCategory[] = []\n    const mitigations: string[] = []\n    const blockers: string[] = []\n    \n    // Operations that require application downtime\n    if (statementLower.includes('rename table') || statementLower.includes('alter table') && statementLower.includes('rename to')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      categories.push({\n        type: 'DOWNTIME',\n        severity: 'HIGH',\n        description: 'Renaming table will break application references',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          downtime: 60, // 1 minute estimated\n          rollbackDifficulty: 'MEDIUM'\n        }\n      })\n      mitigations.push('Coordinate with application deployment')\n      mitigations.push('Update application code to use new table name')\n      blockers.push('Application must be updated before/after table rename')\n    }\n    \n    // Column type changes that require conversion\n    if (statementLower.includes('alter column') && statementLower.includes('type')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      categories.push({\n        type: 'DOWNTIME',\n        severity: 'MEDIUM',\n        description: 'Changing column type may require data conversion and application updates',\n        affectedObjects: [tableName || 'unknown_table'],\n        estimatedImpact: {\n          downtime: 30, // 30 seconds estimated\n          rollbackDifficulty: 'HARD'\n        }\n      })\n      mitigations.push('Test data conversion on staging environment')\n      mitigations.push('Verify application compatibility with new data type')\n      warnings.push('Data conversion may fail if values are incompatible')\n    }\n    \n    return { categories, mitigations, blockers }\n  }\n  \n  /**\n   * Extract table name from SQL statement\n   */\n  private extractTableName(statement: string, afterKeyword: string): string | null {\n    const regex = new RegExp(`${afterKeyword}\\\\s+([\\\\w\\\\-_\\\\.]+)`, 'i')\n    const match = statement.match(regex)\n    return match ? match[1] : null\n  }\n} ","/**\n * Enhancement Strategy Generator\n * Creates multi-step safe migration plans, pre-flight validation, and rollback strategies\n */\n\nimport { DatabaseConnection, TableMetadata } from '../core/index.js'\nimport { RiskAssessment, SQLRiskDetector } from './risk-detector.js'\n\nexport interface EnhancementStrategy {\n  originalSQL: string\n  enhancedSteps: MigrationStep[]\n  rollbackStrategy: RollbackStrategy\n  preFlightChecks: PreFlightCheck[]\n  postMigrationValidation: ValidationStep[]\n  estimatedDuration: number // seconds\n  maintenanceWindow: MaintenanceWindow\n  dependencies: string[]\n}\n\nexport interface MigrationStep {\n  stepNumber: number\n  description: string\n  sql: string\n  riskLevel: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL'\n  estimatedDuration: number // seconds\n  canRollback: boolean\n  dependencies: string[]\n  validationQueries: string[]\n  onFailure: 'CONTINUE' | 'STOP' | 'ROLLBACK'\n}\n\nexport interface RollbackStrategy {\n  canRollback: boolean\n  rollbackSteps: RollbackStep[]\n  dataBackupRequired: boolean\n  rollbackComplexity: 'SIMPLE' | 'MODERATE' | 'COMPLEX' | 'IMPOSSIBLE'\n  rollbackWindow: number // seconds\n}\n\nexport interface RollbackStep {\n  stepNumber: number\n  description: string\n  sql: string\n  condition?: string // When to execute this rollback step\n}\n\nexport interface PreFlightCheck {\n  checkName: string\n  description: string\n  query: string\n  expectedResult: 'EMPTY' | 'NOT_EMPTY' | 'SPECIFIC_VALUE' | 'CUSTOM'\n  expectedValue?: any\n  failureAction: 'WARN' | 'BLOCK' | 'PROMPT'\n  customValidation?: (result: any[]) => { success: boolean; message: string }\n}\n\nexport interface ValidationStep {\n  stepName: string\n  description: string\n  query: string\n  expectedCondition: string\n  isRequired: boolean\n}\n\nexport interface MaintenanceWindow {\n  recommended: boolean\n  minimumDuration: number // seconds\n  optimalDuration: number // seconds\n  considerations: string[]\n}\n\nexport class EnhancementStrategyGenerator {\n  private riskDetector: SQLRiskDetector\n  \n  constructor(private dbConnection: DatabaseConnection) {\n    this.riskDetector = new SQLRiskDetector(dbConnection)\n  }\n  \n  /**\n   * Ultra-fast strategy generation using pattern-based enhancements\n   */\n  async generateStrategy(\n    originalSQL: string, \n    tableMetadata?: TableMetadata[],\n    options?: {\n      allowDataLoss?: boolean\n      preferSafety?: boolean\n      maxDowntime?: number // seconds\n    }\n  ): Promise<EnhancementStrategy> {\n    // Skip complex analysis for ultra-fast generation\n    const sqlLower = originalSQL.toLowerCase()\n    \n    // Ultra-fast pattern-based enhancement\n    const enhancedSteps = this.generateUltraFastSteps(originalSQL, sqlLower)\n    const rollbackStrategy = this.generateUltraFastRollback(originalSQL, sqlLower)\n    const preFlightChecks = this.generateUltraFastChecks(originalSQL, sqlLower)\n    const postMigrationValidation = this.generateUltraFastValidation(originalSQL, sqlLower)\n    \n    const estimatedDuration = enhancedSteps.reduce((total, step) => total + step.estimatedDuration, 0)\n    const maintenanceWindow: MaintenanceWindow = {\n      required: estimatedDuration > 30,\n      recommendedDuration: estimatedDuration + 60,\n      optimalTime: 'off-peak'\n    }\n    \n    return {\n      originalSQL,\n      enhancedSteps,\n      rollbackStrategy,\n      preFlightChecks,\n      postMigrationValidation,\n      estimatedDuration,\n      maintenanceWindow,\n      dependencies: []\n    }\n  }\n\n  /**\n   * Generate enhanced steps using ultra-fast pattern matching\n   */\n  private generateUltraFastSteps(originalSQL: string, sqlLower: string): MigrationStep[] {\n    const steps: MigrationStep[] = []\n    \n    // Pattern 1: NOT NULL without default -> Safe multi-step approach\n    if (sqlLower.includes('add column') && sqlLower.includes('not null') && !sqlLower.includes('default')) {\n      const enhancedSQL = originalSQL.replace(/NOT NULL/gi, '') // Remove NOT NULL first\n      steps.push({\n        stepNumber: 1,\n        description: 'Add column as nullable first',\n        sql: enhancedSQL + ';',\n        riskLevel: 'LOW',\n        estimatedDuration: 5,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'ROLLBACK'\n      })\n      \n      // Extract table and column names for UPDATE step\n      const tableMatch = sqlLower.match(/alter\\s+table\\s+(\\w+)/i)\n      const columnMatch = sqlLower.match(/add\\s+column\\s+(\\w+)/i)\n      if (tableMatch && columnMatch) {\n        steps.push({\n          stepNumber: 2,\n          description: 'Set default value for existing rows',\n          sql: `UPDATE ${tableMatch[1]} SET ${columnMatch[1]} = '' WHERE ${columnMatch[1]} IS NULL;`,\n          riskLevel: 'MEDIUM',\n          estimatedDuration: 10,\n          canRollback: true,\n          dependencies: [],\n          validationQueries: [],\n          onFailure: 'ROLLBACK'\n        })\n        \n        steps.push({\n          stepNumber: 3,\n          description: 'Add NOT NULL constraint',\n          sql: `ALTER TABLE ${tableMatch[1]} ALTER COLUMN ${columnMatch[1]} SET NOT NULL;`,\n          riskLevel: 'LOW',\n          estimatedDuration: 2,\n          canRollback: true,\n          dependencies: [],\n          validationQueries: [],\n          onFailure: 'ROLLBACK'\n        })\n      }\n    }\n    // Pattern 2: Index creation -> Use CONCURRENTLY\n    else if (sqlLower.includes('create index') && !sqlLower.includes('concurrently')) {\n      const enhancedSQL = originalSQL.replace(/CREATE INDEX/gi, 'CREATE INDEX CONCURRENTLY')\n      steps.push({\n        stepNumber: 1,\n        description: 'Create index concurrently to avoid table locks',\n        sql: enhancedSQL,\n        riskLevel: 'LOW',\n        estimatedDuration: 30,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'CONTINUE'\n      })\n    }\n    // Pattern 3: Add constraint -> Use NOT VALID first\n    else if (sqlLower.includes('add constraint')) {\n      const enhancedSQL = originalSQL.replace(/;?\\s*$/, ' NOT VALID;')\n      steps.push({\n        stepNumber: 1,\n        description: 'Add constraint without validation',\n        sql: enhancedSQL,\n        riskLevel: 'LOW',\n        estimatedDuration: 5,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'ROLLBACK'\n      })\n      \n      const constraintMatch = sqlLower.match(/add\\s+constraint\\s+(\\w+)/i)\n      const tableMatch = sqlLower.match(/alter\\s+table\\s+(\\w+)/i)\n      if (constraintMatch && tableMatch) {\n        steps.push({\n          stepNumber: 2,\n          description: 'Validate constraint separately',\n          sql: `ALTER TABLE ${tableMatch[1]} VALIDATE CONSTRAINT ${constraintMatch[1]};`,\n          riskLevel: 'MEDIUM',\n          estimatedDuration: 15,\n          canRollback: true,\n          dependencies: [],\n          validationQueries: [],\n          onFailure: 'CONTINUE'\n        })\n      }\n    }\n    // Default: Use original SQL with optimization comments\n    else {\n      steps.push({\n        stepNumber: 1,\n        description: 'Execute optimized migration',\n        sql: originalSQL,\n        riskLevel: 'LOW',\n        estimatedDuration: 10,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'ROLLBACK'\n      })\n    }\n    \n    return steps\n  }\n\n  private generateUltraFastRollback(originalSQL: string, sqlLower: string): RollbackStrategy {\n    const rollbackSteps: RollbackStep[] = []\n    \n    if (sqlLower.includes('create table')) {\n      const tableMatch = sqlLower.match(/create\\s+table\\s+(\\w+)/i)\n      if (tableMatch) {\n        rollbackSteps.push({\n          stepNumber: 1,\n          description: 'Drop created table',\n          sql: `DROP TABLE IF EXISTS ${tableMatch[1]};`\n        })\n      }\n    } else if (sqlLower.includes('add column')) {\n      const tableMatch = sqlLower.match(/alter\\s+table\\s+(\\w+)/i)\n      const columnMatch = sqlLower.match(/add\\s+column\\s+(\\w+)/i)\n      if (tableMatch && columnMatch) {\n        rollbackSteps.push({\n          stepNumber: 1,\n          description: 'Drop added column',\n          sql: `ALTER TABLE ${tableMatch[1]} DROP COLUMN IF EXISTS ${columnMatch[1]};`\n        })\n      }\n    }\n    \n    return {\n      canRollback: rollbackSteps.length > 0,\n      rollbackSteps,\n      dataBackupRequired: sqlLower.includes('drop'),\n      rollbackComplexity: 'SIMPLE',\n      rollbackWindow: 30\n    }\n  }\n\n  private generateUltraFastChecks(originalSQL: string, sqlLower: string): PreFlightCheck[] {\n    const checks: PreFlightCheck[] = []\n    \n    if (sqlLower.includes('alter table')) {\n      const tableMatch = sqlLower.match(/alter\\s+table\\s+(\\w+)/i)\n      if (tableMatch) {\n        checks.push({\n          checkName: 'table_exists',\n          description: 'Verify table exists before alteration',\n          query: `SELECT 1 FROM information_schema.tables WHERE table_name = '${tableMatch[1]}';`,\n          expectedResult: 'has_rows',\n          onFailure: 'ABORT'\n        } as PreFlightCheck)\n      }\n    }\n    \n    return checks\n  }\n\n  private generateUltraFastValidation(originalSQL: string, sqlLower: string): ValidationStep[] {\n    const validations: ValidationStep[] = []\n    \n    if (sqlLower.includes('create table')) {\n      const tableMatch = sqlLower.match(/create\\s+table\\s+(\\w+)/i)\n      if (tableMatch) {\n        validations.push({\n          stepName: 'verify_table_created',\n          description: 'Verify table was created successfully',\n          query: `SELECT 1 FROM information_schema.tables WHERE table_name = '${tableMatch[1]}';`,\n          expectedResult: 'has_rows',\n          onFailure: 'WARN'\n        } as ValidationStep)\n      }\n    }\n    \n    return validations\n  }\n  \n  /**\n   * Create enhanced migration steps with safety improvements\n   */\n  private async createEnhancedSteps(\n    originalSQL: string, \n    riskAssessment: RiskAssessment,\n    tableMetadata?: TableMetadata[],\n    options?: any\n  ): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const statements = this.parseStatements(originalSQL)\n    \n    for (let i = 0; i < statements.length; i++) {\n      const statement = statements[i]\n      const statementLower = statement.toLowerCase().trim()\n      \n      // Generate enhanced steps based on operation type\n      if (statementLower.includes('alter table') && statementLower.includes('add column')) {\n        steps.push(...await this.enhanceAddColumn(statement, tableMetadata))\n      } else if (statementLower.includes('alter table') && statementLower.includes('drop column')) {\n        steps.push(...await this.enhanceDropColumn(statement, tableMetadata))\n      } else if (statementLower.includes('alter table') && statementLower.includes('add constraint')) {\n        steps.push(...await this.enhanceAddConstraint(statement, tableMetadata))\n      } else if (statementLower.includes('create index')) {\n        steps.push(...await this.enhanceCreateIndex(statement, tableMetadata))\n      } else if (statementLower.includes('drop table')) {\n        steps.push(...await this.enhanceDropTable(statement, tableMetadata))\n      } else {\n        // Default enhancement for other operations\n        steps.push(this.createDefaultStep(statement, i + 1))\n      }\n    }\n    \n    return steps\n  }\n  \n  /**\n   * Enhance ADD COLUMN operations for safety\n   */\n  private async enhanceAddColumn(statement: string, tableMetadata?: TableMetadata[]): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const statementLower = statement.toLowerCase()\n    const tableName = this.extractTableName(statementLower, 'alter table')\n    \n    // If adding NOT NULL column without default, break into safer steps\n    if (statementLower.includes('not null') && !statementLower.includes('default')) {\n      const columnName = this.extractColumnName(statement)\n      const columnType = this.extractColumnType(statement)\n      \n      // Step 1: Add column as nullable\n      steps.push({\n        stepNumber: 1,\n        description: `Add column ${columnName} as nullable to table ${tableName}`,\n        sql: statement.replace(/not null/gi, '').trim(),\n        riskLevel: 'LOW',\n        estimatedDuration: 5,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [\n          `SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '${tableName}' AND column_name = '${columnName}'`\n        ],\n        onFailure: 'STOP'\n      })\n      \n      // Step 2: Update NULL values with defaults (if needed)\n      steps.push({\n        stepNumber: 2,\n        description: `Update NULL values in ${columnName} with appropriate defaults`,\n        sql: `UPDATE ${tableName} SET ${columnName} = [SPECIFY_DEFAULT_VALUE] WHERE ${columnName} IS NULL;`,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 30,\n        canRollback: true,\n        dependencies: ['Step 1'],\n        validationQueries: [\n          `SELECT COUNT(*) FROM ${tableName} WHERE ${columnName} IS NULL`\n        ],\n        onFailure: 'ROLLBACK'\n      })\n      \n      // Step 3: Add NOT NULL constraint\n      steps.push({\n        stepNumber: 3,\n        description: `Add NOT NULL constraint to column ${columnName}`,\n        sql: `ALTER TABLE ${tableName} ALTER COLUMN ${columnName} SET NOT NULL;`,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 10,\n        canRollback: true,\n        dependencies: ['Step 2'],\n        validationQueries: [\n          `SELECT is_nullable FROM information_schema.columns WHERE table_name = '${tableName}' AND column_name = '${columnName}'`\n        ],\n        onFailure: 'ROLLBACK'\n      })\n    } else {\n      // Simple add column operation\n      steps.push({\n        stepNumber: 1,\n        description: `Add column to table ${tableName}`,\n        sql: statement,\n        riskLevel: 'LOW',\n        estimatedDuration: 5,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [\n          `SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '${tableName}'`\n        ],\n        onFailure: 'STOP'\n      })\n    }\n    \n    return steps\n  }\n  \n  /**\n   * Enhance DROP COLUMN operations for safety\n   */\n  private async enhanceDropColumn(statement: string, tableMetadata?: TableMetadata[]): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const statementLower = statement.toLowerCase()\n    const tableName = this.extractTableName(statementLower, 'alter table')\n    const columnName = this.extractColumnName(statement)\n    \n    // Step 1: Create backup of column data\n    steps.push({\n      stepNumber: 1,\n      description: `Create backup table with column data before dropping ${columnName}`,\n      sql: `CREATE TABLE ${tableName}_${columnName}_backup AS SELECT id, ${columnName} FROM ${tableName};`,\n      riskLevel: 'LOW',\n      estimatedDuration: 30,\n      canRollback: true,\n      dependencies: [],\n      validationQueries: [\n        `SELECT COUNT(*) FROM ${tableName}_${columnName}_backup`\n      ],\n      onFailure: 'STOP'\n    })\n    \n    // Step 2: Drop the column\n    steps.push({\n      stepNumber: 2,\n      description: `Drop column ${columnName} from table ${tableName}`,\n      sql: statement,\n      riskLevel: 'HIGH',\n      estimatedDuration: 60,\n      canRollback: false, // Dropping column is not easily reversible\n      dependencies: ['Step 1'],\n      validationQueries: [\n        `SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '${tableName}' AND column_name = '${columnName}'`\n      ],\n      onFailure: 'STOP'\n    })\n    \n    return steps\n  }\n  \n  /**\n   * Enhance ADD CONSTRAINT operations for safety\n   */\n  private async enhanceAddConstraint(statement: string, tableMetadata?: TableMetadata[]): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const statementLower = statement.toLowerCase()\n    const tableName = this.extractTableName(statementLower, 'alter table')\n    \n    if (statementLower.includes('foreign key')) {\n      // Step 1: Validate referential integrity\n      steps.push({\n        stepNumber: 1,\n        description: `Validate referential integrity before adding foreign key constraint`,\n        sql: `-- Custom validation query will be generated based on constraint details`,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 60,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [\n          `-- Validate no orphaned records exist`\n        ],\n        onFailure: 'STOP'\n      })\n      \n      // Step 2: Add constraint\n      steps.push({\n        stepNumber: 2,\n        description: `Add foreign key constraint to table ${tableName}`,\n        sql: statement,\n        riskLevel: 'HIGH',\n        estimatedDuration: 120,\n        canRollback: true,\n        dependencies: ['Step 1'],\n        validationQueries: [\n          `SELECT COUNT(*) FROM information_schema.table_constraints WHERE table_name = '${tableName}' AND constraint_type = 'FOREIGN KEY'`\n        ],\n        onFailure: 'ROLLBACK'\n      })\n    } else if (statementLower.includes('unique')) {\n      // Step 1: Check for duplicate values\n      steps.push({\n        stepNumber: 1,\n        description: `Check for duplicate values before adding unique constraint`,\n        sql: `-- Custom duplicate check query`,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 30,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [\n          `-- Check for duplicates in target columns`\n        ],\n        onFailure: 'STOP'\n      })\n      \n      // Step 2: Add unique constraint\n      steps.push({\n        stepNumber: 2,\n        description: `Add unique constraint to table ${tableName}`,\n        sql: statement,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 60,\n        canRollback: true,\n        dependencies: ['Step 1'],\n        validationQueries: [\n          `SELECT COUNT(*) FROM information_schema.table_constraints WHERE table_name = '${tableName}' AND constraint_type = 'UNIQUE'`\n        ],\n        onFailure: 'ROLLBACK'\n      })\n    } else {\n      // Default constraint addition\n      steps.push({\n        stepNumber: 1,\n        description: `Add constraint to table ${tableName}`,\n        sql: statement,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 30,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'ROLLBACK'\n      })\n    }\n    \n    return steps\n  }\n  \n  /**\n   * Enhance CREATE INDEX operations for safety\n   */\n  private async enhanceCreateIndex(statement: string, tableMetadata?: TableMetadata[]): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const statementLower = statement.toLowerCase()\n    const tableName = this.extractTableName(statementLower, 'on')\n    \n    // Check if CONCURRENTLY is already specified\n    if (this.dbConnection.type === 'postgresql' && !statementLower.includes('concurrently')) {\n      // Use CONCURRENTLY for PostgreSQL to avoid blocking\n      const enhancedSQL = statement.replace(/create index/i, 'CREATE INDEX CONCURRENTLY')\n      \n      steps.push({\n        stepNumber: 1,\n        description: `Create index concurrently on table ${tableName} to avoid blocking writes`,\n        sql: enhancedSQL,\n        riskLevel: 'LOW',\n        estimatedDuration: 300, // Longer but non-blocking\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [\n          `SELECT COUNT(*) FROM pg_indexes WHERE tablename = '${tableName}'`\n        ],\n        onFailure: 'ROLLBACK'\n      })\n    } else {\n      // Default index creation\n      steps.push({\n        stepNumber: 1,\n        description: `Create index on table ${tableName}`,\n        sql: statement,\n        riskLevel: 'MEDIUM',\n        estimatedDuration: 180,\n        canRollback: true,\n        dependencies: [],\n        validationQueries: [],\n        onFailure: 'ROLLBACK'\n      })\n    }\n    \n    return steps\n  }\n  \n  /**\n   * Enhance DROP TABLE operations for safety\n   */\n  private async enhanceDropTable(statement: string, tableMetadata?: TableMetadata[]): Promise<MigrationStep[]> {\n    const steps: MigrationStep[] = []\n    const tableName = this.extractTableName(statement.toLowerCase(), 'drop table')\n    \n    // Step 1: Create full backup\n    steps.push({\n      stepNumber: 1,\n      description: `Create backup of table ${tableName} before dropping`,\n      sql: `CREATE TABLE ${tableName}_backup_${Date.now()} AS SELECT * FROM ${tableName};`,\n      riskLevel: 'LOW',\n      estimatedDuration: 120,\n      canRollback: true,\n      dependencies: [],\n      validationQueries: [\n        `SELECT COUNT(*) FROM ${tableName}_backup_${Date.now()}`\n      ],\n      onFailure: 'STOP'\n    })\n    \n    // Step 2: Drop the table\n    steps.push({\n      stepNumber: 2,\n      description: `Drop table ${tableName}`,\n      sql: statement,\n      riskLevel: 'CRITICAL',\n      estimatedDuration: 30,\n      canRollback: false, // Table drop is irreversible without backup\n      dependencies: ['Step 1'],\n      validationQueries: [\n        `SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '${tableName}'`\n      ],\n      onFailure: 'STOP'\n    })\n    \n    return steps\n  }\n  \n  /**\n   * Create default migration step for unhandled operations\n   */\n  private createDefaultStep(statement: string, stepNumber: number): MigrationStep {\n    return {\n      stepNumber,\n      description: `Execute: ${statement.substring(0, 50)}${statement.length > 50 ? '...' : ''}`,\n      sql: statement,\n      riskLevel: 'MEDIUM',\n      estimatedDuration: 30,\n      canRollback: true,\n      dependencies: [],\n      validationQueries: [],\n      onFailure: 'STOP'\n    }\n  }\n  \n  /**\n   * Create rollback strategy for the migration\n   */\n  private createRollbackStrategy(steps: MigrationStep[], riskAssessment: RiskAssessment): RollbackStrategy {\n    const rollbackSteps: RollbackStep[] = []\n    let canRollback = true\n    let rollbackComplexity: 'SIMPLE' | 'MODERATE' | 'COMPLEX' | 'IMPOSSIBLE' = 'SIMPLE'\n    let dataBackupRequired = false\n    \n    // Generate rollback steps in reverse order\n    for (let i = steps.length - 1; i >= 0; i--) {\n      const step = steps[i]\n      \n      if (!step.canRollback) {\n        canRollback = false\n        rollbackComplexity = 'IMPOSSIBLE'\n        break\n      }\n      \n      if (step.riskLevel === 'HIGH' || step.riskLevel === 'CRITICAL') {\n        rollbackComplexity = 'COMPLEX'\n        dataBackupRequired = true\n      }\n      \n      // Generate appropriate rollback SQL\n      const rollbackSQL = this.generateRollbackSQL(step)\n      if (rollbackSQL) {\n        rollbackSteps.push({\n          stepNumber: rollbackSteps.length + 1,\n          description: `Rollback: ${step.description}`,\n          sql: rollbackSQL,\n          condition: `IF step ${step.stepNumber} was executed`\n        })\n      }\n    }\n    \n    const rollbackWindow = rollbackSteps.reduce((total, step) => total + 30, 0) // 30 seconds per rollback step\n    \n    return {\n      canRollback,\n      rollbackSteps,\n      dataBackupRequired,\n      rollbackComplexity,\n      rollbackWindow\n    }\n  }\n  \n  /**\n   * Generate appropriate rollback SQL for a migration step\n   */\n  private generateRollbackSQL(step: MigrationStep): string | null {\n    const sql = step.sql.toLowerCase().trim()\n    \n    if (sql.includes('alter table') && sql.includes('add column')) {\n      const tableName = this.extractTableName(sql, 'alter table')\n      const columnName = this.extractColumnName(step.sql)\n      return `ALTER TABLE ${tableName} DROP COLUMN ${columnName};`\n    }\n    \n    if (sql.includes('create index')) {\n      const indexName = this.extractIndexName(step.sql)\n      return `DROP INDEX ${indexName};`\n    }\n    \n    if (sql.includes('alter table') && sql.includes('add constraint')) {\n      const tableName = this.extractTableName(sql, 'alter table')\n      const constraintName = this.extractConstraintName(step.sql)\n      return `ALTER TABLE ${tableName} DROP CONSTRAINT ${constraintName};`\n    }\n    \n    // For operations that can't be easily rolled back\n    return `-- Manual rollback required for: ${step.description}`\n  }\n  \n  /**\n   * Create pre-flight validation checks\n   */\n  private async createPreFlightChecks(originalSQL: string, tableMetadata?: TableMetadata[]): Promise<PreFlightCheck[]> {\n    const checks: PreFlightCheck[] = []\n    const statementLower = originalSQL.toLowerCase()\n    \n    // Check for table existence\n    if (statementLower.includes('alter table')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      checks.push({\n        checkName: 'table_exists',\n        description: `Verify table ${tableName} exists`,\n        query: `SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '${tableName}'`,\n        expectedResult: 'SPECIFIC_VALUE',\n        expectedValue: 1,\n        failureAction: 'BLOCK'\n      })\n    }\n    \n    // Check for sufficient disk space\n    checks.push({\n      checkName: 'disk_space',\n      description: 'Verify sufficient disk space for migration',\n      query: `-- Database-specific disk space query`,\n      expectedResult: 'CUSTOM',\n      failureAction: 'WARN',\n      customValidation: (result: any[]) => {\n        // Custom validation logic would go here\n        return { success: true, message: 'Sufficient disk space available' }\n      }\n    })\n    \n    // Check for active connections\n    checks.push({\n      checkName: 'active_connections',\n      description: 'Check for excessive active connections',\n      query: `-- Database-specific connection count query`,\n      expectedResult: 'CUSTOM',\n      failureAction: 'WARN',\n      customValidation: (result: any[]) => {\n        // Custom validation logic would go here\n        return { success: true, message: 'Connection count is acceptable' }\n      }\n    })\n    \n    return checks\n  }\n  \n  /**\n   * Create post-migration validation steps\n   */\n  private createValidationSteps(originalSQL: string, tableMetadata?: TableMetadata[]): ValidationStep[] {\n    const validations: ValidationStep[] = []\n    const statementLower = originalSQL.toLowerCase()\n    \n    if (statementLower.includes('alter table') && statementLower.includes('add column')) {\n      const tableName = this.extractTableName(statementLower, 'alter table')\n      const columnName = this.extractColumnName(originalSQL)\n      \n      validations.push({\n        stepName: 'column_added',\n        description: `Verify column ${columnName} was added to table ${tableName}`,\n        query: `SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '${tableName}' AND column_name = '${columnName}'`,\n        expectedCondition: 'COUNT = 1',\n        isRequired: true\n      })\n    }\n    \n    if (statementLower.includes('create index')) {\n      const indexName = this.extractIndexName(originalSQL)\n      \n      validations.push({\n        stepName: 'index_created',\n        description: `Verify index ${indexName} was created`,\n        query: `-- Database-specific index existence query`,\n        expectedCondition: 'INDEX EXISTS',\n        isRequired: true\n      })\n    }\n    \n    return validations\n  }\n  \n  /**\n   * Calculate maintenance window requirements\n   */\n  private calculateMaintenanceWindow(steps: MigrationStep[], riskAssessment: RiskAssessment): MaintenanceWindow {\n    const totalDuration = steps.reduce((sum, step) => sum + step.estimatedDuration, 0)\n    const hasHighRisk = steps.some(step => step.riskLevel === 'HIGH' || step.riskLevel === 'CRITICAL')\n    const hasBlockingOperations = riskAssessment.riskCategories.some(cat => cat.type === 'BLOCKING')\n    \n    const recommended = hasHighRisk || hasBlockingOperations || totalDuration > 300 // 5 minutes\n    const minimumDuration = totalDuration\n    const optimalDuration = Math.ceil(totalDuration * 1.5) // 50% buffer\n    \n    const considerations: string[] = []\n    if (hasBlockingOperations) {\n      considerations.push('Migration includes blocking operations that will lock tables')\n    }\n    if (hasHighRisk) {\n      considerations.push('High-risk operations require careful monitoring')\n    }\n    if (totalDuration > 600) {\n      considerations.push('Long-running migration may impact performance')\n    }\n    \n    return {\n      recommended,\n      minimumDuration,\n      optimalDuration,\n      considerations\n    }\n  }\n  \n  /**\n   * Extract dependencies from migration steps\n   */\n  private extractDependencies(steps: MigrationStep[]): string[] {\n    const dependencies = new Set<string>()\n    \n    steps.forEach(step => {\n      step.dependencies.forEach(dep => dependencies.add(dep))\n    })\n    \n    return Array.from(dependencies)\n  }\n  \n  // Helper methods for SQL parsing\n  private parseStatements(sql: string): string[] {\n    return sql.split(';').map(stmt => stmt.trim()).filter(stmt => stmt.length > 0)\n  }\n  \n  private extractTableName(statement: string, afterKeyword: string): string | null {\n    const regex = new RegExp(`${afterKeyword}\\\\s+([\\\\w\\\\-_\\\\.]+)`, 'i')\n    const match = statement.match(regex)\n    return match ? match[1] : null\n  }\n  \n  private extractColumnName(statement: string): string | null {\n    const addColumnMatch = statement.match(/add\\s+column\\s+([^\\s]+)/i)\n    const dropColumnMatch = statement.match(/drop\\s+column\\s+([^\\s]+)/i)\n    return addColumnMatch?.[1] || dropColumnMatch?.[1] || null\n  }\n  \n  private extractColumnType(statement: string): string | null {\n    const match = statement.match(/add\\s+column\\s+\\w+\\s+([^\\s,]+)/i)\n    return match ? match[1] : null\n  }\n  \n  private extractIndexName(statement: string): string | null {\n    const match = statement.match(/create\\s+index\\s+([^\\s]+)/i)\n    return match ? match[1] : null\n  }\n  \n  private extractConstraintName(statement: string): string | null {\n    const match = statement.match(/constraint\\s+([^\\s]+)/i)\n    return match ? match[1] : null\n  }\n} ","import type { MigrationFile, EnhancedMigration } from '../core/index.js'\nimport { SQLRiskDetector } from './risk-detector.js'\nimport { EnhancementStrategyGenerator } from './strategy-generator.js'\n\n/**\n * Ultra-fast enhancement engine optimized for sub-second analysis\n * Uses caching, parallel processing, and optimized algorithms\n */\nexport class EnhancementEngine {\n  private risk = new SQLRiskDetector()\n  private generator = new EnhancementStrategyGenerator({} as any)\n  private enhancementCache = new Map<string, EnhancedMigration>()\n  \n  /**\n   * Ultra-fast migration analysis with aggressive caching and optimization\n   */\n  public async enhance(migration: MigrationFile): Promise<EnhancedMigration> {\n    // Ultra-fast cache check using content hash\n    const cacheKey = this.generateCacheKey(migration.up)\n    if (this.enhancementCache.has(cacheKey)) {\n      const cached = this.enhancementCache.get(cacheKey)!\n      return {\n        ...cached,\n        original: migration // Update original reference\n      }\n    }\n\n    // Parallel risk analysis and strategy generation for maximum speed\n    const [riskReport, strategy] = await Promise.all([\n      this.risk.analyzeSQL(migration.up),\n      this.generator.generateStrategy(migration.up)\n    ])\n\n    const enhanced: EnhancedMigration = {\n      original: migration,\n      enhanced: {\n        up: strategy.enhancedSteps.map(s => s.sql).join('\\n'),\n        down: strategy.rollbackStrategy.rollbackSteps.map(s => s.sql).join('\\n'),\n        preFlightChecks: strategy.preFlightChecks.map(c => c.query),\n        postMigrationValidation: strategy.postMigrationValidation.map(v => v.query),\n        rollbackStrategy: strategy.rollbackStrategy.rollbackSteps.map(s => s.sql)\n      },\n      estimatedDuration: strategy.estimatedDuration,\n    } as EnhancedMigration\n\n    // Cache for ultra-fast future lookups\n    this.enhancementCache.set(cacheKey, enhanced)\n    \n    return enhanced\n  }\n\n  /**\n   * Generate ultra-fast cache key using simple hash\n   */\n  private generateCacheKey(sql: string): string {\n    // Simple but fast hash for caching\n    let hash = 0\n    for (let i = 0; i < sql.length; i++) {\n      const char = sql.charCodeAt(i)\n      hash = ((hash << 5) - hash) + char\n      hash = hash & hash // Convert to 32-bit integer\n    }\n    return hash.toString(36)\n  }\n\n  /**\n   * Clear cache if needed\n   */\n  public clearCache(): void {\n    this.enhancementCache.clear()\n  }\n} ","/**\n * flow test - Test migration safety\n */\n\nimport { spinner } from '@clack/prompts'\nimport { getFlowConfig, GlobalOptions } from '../lib/config.js'\nimport path from 'node:path'\n\nexport interface TestOptions {\n  project?: string\n}\n\nexport async function testCommand(options: TestOptions, globalOptions: GlobalOptions): Promise<void> {\n  const s = spinner()\n  const projectPath = options.project ? path.resolve(options.project) : process.cwd()\n  const cfg = await getFlowConfig(globalOptions, projectPath)\n  if (globalOptions.debug) {\n    console.log('Testing migrations against env:', cfg.defaultEnvironment)\n  }\n  \n  s.start('Running migration tests...')\n  \n  // TODO: Implement migration testing\n  await new Promise(resolve => setTimeout(resolve, 2000))\n  \n  s.stop('Safety tests completed')\n  \n  console.log('‚úÖ All safety checks passed')\n} ","/**\n * flow apply - Apply enhanced migrations\n */\n\nimport { confirm, select } from '@clack/prompts'\nimport { getFlowConfig, GlobalOptions } from '../lib/config.js'\nimport { createSpinner } from '../lib/prompts.js'\nimport fs from 'fs-extra'\nimport path from 'node:path'\nimport pc from 'picocolors'\nimport { Client as PgClient } from 'pg'\nimport mysql from 'mysql2/promise'\nimport Database from 'better-sqlite3'\n\nexport interface ApplyOptions {\n  migration?: string\n  target?: string\n  project?: string\n  yes?: boolean\n}\n\ninterface DatabaseConnection {\n  type: 'postgresql' | 'mysql' | 'sqlite'\n  client: any\n}\n\nexport async function applyCommand(options: ApplyOptions, globalOptions: GlobalOptions): Promise<void> {\n  const projectPath = options.project ? path.resolve(options.project) : process.cwd()\n  const cfg = await getFlowConfig(globalOptions, projectPath)\n  const envCfg = cfg.environments[cfg.defaultEnvironment]\n  \n  if (globalOptions.debug) {\n    console.log('Applying migration using env:', cfg.defaultEnvironment)\n  }\n  \n  const spinner = createSpinner('Connecting to database...')\n  \n  // Connect to database\n  let connection: DatabaseConnection | null = null\n  try {\n    console.log('connecting to database...'); // DEBUG LOG\n    connection = await connectToDatabase(envCfg)\n    console.log('connected to database'); // DEBUG LOG\n    spinner.update('Connected to database successfully')\n  } catch (error) {\n    console.log('error connecting to database', error); // DEBUG LOG\n    spinner.fail('Failed to connect to database')\n    console.log(pc.red(`‚ùå Database connection failed: ${error}`))\n    return\n  }\n  \n  try {\n    // Ensure migrations tracking table exists\n    spinner.update('Ensuring migration tracking table exists...')\n    await ensureMigrationsTable(connection)\n    console.log('ensured migrations table'); // DEBUG LOG\n    \n    // Find migrations to apply\n    spinner.update('Finding migrations to apply...')\n    const migrationsDir = envCfg.migrationsPath || './migrations'\n    const absoluteMigrationsDir = path.resolve(projectPath, migrationsDir)\n    \n    const pendingMigrations = await findPendingMigrations(connection, absoluteMigrationsDir, options.migration)\n    console.log('found pending migrations'); // DEBUG LOG\n    \n    if (pendingMigrations.length === 0) {\n      spinner.succeed('No pending migrations to apply')\n      console.log(pc.green('‚úÖ Database is up to date'))\n      return\n    }\n    \n    spinner.stop()\n    \n    console.log(`\\nüìã Found ${pendingMigrations.length} pending migration(s):`)\n    pendingMigrations.forEach((migration, idx) => {\n      console.log(`  ${idx + 1}. ${pc.cyan(migration.name)}`)\n    })\n    \n    if (globalOptions.dryRun) {\n      console.log(pc.yellow('\\nüîç Dry run mode - showing what would be applied:'))\n      for (const migration of pendingMigrations) {\n        console.log(`\\n${pc.cyan(`--- ${migration.name} ---`)}`)\n        console.log(pc.gray(migration.content))\n      }\n      return\n    }\n    \n    const proceed = options.yes ? true : await confirm({\n      message: `Apply ${pendingMigrations.length} migration(s) to the database?`\n    })\n    console.log('confirmed apply'); // DEBUG LOG\n    \n    if (!proceed) {\n      console.log(pc.gray('Migration cancelled.'))\n      return\n    }\n    \n    // Apply migrations\n    const applySpinner = createSpinner('Applying migrations...')\n    \n    for (const migration of pendingMigrations) {\n      try {\n        applySpinner.update(`Applying ${migration.name}...`)\n        \n        await applyMigration(connection, migration)\n        await recordMigrationApplied(connection, migration)\n        \n        if (globalOptions.debug) {\n          console.log(pc.green(`  ‚úÖ Applied ${migration.name}`))\n        }\n      } catch (error) {\n        applySpinner.fail(`Failed to apply ${migration.name}`)\n        console.log(pc.red(`‚ùå Migration failed: ${error}`))\n        \n        // Ask if they want to continue with remaining migrations\n        if (pendingMigrations.indexOf(migration) < pendingMigrations.length - 1) {\n          const continueApplying = await confirm({\n            message: 'Continue applying remaining migrations?'\n          })\n          \n          if (!continueApplying) {\n            console.log(pc.yellow('‚ö†Ô∏è  Migration process stopped'))\n            return\n          }\n        }\n      }\n    }\n    \n    applySpinner.succeed('All migrations applied successfully')\n    console.log(pc.green('‚úÖ Enhanced migrations applied successfully'))\n    \n  } finally {\n    // Close database connection\n    if (connection) {\n      await closeDatabaseConnection(connection)\n    }\n  }\n}\n\nasync function connectToDatabase(envCfg: any): Promise<DatabaseConnection> {\n  console.log('in connectToDatabase'); // DEBUG LOG\n  const connectionString = envCfg.db_connection_string || envCfg.databaseUrl;\n  console.log('connectionString:', connectionString); // DEBUG LOG\n  \n  if (!connectionString) {\n    throw new Error('Database connection string not found in flow.config.json. Please provide \"db_connection_string\" or \"databaseUrl\".')\n  }\n\n  const dbType = connectionString.split(':')[0];\n  console.log('dbType:', dbType); // DEBUG LOG\n\n  switch (dbType) {\n    case 'postgresql':\n      console.log('connecting to postgresql'); // DEBUG LOG\n      const pgClient = new PgClient({ connectionString })\n      await pgClient.connect()\n      console.log('connected to postgresql'); // DEBUG LOG\n      return { type: 'postgresql', client: pgClient }\n      \n    case 'mysql':\n      console.log('connecting to mysql'); // DEBUG LOG\n      const mysqlConnection = await mysql.createConnection(connectionString)\n      console.log('connected to mysql'); // DEBUG LOG\n      return { type: 'mysql', client: mysqlConnection }\n      \n    case 'sqlite':\n      console.log('connecting to sqlite'); // DEBUG LOG\n      const sqlitePath = connectionString.substring('sqlite:'.length);\n      const sqliteDb = new Database(sqlitePath || './database.db')\n      console.log('connected to sqlite'); // DEBUG LOG\n      return { type: 'sqlite', client: sqliteDb }\n      \n    default:\n      throw new Error(`Unsupported database type: ${dbType}`)\n  }\n}\n\nasync function ensureMigrationsTable(connection: DatabaseConnection): Promise<void> {\n  const createTableQuery = (() => {\n    switch (connection.type) {\n      case 'postgresql':\n        return `\n          CREATE TABLE IF NOT EXISTS flow_migrations (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(255) NOT NULL UNIQUE,\n            checksum VARCHAR(64),\n            applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n          )\n        `\n      case 'mysql':\n        return `\n          CREATE TABLE IF NOT EXISTS flow_migrations (\n            id INT AUTO_INCREMENT PRIMARY KEY,\n            name VARCHAR(255) NOT NULL UNIQUE,\n            checksum VARCHAR(64),\n            applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n          )\n        `\n      case 'sqlite':\n        return `\n          CREATE TABLE IF NOT EXISTS flow_migrations (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL UNIQUE,\n            checksum TEXT,\n            applied_at DATETIME DEFAULT CURRENT_TIMESTAMP\n          )\n        `\n    }\n  })()\n  \n  await executeQuery(connection, createTableQuery)\n}\n\nasync function findPendingMigrations(connection: DatabaseConnection, migrationsDir: string, targetMigration?: string): Promise<Array<{name: string, path: string, content: string}>> {\n  if (!await fs.pathExists(migrationsDir)) {\n    throw new Error(`Migrations directory not found: ${migrationsDir}`)\n  }\n  \n  const appliedMigrations = await executeQuery(connection, 'SELECT name FROM flow_migrations ORDER BY applied_at')\n  const appliedNames = new Set(appliedMigrations.map((row: any) => row.name))\n  \n  const allFiles = await fs.readdir(migrationsDir)\n  const migrationFiles = allFiles\n    .filter(f => f.endsWith('.sql') || f.endsWith('.ts') || f.endsWith('.js'))\n    .sort()\n  \n  const pendingMigrations = []\n  \n  for (const file of migrationFiles) {\n    const migrationName = path.parse(file).name\n    \n    if (appliedNames.has(migrationName)) {\n      continue\n    }\n    \n    if (targetMigration && migrationName !== targetMigration) {\n      continue\n    }\n    \n    const filePath = path.join(migrationsDir, file)\n    let content = await fs.readFile(filePath, 'utf-8')\n    \n    if (file.endsWith('.ts') || file.endsWith('.js')) {\n      content = extractSQLFromMigrationFile(content)\n    }\n    \n    pendingMigrations.push({\n      name: migrationName,\n      path: filePath,\n      content\n    })\n    \n    if (targetMigration && migrationName === targetMigration) {\n      break\n    }\n  }\n  \n  return pendingMigrations\n}\n\nasync function applyMigration(connection: DatabaseConnection, migration: {name: string, content: string}): Promise<void> {\n  // Split migration content by semicolon and execute each statement\n  const statements = migration.content\n    .split(';')\n    .map(stmt => stmt.trim())\n    .filter(stmt => stmt.length > 0)\n  \n  for (const statement of statements) {\n    if (statement.trim()) {\n      await executeQuery(connection, statement)\n    }\n  }\n}\n\nasync function recordMigrationApplied(connection: DatabaseConnection, migration: {name: string, content: string}): Promise<void> {\n  // Simple checksum calculation (could be improved)\n  const checksum = Buffer.from(migration.content).toString('base64').slice(0, 32)\n  \n  const insertQuery = (() => {\n    switch (connection.type) {\n      case 'postgresql':\n        return 'INSERT INTO flow_migrations (name, checksum) VALUES ($1, $2)'\n      case 'mysql':\n        return 'INSERT INTO flow_migrations (name, checksum) VALUES (?, ?)'\n      case 'sqlite':\n        return 'INSERT INTO flow_migrations (name, checksum) VALUES (?, ?)'\n    }\n  })()\n  \n  await executeQuery(connection, insertQuery, [migration.name, checksum])\n}\n\nasync function executeQuery(connection: DatabaseConnection, query: string, params?: any[]): Promise<any[]> {\n  switch (connection.type) {\n    case 'postgresql':\n      const pgResult = await connection.client.query(query, params)\n      return pgResult.rows\n      \n    case 'mysql':\n      const [mysqlResult] = await connection.client.execute(query, params)\n      return mysqlResult as any[]\n      \n    case 'sqlite':\n      const stmt = connection.client.prepare(query)\n      return stmt.all(params)\n  }\n}\n\nasync function closeDatabaseConnection(connection: DatabaseConnection): Promise<void> {\n  switch (connection.type) {\n    case 'postgresql':\n      await connection.client.end()\n      break\n    case 'mysql':\n      await connection.client.end()\n      break\n    case 'sqlite':\n      connection.client.close()\n      break\n  }\n}\n\nfunction extractSQLFromMigrationFile(content: string): string {\n  // Simple extraction logic, can be improved\n  // For now, assume SQL is in a template literal tagged with `sql`\n  const match = content.match(/sql`([\\s\\S]*)`/)\n  \n  const extractedSQL = match ? match[1].trim() : ''\n  \n  if (!extractedSQL) {\n    // Fallback for plain SQL files or different export format\n    if (content.includes('CREATE') || content.includes('ALTER') || content.includes('INSERT') || content.includes('UPDATE') || content.includes('DELETE')) {\n      return content;\n    }\n  }\n  \n  return extractedSQL || content\n} ","/**\n * flow back - Rollback migrations\n */\n\nimport { confirm, select } from '@clack/prompts'\nimport { getFlowConfig, GlobalOptions } from '../lib/config.js'\nimport { createSpinner } from '../lib/prompts.js'\nimport fs from 'fs-extra'\nimport path from 'node:path'\nimport pc from 'picocolors'\nimport { Client as PgClient } from 'pg'\nimport mysql from 'mysql2/promise'\nimport Database from 'better-sqlite3'\n\nexport interface BackOptions {\n  steps?: number\n  to?: string\n  project?: string\n  yes?: boolean\n}\n\ninterface DatabaseConnection {\n  type: 'postgresql' | 'mysql' | 'sqlite'\n  client: any\n}\n\ninterface AppliedMigration {\n  id: number\n  name: string\n  checksum: string\n  applied_at: string\n}\n\nexport async function backCommand(options: BackOptions, globalOptions: GlobalOptions): Promise<void> {\n  const projectPath = options.project ? path.resolve(options.project) : process.cwd()\n  const cfg = await getFlowConfig(globalOptions, projectPath)\n  const envCfg = cfg.environments[cfg.defaultEnvironment]\n  \n  if (globalOptions.debug) {\n    console.log('Rolling back using env:', cfg.defaultEnvironment)\n  }\n  \n  const spinner = createSpinner('Connecting to database...')\n  \n  // Connect to database\n  let connection: DatabaseConnection | null = null\n  try {\n    connection = await connectToDatabase(envCfg)\n    spinner.update('Connected to database successfully')\n  } catch (error) {\n    spinner.fail('Failed to connect to database')\n    console.log(pc.red(`‚ùå Database connection failed: ${error}`))\n    return\n  }\n  \n  try {\n    // Get applied migrations\n    spinner.update('Fetching applied migrations...')\n    const appliedMigrations = await getAppliedMigrations(connection)\n    \n    if (appliedMigrations.length === 0) {\n      spinner.succeed('No migrations to rollback')\n      console.log(pc.green('‚úÖ No migrations have been applied'))\n      return\n    }\n    \n    spinner.stop()\n    \n    // Determine which migrations to rollback\n    const migrationsToRollback = await determineMigrationsToRollback(\n      appliedMigrations, \n      options, \n      envCfg,\n      projectPath\n    )\n    \n    if (migrationsToRollback.length === 0) {\n      console.log(pc.yellow('‚ö†Ô∏è  No migrations selected for rollback'))\n      return\n    }\n    \n    console.log(`\\nüìã Migrations to rollback (${migrationsToRollback.length}):`)\n    migrationsToRollback.forEach((migration, idx) => {\n      console.log(`  ${idx + 1}. ${pc.cyan(migration.name)} (applied: ${migration.applied_at})`)\n    })\n    \n    if (globalOptions.dryRun) {\n      console.log(pc.yellow('\\nüîç Dry run mode - showing what would be rolled back'))\n      for (const migration of migrationsToRollback) {\n        console.log(`\\n${pc.cyan(`--- Rollback ${migration.name} ---`)}`)\n        \n        // Try to find the DOWN migration content\n        const downContent = await findDownMigration(migration, envCfg, projectPath)\n        if (downContent) {\n          console.log(pc.gray(downContent))\n        } else {\n          console.log(pc.yellow('‚ö†Ô∏è  No rollback script found for this migration'))\n        }\n      }\n      return\n    }\n    \n    // Confirm rollback\n    const riskyMigrations = migrationsToRollback.filter(m => \n      m.name.toLowerCase().includes('drop') || \n      m.name.toLowerCase().includes('delete')\n    )\n    \n    if (riskyMigrations.length > 0) {\n      console.log(pc.red('\\n‚ö†Ô∏è  WARNING: Some migrations may contain destructive operations:'))\n      riskyMigrations.forEach(m => {\n        console.log(`  ‚Ä¢ ${pc.red(m.name)}`)\n      })\n    }\n    \n    const proceed = options.yes ? true : await confirm({\n      message: riskyMigrations.length > 0\n        ? pc.red('‚ö†Ô∏è  Are you sure you want to rollback these potentially destructive migrations?')\n        : `Rollback ${migrationsToRollback.length} migration(s)?`\n    })\n    \n    if (!proceed) {\n      console.log(pc.gray('Rollback cancelled.'))\n      return\n    }\n    \n    // Perform rollback\n    const rollbackSpinner = createSpinner('Rolling back migrations...')\n    \n    for (const migration of migrationsToRollback) {\n      try {\n        rollbackSpinner.update(`Rolling back ${migration.name}...`)\n        \n        const downContent = await findDownMigration(migration, envCfg, projectPath)\n        if (downContent) {\n          await executeMigrationRollback(connection, downContent)\n        } else {\n          console.log(pc.yellow(`‚ö†Ô∏è  No rollback script found for ${migration.name}, skipping...`))\n        }\n        \n        await removeMigrationRecord(connection, migration)\n        \n        if (globalOptions.debug) {\n          console.log(pc.green(`  ‚úÖ Rolled back ${migration.name}`))\n        }\n      } catch (error) {\n        rollbackSpinner.fail(`Failed to rollback ${migration.name}`)\n        console.log(pc.red(`‚ùå Rollback failed: ${error}`))\n        \n        // Ask if they want to continue with remaining rollbacks\n        if (migrationsToRollback.indexOf(migration) < migrationsToRollback.length - 1) {\n          const continueRollback = await confirm({\n            message: 'Continue rolling back remaining migrations?'\n          })\n          \n          if (!continueRollback) {\n            console.log(pc.yellow('‚ö†Ô∏è  Rollback process stopped'))\n            return\n          }\n        }\n      }\n    }\n    \n    rollbackSpinner.succeed('All migrations rolled back successfully')\n    console.log(pc.green('‚úÖ Migration rollback completed safely'))\n    \n  } finally {\n    // Close database connection\n    if (connection) {\n      await closeDatabaseConnection(connection)\n    }\n  }\n}\n\nasync function connectToDatabase(envCfg: any): Promise<DatabaseConnection> {\n  const connectionString = envCfg.db_connection_string || envCfg.databaseUrl;\n  \n  if (!connectionString) {\n    throw new Error('Database connection string not found in flow.config.json. Please provide \"db_connection_string\" or \"databaseUrl\".')\n  }\n\n  const dbType = connectionString.split(':')[0];\n\n  switch (dbType) {\n    case 'postgresql':\n      const pgClient = new PgClient({ connectionString })\n      await pgClient.connect()\n      return { type: 'postgresql', client: pgClient }\n      \n    case 'mysql':\n      const mysqlConnection = await mysql.createConnection(connectionString)\n      return { type: 'mysql', client: mysqlConnection }\n      \n    case 'sqlite':\n      const sqlitePath = connectionString.substring('sqlite:'.length);\n      const sqliteDb = new Database(sqlitePath || './database.db')\n      return { type: 'sqlite', client: sqliteDb }\n      \n    default:\n      throw new Error(`Unsupported database type: ${dbType}`)\n  }\n}\n\nasync function getAppliedMigrations(connection: DatabaseConnection): Promise<AppliedMigration[]> {\n  try {\n    const result = await executeQuery(\n      connection, \n      'SELECT id, name, checksum, applied_at FROM flow_migrations ORDER BY id DESC'\n    )\n    return result\n  } catch (error) {\n    // If the table doesn't exist, no migrations have been applied\n    return []\n  }\n}\n\nasync function determineMigrationsToRollback(\n  appliedMigrations: AppliedMigration[], \n  options: BackOptions,\n  envCfg: any,\n  projectPath: string\n): Promise<AppliedMigration[]> {\n  if (options.to) {\n    // Rollback to a specific migration (exclusive)\n    const targetIndex = appliedMigrations.findIndex(m => m.name === options.to)\n    if (targetIndex === -1) {\n      throw new Error(`Migration '${options.to}' not found in applied migrations`)\n    }\n    return appliedMigrations.slice(0, targetIndex)\n  }\n  \n  const steps = options.steps || 1\n  \n  if (steps >= appliedMigrations.length) {\n    // Confirm rolling back all migrations\n    const confirmAll = options.yes ? true : await confirm({\n      message: pc.yellow(`‚ö†Ô∏è  This will rollback ALL ${appliedMigrations.length} migrations. Continue?`)\n    })\n    \n    if (!confirmAll) {\n      return []\n    }\n    \n    return appliedMigrations\n  }\n  \n  return appliedMigrations.slice(0, steps)\n}\n\nasync function findDownMigration(migration: AppliedMigration, envCfg: any, projectPath: string): Promise<string | null> {\n  const migrationsDir = envCfg.migrationsPath || './migrations'\n  const absoluteMigrationsDir = path.resolve(projectPath, migrationsDir)\n  \n  // Find the corresponding migration file\n  const files = await fs.readdir(absoluteMigrationsDir)\n  \n  for (const filename of files) {\n    if (path.parse(filename).name === migration.name) {\n      const content = await fs.readFile(path.join(absoluteMigrationsDir, filename), 'utf-8')\n      return extractDownSQLFromMigrationFile(content)\n    }\n  }\n  \n  return null\n}\n\nasync function executeMigrationRollback(connection: DatabaseConnection, downContent: string): Promise<void> {\n  const statements = downContent.split(';').filter(s => s.trim() !== '')\n  for (const statement of statements) {\n    await executeQuery(connection, statement)\n  }\n}\n\nasync function removeMigrationRecord(connection: DatabaseConnection, migration: AppliedMigration): Promise<void> {\n  const deleteQuery = (() => {\n    switch (connection.type) {\n      case 'postgresql':\n        return 'DELETE FROM flow_migrations WHERE id = $1'\n      case 'mysql':\n        return 'DELETE FROM flow_migrations WHERE id = ?'\n      case 'sqlite':\n        return 'DELETE FROM flow_migrations WHERE id = ?'\n    }\n  })()\n  \n  await executeQuery(connection, deleteQuery, [migration.id])\n}\n\nasync function executeQuery(connection: DatabaseConnection, query: string, params?: any[]): Promise<any[]> {\n  switch (connection.type) {\n    case 'postgresql':\n      const pgResult = await connection.client.query(query, params)\n      return pgResult.rows\n    case 'mysql':\n      const [mysqlResult] = await connection.client.execute(query, params)\n      return mysqlResult as any[]\n    case 'sqlite':\n      const stmt = connection.client.prepare(query)\n      if (query.toLowerCase().trim().startsWith('select')) {\n        return stmt.all(params)\n      } else {\n        stmt.run(params)\n        return []\n      }\n    default:\n      throw new Error(`Unsupported database type: ${connection.type}`)\n  }\n}\n\nasync function closeDatabaseConnection(connection: DatabaseConnection): Promise<void> {\n  switch (connection.type) {\n    case 'postgresql':\n      await connection.client.end()\n      break\n      \n    case 'mysql':\n      await connection.client.end()\n      break\n      \n    case 'sqlite':\n      connection.client.close()\n      break\n  }\n}\n\nfunction extractDownSQLFromMigrationFile(content: string): string {\n  // Look for down migration patterns in TypeScript/JavaScript files\n  const downPatterns = [\n    /public async down\\(.*?\\): Promise<void> \\{([\\s\\S]*?)\\}/,\n    /async down\\(.*?\\) \\{([\\s\\S]*?)\\}/,\n    /down.*?{([\\s\\S]*?)}/,\n  ]\n  \n  for (const pattern of downPatterns) {\n    const match = content.match(pattern)\n    if (match) {\n      const downCode = match[1]\n      \n      // Extract SQL from the down method\n      const sqlPatterns = [\n        /queryRunner\\.query\\s*\\(\\s*[`\"']([^`\"']+)[`\"']/g,\n        /sql\\s*`([^`]+)`/g,\n        /\"((?:DROP|ALTER|CREATE|INSERT|UPDATE|DELETE)[^\"]+)\"/gi\n      ]\n      \n      let extractedSQL = ''\n      for (const sqlPattern of sqlPatterns) {\n        let sqlMatch\n        while ((sqlMatch = sqlPattern.exec(downCode)) !== null) {\n          extractedSQL += sqlMatch[1] + ';\\n'\n        }\n      }\n      \n      return extractedSQL\n    }\n  }\n  \n  return ''\n} "],"mappings":";;;AAKA,SAAS,eAAe;AACxB,SAAS,OAAO,OAAO,UAAU,QAAQ,OAAAA,YAAW;;;ACJlD,cAAW;;;ACEb,SAAS,SAAS,QAAQ,aAAa,MAAM,SAAS,WAAW;AACjE,OAAO,YAAY;AAYnB,eAAsB,cACpB,SACA,UAAe,CAAC,GACE;AAClB,SAAO,MAAM,QAAQ;AAAA,IACnB,SAAS,OAAO,KAAK,OAAO;AAAA,IAC5B,GAAG;AAAA,EACL,CAAC;AACH;AA4CA,eAAsB,UACpB,SACA,UAAe,CAAC,GACC;AACjB,SAAO,MAAM,KAAK;AAAA,IAChB,SAAS,OAAO,KAAK,OAAO;AAAA,IAC5B,GAAG;AAAA,EACL,CAAC;AACH;AAqEO,SAAS,cAAc,SAAiB;AAC7C,QAAM,IAAI,QAAQ;AAClB,IAAE,MAAM,OAAO,KAAK,aAAM,OAAO,EAAE,CAAC;AAEpC,SAAO;AAAA,IACL,QAAQ,CAAC,eAAuB,EAAE,QAAQ,OAAO,KAAK,aAAM,UAAU,EAAE,CAAC;AAAA,IACzE,SAAS,CAACC,aAAqB,EAAE,KAAK,OAAO,MAAM,UAAKA,YAAW,UAAU,EAAE,CAAC;AAAA,IAChF,MAAM,CAACA,aAAqB,EAAE,KAAK,OAAO,IAAI,UAAKA,YAAW,QAAQ,EAAE,CAAC;AAAA,IACzE,MAAM,MAAM,EAAE,KAAK;AAAA,EACrB;AACF;;;ACtJA,OAAO,aAAa;AACpB,SAAS,WAAAC,gBAAe;AACxB,OAAO,YAAY;;;ACJnB,SAAS,QAAAC,aAAY;;;ACArB,OAAO,QAAQ;AAEf,SAAS,MAAM,SAAS,gBAAyB;AADjD,IAAM,EAAE,UAAU,WAAW,QAAQ,MAAM,QAAQ,IAAI;AAOvD,eAAsB,OAAOC,OAAgC;AAC3D,MAAI;AACF,UAAM,OAAOA,KAAI;AACjB,WAAO;AAAA,EACT,QAAQ;AACN,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,eAAeA,OAAc,WAAmB,QAAQ,IAAI,GAAsB;AACtG,QAAM,eAAe,QAAQ,UAAUA,KAAI;AAC3C,QAAM,eAAe,SAAS,UAAU,YAAY;AACpD,QAAM,aAAa,MAAM,OAAO,YAAY;AAE5C,SAAO;AAAA,IACL,UAAU;AAAA,IACV,UAAU;AAAA,IACV,QAAQ;AAAA,EACV;AACF;AAKA,eAAsB,gBAAgBA,OAAuC;AAC3E,MAAI;AACF,UAAM,UAAU,MAAM,SAASA,OAAM,OAAO;AAC5C,WAAO,EAAE,SAAS,MAAM,MAAM,QAAQ;AAAA,EACxC,SAAS,OAAO;AACd,WAAO;AAAA,MACL,SAAS;AAAA,MACT,OAAO,iBAAiB,QAAQ,QAAQ,IAAI,MAAM,4BAA4B;AAAA,IAChF;AAAA,EACF;AACF;AA0CA,eAAsB,UACpB,WACA,SACA,YAAqB,MACF;AACnB,QAAM,QAAkB,CAAC;AAEzB,MAAI;AACF,UAAM,QAAQ,MAAM,QAAQ,WAAW,EAAE,eAAe,KAAK,CAAC;AAE9D,eAAW,QAAQ,OAAO;AACxB,YAAM,WAAW,KAAK,WAAW,KAAK,IAAI;AAE1C,UAAI,KAAK,YAAY,KAAK,WAAW;AACnC,cAAM,WAAW,MAAM,UAAU,UAAU,SAAS,SAAS;AAC7D,cAAM,KAAK,GAAG,QAAQ;AAAA,MACxB,WAAW,KAAK,OAAO,KAAK,QAAQ,KAAK,KAAK,IAAI,GAAG;AACnD,cAAM,KAAK,QAAQ;AAAA,MACrB;AAAA,IACF;AAAA,EACF,QAAQ;AAAA,EAER;AAEA,SAAO;AACT;AAaA,eAAsB,aAAsBC,OAAkC;AAC5E,QAAM,aAAa,MAAM,gBAAgBA,KAAI;AAE7C,MAAI,CAAC,WAAW,SAAS;AACvB,WAAO;AAAA,EACT;AAEA,MAAI;AACF,UAAM,OAAO,KAAK,MAAM,WAAW,IAAI;AACvC,WAAO,EAAE,SAAS,MAAM,KAAK;AAAA,EAC/B,SAAS,OAAO;AACd,WAAO;AAAA,MACL,SAAS;AAAA,MACT,OAAO,IAAI,MAAM,wBAAwBA,KAAI,KAAK,iBAAiB,QAAQ,MAAM,UAAU,eAAe,EAAE;AAAA,IAC9G;AAAA,EACF;AACF;;;ACjJA,OAAOC,SAAQ;AACf,OAAO,UAAU;;;AFQV,IAAe,kBAAf,MAAsD;AAAA;AAAA;AAAA;AAAA,EAqB3D,MAAgB,6BACd,aACA,cACiD;AACjD,UAAM,kBAAkBC,MAAK,aAAa,cAAc;AACxD,UAAM,gBAAgB,MAAM,aAAkG,eAAe;AAE7I,QAAI,CAAC,cAAc,SAAS;AAC1B,aAAO,EAAE,OAAO,CAAC,GAAG,SAAS,aAAa;AAAA,IAC5C;AAEA,UAAM,UAAU;AAAA,MACd,GAAG,cAAc,KAAK;AAAA,MACtB,GAAG,cAAc,KAAK;AAAA,IACxB;AAEA,UAAM,QAAQ,aAAa,OAAO,SAAO,OAAO,OAAO;AACvD,UAAM,UAAU,aAAa,OAAO,SAAO,EAAE,OAAO,QAAQ;AAE5D,WAAO,EAAE,OAAO,QAAQ;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA,EAKA,MAAgB,WACd,aACA,WACsD;AACtD,UAAM,WAAuB,CAAC;AAC9B,UAAM,UAAoB,CAAC;AAE3B,eAAW,YAAY,WAAW;AAChC,YAAM,WAAWA,MAAK,aAAa,QAAQ;AAC3C,YAAM,aAAa,MAAM,OAAO,QAAQ;AAExC,UAAI,YAAY;AACd,iBAAS,KAAK,MAAM,eAAe,UAAU,WAAW,CAAC;AAAA,MAC3D,OAAO;AACL,gBAAQ,KAAK,QAAQ;AAAA,MACvB;AAAA,IACF;AAEA,WAAO,EAAE,UAAU,QAAQ;AAAA,EAC7B;AAAA;AAAA;AAAA;AAAA,EAKA,MAAgB,mBACd,aACA,UACA,cAAwB,CAAC,GAAG,GACT;AACnB,UAAM,WAAqB,CAAC;AAE5B,eAAW,aAAa,aAAa;AACnC,YAAM,gBAAgBA,MAAK,aAAa,SAAS;AAEjD,iBAAW,WAAW,UAAU;AAC9B,cAAM,QAAQ,MAAM,UAAU,eAAe,SAAS,IAAI;AAC1D,iBAAS,KAAK,GAAG,KAAK;AAAA,MACxB;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKU,oBAAoB,UAInB;AACT,QAAI,SAAS,SAAS,UAAU,GAAG;AACjC,aAAO;AAAA,IACT;AAEA,UAAM,gBAAgB,SAAS,SAAS,QAAQ,SAAS,SAAS;AAClE,UAAM,gBAAgB,SAAS,SAAS,QAAQ,IAC5C,SAAS,SAAS,QAAQ,SAAS,SAAS,QAC5C;AAGJ,UAAM,YAAY,gBAAgB;AAGlC,UAAM,aAAa,gBAAgB;AAGnC,UAAM,UAAU,KAAK,IAAI,SAAS,WAAW,KAAK,GAAG;AAErD,WAAO,KAAK,IAAI,GAAG,KAAK,IAAI,GAAG,YAAY,aAAa,OAAO,CAAC;AAAA,EAClE;AAAA;AAAA;AAAA;AAAA,EAKU,iBAAiB,KAAoC;AAC7D,QAAI;AACF,YAAM,SAAS,IAAI,IAAI,GAAG;AAE1B,UAAI;AAEJ,cAAQ,OAAO,UAAU;AAAA,QACvB,KAAK;AAAA,QACL,KAAK;AACH,iBAAO;AACP;AAAA,QACF,KAAK;AACH,iBAAO;AACP;AAAA,QACF,KAAK;AACH,iBAAO;AACP;AAAA,QACF;AACE,iBAAO;AAAA,MACX;AAEA,aAAO;AAAA,QACL;AAAA,QACA,MAAM,OAAO,YAAY;AAAA,QACzB,MAAM,OAAO,OAAO,SAAS,OAAO,IAAI,IAAI;AAAA,QAC5C,UAAU,OAAO,SAAS,MAAM,CAAC;AAAA;AAAA,QACjC,UAAU,OAAO,YAAY;AAAA,QAC7B,UAAU,OAAO,YAAY;AAAA,QAC7B;AAAA,MACF;AAAA,IACF,QAAQ;AACN,aAAO;AAAA,IACT;AAAA,EACF;AACF;;;AG1JO,IAAM,iBAAN,cAA6B,gBAAgB;AAAA,EAClD,OAAO;AAAA;AAAA;AAAA;AAAA,EAKP,MAAM,OAAO,aAA+C;AAC1D,UAAM,WAAqB,CAAC;AAC5B,UAAMC,YAAqB,CAAC;AAG5B,UAAM,EAAE,OAAO,WAAW,SAAS,YAAY,IAAI,MAAM,KAAK;AAAA,MAC5D;AAAA,MACA,CAAC,UAAU,gBAAgB;AAAA,IAC7B;AAEA,aAAS,KAAK,GAAG,UAAU,IAAI,SAAO,qBAAqB,GAAG,EAAE,CAAC;AAGjE,UAAM,EAAE,UAAU,YAAY,IAAI,MAAM,KAAK,WAAW,aAAa;AAAA,MACnE;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI,YAAY,SAAS,GAAG;AAC1B,eAAS,KAAK,sBAAsB,YAAY,CAAC,EAAE,QAAQ,EAAE;AAAA,IAC/D;AAGA,UAAM,EAAE,UAAU,cAAc,IAAI,MAAM,KAAK,WAAW,aAAa;AAAA,MACrE;AAAA,IACF,CAAC;AAED,QAAI,cAAc,SAAS,GAAG;AAC5B,eAAS,KAAK,+BAA+B,cAAc,CAAC,EAAE,QAAQ,EAAE;AAAA,IAC1E;AAGA,UAAM,iBAAiB,MAAM,KAAK;AAAA,MAChC;AAAA,MACA,CAAC,+BAA+B;AAAA,MAChC,CAAC,cAAc;AAAA,IACjB;AAEA,QAAI,eAAe,SAAS,GAAG;AAC7B,eAAS,KAAK,+BAA+B;AAAA,IAC/C;AAGA,UAAM,aAAa,KAAK,oBAAoB;AAAA,MAC1C,UAAU,EAAE,OAAO,UAAU,QAAQ,OAAO,EAAE;AAAA;AAAA,MAC9C,UAAU,EAAE,OAAO,YAAY,SAAS,cAAc,QAAQ,OAAO,EAAE;AAAA,MACvE,UAAU;AAAA,IACZ,CAAC;AAGD,QAAI,UAAU,SAAS,KAAK,YAAY,WAAW,GAAG;AACpD,MAAAA,UAAS,KAAK,4DAA4D;AAAA,IAC5E;AAEA,QAAI,YAAY,SAAS,KAAK,CAAC,UAAU,SAAS,gBAAgB,GAAG;AACnE,MAAAA,UAAS,KAAK,oDAAoD;AAAA,IACpE;AAEA,WAAO;AAAA,MACL,OAAO,aAAa;AAAA,MACpB;AAAA,MACA;AAAA,MACA,UAAUA,UAAS,SAAS,IAAIA,YAAW;AAAA,IAC7C;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,aAAmD;AAErE,UAAM,EAAE,UAAU,YAAY,IAAI,MAAM,KAAK,WAAW,aAAa;AAAA,MACnE;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI,YAAY,WAAW,GAAG;AAC5B,aAAO;AAAA,IACT;AAEA,UAAM,aAAa,YAAY,CAAC;AAChC,UAAM,qBAAqB,MAAM,eAAe,qBAAqB,WAAW;AAGhF,UAAM,eAAe,MAAM,gBAAgB,WAAW,QAAQ;AAC9D,QAAI,CAAC,aAAa,SAAS;AACzB,aAAO;AAAA,IACT;AAGA,QAAI;AACJ,UAAM,iBAAiB,aAAa,KAAK,MAAM,iCAAiC;AAChF,QAAI,gBAAgB;AAClB,YAAM,kBAAkB,eAAe,CAAC;AACxC,YAAM,gBAAgB,gBAAgB,MAAM,0BAA0B;AACtE,YAAM,cAAc,gBAAgB,MAAM,wBAAwB;AAElE,wBAAkB;AAAA,QAChB,UAAU,gBAAgB,CAAC,KAAK;AAAA,QAChC,QAAQ,cAAc,CAAC;AAAA,MACzB;AAAA,IACF;AAEA,WAAO;AAAA,MACL,MAAM;AAAA,MACN,YAAY;AAAA,MACZ;AAAA,MACA;AAAA,MACA,cAAc,CAAC,UAAU,gBAAgB;AAAA,MACzC;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,aAAqD;AAC3E,UAAM,SAAS,MAAM,KAAK,cAAc,WAAW;AACnD,QAAI,CAAC,QAAQ;AACX,aAAO;AAAA,IACT;AAGA,UAAM,eAAe,MAAM,gBAAgB,OAAO,WAAW,QAAQ;AACrE,QAAI,CAAC,aAAa,SAAS;AACzB,aAAO;AAAA,IACT;AAGA,UAAM,kBAAkB,aAAa,KAAK,MAAM,+BAA+B;AAC/E,QAAI,CAAC,iBAAiB;AACpB,aAAO;AAAA,IACT;AAEA,UAAM,mBAAmB,gBAAgB,CAAC;AAG1C,UAAM,gBAAgB,iBAAiB,MAAM,0BAA0B;AACvE,UAAM,WAAW,gBAAgB,CAAC;AAGlC,UAAM,WAAW,iBAAiB,MAAM,4BAA4B,KACpD,iBAAiB,MAAM,qBAAqB;AAE5D,QAAI,CAAC,UAAU;AACb,aAAO;AAAA,IACT;AAEA,QAAI;AACJ,QAAI,SAAS,CAAC,EAAE,SAAS,MAAM,GAAG;AAEhC,YAAM,SAAS,SAAS,CAAC;AACzB,oBAAc,QAAQ,IAAI,MAAM,KAAK;AAErC,UAAI,CAAC,aAAa;AAEhB,eAAO;AAAA,UACL,MAAM,KAAK,wBAAwB,QAAQ;AAAA,UAC3C,UAAU;AAAA,QACZ;AAAA,MACF;AAAA,IACF,OAAO;AAEL,oBAAc,SAAS,CAAC;AAAA,IAC1B;AAGA,UAAM,WAAW,KAAK,iBAAiB,WAAW;AAClD,QAAI,UAAU;AACZ,aAAO;AAAA,IACT;AAGA,WAAO;AAAA,MACL,MAAM,KAAK,wBAAwB,QAAQ;AAAA,MAC3C,UAAU;AAAA,IACZ;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,wBAAwB,UAA2C;AACzE,YAAQ,UAAU;AAAA,MAChB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT;AACE,eAAO;AAAA,IACX;AAAA,EACF;AACF;;;AC1MA,OAAOC,WAAU;AACjB,OAAOC,SAAQ;AAER,IAAM,kBAAN,cAA8B,gBAAgB;AAAA,EACnD,OAAO;AAAA,EAEP,MAAM,OAAO,aAA+C;AAC1D,UAAM,WAAqB,CAAC;AAE5B,QAAI;AAEF,YAAM,cAAc;AAAA,QAClB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEO,YAAM,EAAE,UAAU,iBAAiB,IAAI,MAAM,KAAK,WAAW,aAAa,WAAW;AAC3F,eAAS,KAAK,GAAG,iBAAiB,IAAI,OAAK,sBAAsB,EAAE,QAAQ,EAAE,CAAC;AAG9E,YAAM,OAAO,MAAM,KAAK,6BAA6B,aAAa,CAAC,eAAe,aAAa,CAAC;AAChG,eAAS,KAAK,GAAG,KAAK,MAAM,IAAI,SAAO,qBAAqB,GAAG,EAAE,CAAC;AAGlE,YAAM,iBAAiB;AAAA,QACrB;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,YAAM,EAAE,UAAU,YAAY,IAAI,MAAM,KAAK,WAAW,aAAa,cAAc;AACnF,eAAS,KAAK,GAAG,YAAY,IAAI,OAAK,sBAAsB,EAAE,QAAQ,EAAE,CAAC;AAGzE,YAAM,gBAAgB,CAAC,WAAW,cAAc,oBAAoB;AACpE,YAAM,EAAE,UAAU,mBAAmB,IAAI,MAAM,KAAK,WAAW,aAAa,aAAa;AACzF,eAAS,KAAK,GAAG,mBAAmB,IAAI,OAAK,8BAA8B,EAAE,QAAQ,EAAE,CAAC;AAGzF,YAAM,aAAa,KAAK,oBAAoB;AAAA,QAC1C,UAAU;AAAA,UACR,OAAO,KAAK,MAAM,SAAS,IAAI,IAAI;AAAA,UACnC,OAAO;AAAA,QACT;AAAA,QACA,UAAU;AAAA,UACR,OAAO,iBAAiB,SAAS,YAAY,SAAS,mBAAmB;AAAA,UACzE,OAAO,YAAY,SAAS,eAAe,SAAS,cAAc;AAAA,QACpE;AAAA,QACA,UAAU;AAAA,MACZ,CAAC;AAED,aAAO;AAAA,QACL,OAAO,aAAa;AAAA,QACpB,YAAY,KAAK,MAAM,aAAa,GAAG;AAAA,QACvC;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,aAAO;AAAA,QACL,OAAO;AAAA,QACP,YAAY;AAAA,QACZ,UAAU,CAAC,4BAA4B,KAAK,EAAE;AAAA,MAChD;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,aAAoD;AACtE,QAAI;AAEF,YAAM,cAAc;AAAA,QAClB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,YAAM,EAAE,UAAU,iBAAiB,IAAI,MAAM,KAAK,WAAW,aAAa,WAAW;AACrF,UAAI,iBAAiB,WAAW,GAAG;AACjC,eAAO;AAAA,MACT;AAEA,YAAM,aAAa,iBAAiB,CAAC;AACrC,YAAM,gBAAgB,MAAMA,IAAG,SAAS,WAAW,UAAU,OAAO;AAGpE,YAAM,SAAS,KAAK,mBAAmB,eAAe,SAAS,KAAK;AACpE,YAAM,eAAe,CAAC,MAAM,UAAU,kBAAkB,QAAQ;AAChE,YAAM,eAAe,aAAa,SAAS,MAAa,IAAI,SAAwC;AACpG,YAAM,SAAS,KAAK,mBAAmB,eAAe,KAAK,KAAK;AAChE,YAAM,uBAAuBD,MAAK,QAAQ,aAAa,MAAM;AAE7D,YAAM,SAAwB;AAAA,QAC5B,MAAM;AAAA,QACN;AAAA,QACA,QAAQ;AAAA,QACR,YAAY,KAAK,mBAAmB,eAAe,QAAQ,KAAK;AAAA,QAChE;AAAA,QACA,oBAAoB;AAAA,UAClB,UAAU;AAAA,UACV,UAAU;AAAA,UACV,QAAQ,MAAMC,IAAG,WAAW,oBAAoB;AAAA,QAClD;AAAA,QACA,cAAc,CAAC,eAAe,aAAa;AAAA,MAC7C;AAEA,aAAO;AAAA,IACT,SAAS,OAAO;AACd,cAAQ,KAAK,qCAAqC,KAAK,EAAE;AACzD,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAEA,MAAM,kBAAkB,aAAqD;AAC3E,QAAI;AAEF,YAAM,WAAW,CAAC,QAAQ,cAAc,kBAAkB;AAC1D,YAAM,EAAE,UAAU,cAAc,IAAI,MAAM,KAAK,WAAW,aAAa,QAAQ;AAE/E,iBAAW,WAAW,eAAe;AACnC,cAAM,aAAa,MAAMA,IAAG,SAAS,QAAQ,UAAU,OAAO;AAC9D,cAAM,QAAQ,KAAK,gBAAgB,YAAY,cAAc;AAC7D,YAAI,OAAO;AACT,gBAAM,SAAS,KAAK,iBAAiB,KAAK;AAC1C,cAAI,OAAQ,QAAO;AAAA,QACrB;AAAA,MACF;AAGA,YAAM,gBAAgB,MAAM,KAAK,cAAc,WAAW;AAC1D,UAAI,CAAC,cAAe,QAAO;AAG3B,YAAM,YAA+D;AAAA,QACnE,MAAM;AAAA,QACN,UAAU;AAAA,QACV,kBAAkB;AAAA,MACpB;AAEA,YAAM,SAAS,UAAU,cAAc,MAAM,KAAK;AAElD,aAAO;AAAA,QACL,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM,WAAW,eAAe,OAAO,WAAW,UAAU,OAAO;AAAA,QACnE,UAAU;AAAA,MACZ;AAAA,IACF,SAAS,OAAO;AACd,cAAQ,KAAK,sCAAsC,KAAK,EAAE;AAC1D,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAEQ,mBAAmB,SAAiB,KAAiC;AAE3E,UAAM,QAAQ,IAAI,OAAO,GAAG,GAAG,uBAAuB;AACtD,UAAM,QAAQ,QAAQ,MAAM,KAAK;AACjC,WAAO,QAAQ,CAAC;AAAA,EAClB;AAAA,EAEQ,gBAAgB,SAAiB,KAAiC;AACxE,UAAM,QAAQ,IAAI,OAAO,IAAI,GAAG,kBAAkB,GAAG;AACrD,UAAM,QAAQ,QAAQ,MAAM,KAAK;AACjC,WAAO,QAAQ,CAAC,GAAG,QAAQ,SAAS,EAAE,EAAE,KAAK;AAAA,EAC/C;AACF;;;ACpKA,OAAOC,SAAQ;AAER,IAAM,kBAAN,cAA8B,gBAAgB;AAAA,EACnD,OAAO;AAAA,EAEP,MAAM,OAAO,aAA+C;AAC1D,UAAM,WAAqB,CAAC;AAE5B,QAAI;AAEF,YAAM,cAAc;AAAA,QAClB;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,YAAM,EAAE,UAAU,iBAAiB,IAAI,MAAM,KAAK,WAAW,aAAa,WAAW;AACrF,eAAS,KAAK,GAAG,iBAAiB,IAAI,OAAK,sBAAsB,EAAE,QAAQ,EAAE,CAAC;AAG9E,YAAM,OAAO,MAAM,KAAK,6BAA6B,aAAa,CAAC,WAAW,iBAAiB,CAAC;AAChG,eAAS,KAAK,GAAG,KAAK,MAAM,IAAI,SAAO,qBAAqB,GAAG,EAAE,CAAC;AAGlE,YAAM,iBAAiB,MAAM,KAAK;AAAA,QAChC;AAAA,QACA,CAAC,sBAAsB,WAAW;AAAA,QAClC,CAAC,OAAO,YAAY,QAAQ;AAAA,MAC9B;AACA,UAAI,eAAe,SAAS,GAAG;AAC7B,iBAAS,KAAK,SAAS,eAAe,MAAM,eAAe;AAAA,MAC7D;AAGA,YAAM,gBAAgB,CAAC,kBAAkB,cAAc,qBAAqB;AAC5E,YAAM,EAAE,UAAU,mBAAmB,IAAI,MAAM,KAAK,WAAW,aAAa,aAAa;AACzF,eAAS,KAAK,GAAG,mBAAmB,IAAI,OAAK,8BAA8B,EAAE,QAAQ,EAAE,CAAC;AAGxF,YAAM,oBAAoB,MAAM,KAAK;AAAA,QACnC;AAAA,QACA,CAAC,iBAAiB;AAAA,QAClB,CAAC,kBAAkB,cAAc,qBAAqB;AAAA,MACxD;AACA,UAAI,kBAAkB,SAAS,GAAG;AAChC,iBAAS,KAAK,SAAS,kBAAkB,MAAM,kBAAkB;AAAA,MACnE;AAGA,YAAM,aAAa,KAAK,oBAAoB;AAAA,QAC1C,UAAU;AAAA,UACR,OAAO,KAAK,MAAM,SAAS,IAAI,IAAI;AAAA,UACnC,OAAO;AAAA,QACT;AAAA,QACA,UAAU;AAAA,UACR,OAAO,iBAAiB,UAAU,eAAe,SAAS,IAAI,IAAI,KAAK,mBAAmB,UAAU,kBAAkB,SAAS,IAAI,IAAI;AAAA,UACvI,OAAO;AAAA,QACT;AAAA,QACA,UAAU;AAAA,MACZ,CAAC;AAED,aAAO;AAAA,QACL,OAAO,aAAa;AAAA,QACpB,YAAY,KAAK,MAAM,aAAa,GAAG;AAAA,QACvC;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,aAAO;AAAA,QACL,OAAO;AAAA,QACP,YAAY;AAAA,QACZ,UAAU,CAAC,4BAA4B,KAAK,EAAE;AAAA,MAChD;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,aAAoD;AACtE,QAAI;AAEF,YAAM,cAAc;AAAA,QAClB;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,YAAM,EAAE,UAAU,iBAAiB,IAAI,MAAM,KAAK,WAAW,aAAa,WAAW;AACrF,UAAI,iBAAiB,WAAW,GAAG;AACjC,eAAO;AAAA,MACT;AAEA,YAAM,aAAa,iBAAiB,CAAC;AACrC,UAAI,WAAqB,CAAC;AAC1B,UAAI,aAAuB,CAAC;AAE5B,UAAI,WAAW,SAAS,SAAS,OAAO,GAAG;AAEzC,cAAM,gBAAgB,MAAMA,IAAG,SAAS,WAAW,UAAU,OAAO;AACpE,cAAM,aAAa,KAAK,MAAM,aAAa;AAC3C,mBAAW,MAAM,QAAQ,WAAW,QAAQ,IAAI,WAAW,WAAW,CAAC,yBAAyB;AAChG,qBAAa,MAAM,QAAQ,WAAW,UAAU,IAAI,WAAW,aAAa,CAAC,0BAA0B;AAAA,MACzG,OAAO;AAEL,cAAM,gBAAgB,MAAMA,IAAG,SAAS,WAAW,UAAU,OAAO;AACpE,mBAAW,KAAK,kBAAkB,eAAe,UAAU,KAAK,CAAC,yBAAyB;AAC1F,qBAAa,KAAK,kBAAkB,eAAe,YAAY,KAAK,CAAC,0BAA0B;AAAA,MACjG;AAEA,YAAM,SAAwB;AAAA,QAC5B,MAAM;AAAA,QACN;AAAA,QACA;AAAA,QACA;AAAA,QACA,oBAAoB;AAAA;AAAA,QACpB,cAAc,CAAC,SAAS;AAAA,QACxB,KAAK;AAAA,UACH,eAAe;AAAA,UACf,aAAa;AAAA,QACf;AAAA,MACF;AAEA,aAAO;AAAA,IACT,SAAS,OAAO;AACd,cAAQ,KAAK,qCAAqC,KAAK,EAAE;AACzD,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAEA,MAAM,kBAAkB,aAAqD;AAC3E,QAAI;AAEF,YAAM,WAAW,CAAC,QAAQ,cAAc,kBAAkB;AAC1D,YAAM,EAAE,UAAU,cAAc,IAAI,MAAM,KAAK,WAAW,aAAa,QAAQ;AAE/E,iBAAW,WAAW,eAAe;AACnC,cAAM,aAAa,MAAMA,IAAG,SAAS,QAAQ,UAAU,OAAO;AAC9D,cAAM,QAAQ,KAAK,gBAAgB,YAAY,cAAc,KAC/C,KAAK,gBAAgB,YAAY,QAAQ,KACzC,KAAK,gBAAgB,YAAY,aAAa;AAC5D,YAAI,OAAO;AACT,gBAAM,SAAS,KAAK,iBAAiB,KAAK;AAC1C,cAAI,OAAQ,QAAO;AAAA,QACrB;AAAA,MACF;AAGA,YAAM,gBAAgB,MAAM,KAAK,cAAc,WAAW;AAC1D,UAAI,eAAe,YAAY;AAC7B,cAAM,gBAAgB,MAAMA,IAAG,SAAS,cAAc,WAAW,UAAU,OAAO;AAGlF,cAAM,OAAO,KAAK,mBAAmB,eAAe,MAAM;AAC1D,cAAM,OAAO,KAAK,mBAAmB,eAAe,MAAM;AAC1D,cAAM,OAAO,KAAK,mBAAmB,eAAe,MAAM;AAC1D,cAAM,WAAW,KAAK,mBAAmB,eAAe,UAAU;AAClE,cAAM,WAAW,KAAK,mBAAmB,eAAe,UAAU;AAClE,cAAM,WAAW,KAAK,mBAAmB,eAAe,UAAU;AAElE,YAAI,QAAQ,UAAU;AACpB,gBAAM,YAA+D;AAAA,YACnE,YAAY;AAAA,YACZ,cAAc;AAAA,YACd,SAAS;AAAA,YACT,WAAW;AAAA,YACX,UAAU;AAAA,UACZ;AAEA,gBAAM,aAAa,UAAU,IAAI,KAAK;AAEtC,iBAAO;AAAA,YACL,MAAM;AAAA,YACN,MAAM,QAAQ;AAAA,YACd,MAAM,OAAO,SAAS,IAAI,IAAK,eAAe,eAAe,OAAO,eAAe,UAAU,OAAO;AAAA,YACpG;AAAA,YACA;AAAA,YACA;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAGA,aAAO;AAAA,QACL,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,QACN,UAAU;AAAA,MACZ;AAAA,IACF,SAAS,OAAO;AACd,cAAQ,KAAK,sCAAsC,KAAK,EAAE;AAC1D,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAEQ,mBAAmB,SAAiB,KAAiC;AAE3E,UAAM,QAAQ,IAAI,OAAO,GAAG,GAAG,uBAAuB;AACtD,UAAM,QAAQ,QAAQ,MAAM,KAAK;AACjC,WAAO,QAAQ,CAAC;AAAA,EAClB;AAAA,EAEQ,kBAAkB,SAAiB,KAAmC;AAE5E,UAAM,QAAQ,IAAI,OAAO,GAAG,GAAG,sBAAsB;AACrD,UAAM,QAAQ,QAAQ,MAAM,KAAK;AACjC,QAAI,CAAC,MAAO,QAAO;AAEnB,WAAO,MAAM,CAAC,EACX,MAAM,GAAG,EACT,IAAI,UAAQ,KAAK,KAAK,EAAE,QAAQ,SAAS,EAAE,CAAC,EAC5C,OAAO,UAAQ,KAAK,SAAS,CAAC;AAAA,EACnC;AAAA,EAEQ,gBAAgB,SAAiB,KAAiC;AACxE,UAAM,QAAQ,IAAI,OAAO,IAAI,GAAG,kBAAkB,GAAG;AACrD,UAAM,QAAQ,QAAQ,MAAM,KAAK;AACjC,WAAO,QAAQ,CAAC,GAAG,QAAQ,SAAS,EAAE,EAAE,KAAK;AAAA,EAC/C;AACF;;;ANlNA,eAAe,gBAAgB,SAAiB,aAAsC;AACpF,QAAM,iBAA2B,CAAC;AAElC,iBAAe,KAAKC,SAAQ,aAAa,MAAM,CAAC;AAEhD,QAAM,QAAQ,YAAY,MAAM,GAAG;AACnC,WAAS,IAAI,MAAM,SAAS,GAAG,IAAI,GAAG,KAAK;AACzC,mBAAe,KAAK,MAAM,MAAM,GAAG,IAAI,CAAC,EAAE,KAAK,GAAG,IAAI,OAAO;AAAA,EAC/D;AAEA,QAAM,UAAUA,SAAQ,aAAa,MAAM;AAC3C,QAAM,UAAUA,SAAQ,aAAa,UAAU;AAC/C,MAAI,MAAM,QAAQ,WAAW,OAAO,GAAG;AACrC,UAAM,MAAM,MAAM,QAAQ,QAAQ,OAAO;AACzC,QAAI,QAAQ,CAAC,MAAM,eAAe,KAAKA,SAAQ,SAAS,GAAG,MAAM,CAAC,CAAC;AAAA,EACrE;AACA,MAAI,MAAM,QAAQ,WAAW,OAAO,GAAG;AACrC,UAAM,MAAM,MAAM,QAAQ,QAAQ,OAAO;AACzC,QAAI,QAAQ,CAAC,MAAM,eAAe,KAAKA,SAAQ,SAAS,GAAG,MAAM,CAAC,CAAC;AAAA,EACrE;AAEA,aAAW,QAAQ,gBAAgB;AACjC,QAAI,MAAM,QAAQ,WAAW,IAAI,GAAG;AAClC,UAAI;AACF,cAAM,UAAU,OAAO,MAAM,MAAM,QAAQ,SAAS,IAAI,CAAC;AACzD,cAAM,IACJ,QAAQ,gBAAgB,QAAQ,gBAAgB,QAAQ,YAAY,CAAC,EAAE;AACzE,YAAI,EAAG,QAAO;AAAA,MAChB,QAAQ;AAAA,MAAC;AAAA,IACX;AAAA,EACF;AACA,SAAO;AACT;AAEA,eAAe,oBAAoB,aAA6C;AAC9E,QAAM,aAAa;AAAA,IACjB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,QAAM,qBAAqB,CAAC,qBAAqB,qBAAqB,sBAAsB,oBAAoB;AAChH,aAAW,KAAK,oBAAoB;AAClC,QAAI,MAAM,QAAQ,WAAWA,SAAQ,aAAa,CAAC,CAAC,GAAG;AACrD,YAAM,UAAU,MAAM,QAAQ,SAASA,SAAQ,aAAa,CAAC,GAAG,MAAM;AACtE,YAAM,QAAQ,QAAQ,MAAM,2BAA2B;AACvD,UAAI,OAAO;AACT,mBAAW,QAAQ,MAAM,CAAC,CAAC;AAAA,MAC7B;AAAA,IACF;AAAA,EACF;AACA,aAAW,OAAO,YAAY;AAC5B,QAAI,MAAM,QAAQ,WAAWA,SAAQ,aAAa,GAAG,CAAC,EAAG,QAAO;AAAA,EAClE;AACA,SAAO;AACT;AAEA,eAAsB,YAAY,SAAsB,eAA6C;AACnG,QAAM,cAAcA,SAAQ,QAAQ,WAAW,QAAQ,IAAI,CAAC;AAC5D,QAAMC,WAAU,cAAc,gCAAgC;AAE9D,MAAI,SAAiB,aAAqB;AAE1C,MAAI,QAAQ,KAAK;AAEf,cAAU,QAAQ,WAAW;AAC7B,UAAM,aAAa,MAAM,gBAAgB,SAAS,WAAW;AAC7D,kBAAc,QAAQ,SAAS;AAE/B,QAAI,CAAC,aAAa;AAChB,MAAAA,SAAQ,KAAK,0EAA0E;AACvF,YAAM,IAAI,MAAM,iCAAiC;AAAA,IACnD;AAEA,UAAM,eAAe,MAAM,oBAAoB,WAAW;AAC1D,qBAAiB,QAAQ,kBAAkB,gBAAgB;AAAA,EAE7D,OAAO;AAEL,UAAM,eAAe,MAAM,UAAU,oBAAoB;AAAA,MACvD,aAAa;AAAA,MACb,cAAc;AAAA,IAChB,CAAC;AACD,cAAU,cAAc,KAAK,KAAK;AAElC,UAAM,YAAY,MAAM,gBAAgB,SAAS,WAAW;AAC5D,UAAM,WAAW;AACjB,UAAM,UAAU,MAAM,UAAU,UAAU;AAAA,MACxC,aAAa,aAAa;AAAA,MAC1B,cAAc;AAAA,IAChB,CAAC;AACD,mBAAe,SAAS,KAAK,KAAK,WAAW,KAAK;AAElD,QAAI,CAAC,aAAa;AAChB,MAAAA,SAAQ,KAAK,wCAAwC;AACrD,YAAM,IAAI,MAAM,iBAAiB;AAAA,IACnC;AAEA,UAAM,eAAgB,MAAM,oBAAoB,WAAW,KAAM;AACjE,UAAM,WAAW,MAAM,UAAU,6BAA6B;AAAA,MAC5D,aAAa;AAAA,MACb,cAAc;AAAA,IAChB,CAAC;AACD,qBAAiB,UAAU,KAAK,KAAK;AAErC,UAAM,UAAU,MAAM,cAAc,2CAA2C;AAC/E,QAAI,CAAC,SAAS;AACZ,MAAAA,SAAQ,KAAK,gBAAgB;AAC7B;AAAA,IACF;AAAA,EACF;AAEA,EAAAA,SAAQ,OAAO,wBAAwB;AAGvC,QAAM,YAAY;AAAA,IAChB,EAAE,MAAM,UAAU,UAAU,IAAI,eAAe,EAAE;AAAA,IACjD,EAAE,MAAM,WAAW,UAAU,IAAI,gBAAgB,EAAE;AAAA,IACnD,EAAE,MAAM,WAAW,UAAU,IAAI,gBAAgB,EAAE;AAAA,EACrD;AACA,MAAI,cAA6B;AACjC,aAAW,EAAE,MAAM,SAAS,KAAK,WAAW;AAC1C,UAAM,SAAS,MAAM,SAAS,OAAO,WAAW;AAChD,QAAI,OAAO,OAAO;AAChB,oBAAc;AACd;AAAA,IACF;AAAA,EACF;AAGA,QAAM,SAAc;AAAA,IAClB,SAAS;AAAA,IACT,oBAAoB;AAAA,IACpB,GAAI,eAAe,EAAE,KAAK,YAAY;AAAA,IACtC,cAAc;AAAA,MACZ,CAAC,OAAO,GAAG;AAAA,QACT,sBAAsB;AAAA,QACtB;AAAA,MACF;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,MACN,gBAAgB;AAAA,MAChB,eAAe;AAAA,IACjB;AAAA,EACF;AAEA,QAAM,aAAaD,SAAQ,aAAa,cAAc,UAAU,kBAAkB;AAElF,MAAI,MAAM,QAAQ,WAAW,UAAU,KAAK,CAAC,QAAQ,OAAO,CAAE,MAAM,cAAc,sBAAsB,UAAU,GAAG,GAAI;AACvH,IAAAC,SAAQ,KAAK,mCAA8B;AAC3C;AAAA,EACF;AAEA,MAAI,CAAC,cAAc,QAAQ;AACzB,UAAM,QAAQ,UAAU,YAAY,KAAK,UAAU,QAAQ,MAAM,CAAC,CAAC;AACnE,IAAAA,SAAQ,QAAQ,4BAA4B,UAAU,EAAE;AAAA,EAC1D,OAAO;AACL,IAAAA,SAAQ,QAAQ,iDAA4C;AAC5D,YAAQ,IAAI,KAAK,UAAU,QAAQ,MAAM,CAAC,CAAC;AAAA,EAC7C;AAGA,MAAI;AACF,UAAM,UAAUD,SAAQ,aAAa,cAAc;AACnD,QAAI,MAAM,QAAQ,WAAW,OAAO,GAAG;AAErC,YAAM,QAAa,MAAM,OAAO,UAAU;AAC1C,YAAM,QAAS,MAAM,WAAW;AAChC,YAAME,OAAM,MAAM,MAAM,SAAS,OAAO;AACxC,MAAAA,KAAI,UAAUA,KAAI,WAAW,CAAC;AAC9B,UAAI,CAACA,KAAI,QAAQ,MAAM;AACrB,QAAAA,KAAI,QAAQ,OAAO;AACnB,cAAM,MAAM,UAAU,SAASA,MAAK,EAAE,QAAQ,EAAE,CAAC;AACjD,QAAAD,SAAQ,OAAO,qCAAqC;AAAA,MACtD;AAAA,IACF;AAAA,EACF,SAAS,KAAK;AAEZ,YAAQ,KAAK,gDAAsC,eAAe,QAAQ,IAAI,UAAU,GAAG;AAAA,EAC7F;AACF;;;AOxMA,SAAS,WAAAE,gBAAuB;;;ACEhC,OAAOC,cAAa;AACpB,SAAS,WAAAC,UAAS,WAAAC,gBAAe;AAMjC,eAAsB,cAAc,QAAuB,aAAsB;AAC/E,QAAM,aAAa,MAAM,eAAe,eAAe,QAAQ,IAAI,GAAG,OAAO,MAAM;AACnF,SAAO,KAAK,MAAM,MAAMF,SAAQ,SAAS,YAAY,MAAM,CAAC;AAC9D;AAEA,eAAe,eAAe,UAAkB,UAAoC;AAClF,MAAI,UAAU;AACZ,UAAM,IAAIC,SAAQ,QAAQ;AAC1B,QAAI,MAAMD,SAAQ,WAAW,CAAC,EAAG,QAAO;AACxC,UAAM,IAAI,MAAM,4BAA4B,CAAC,EAAE;AAAA,EACjD;AAEA,MAAI,MAAM;AACV,SAAO,MAAM;AACX,UAAM,YAAYC,SAAQ,KAAK,kBAAkB;AACjD,QAAI,MAAMD,SAAQ,WAAW,SAAS,EAAG,QAAO;AAChD,UAAM,SAASE,SAAQ,GAAG;AAC1B,QAAI,WAAW,IAAK;AACpB,UAAM;AAAA,EACR;AACA,QAAM,IAAI,MAAM,4BAA4B;AAC9C;;;AClCA,OAAO,SAAS;AAChB,IAAM,EAAE,OAAO,IAAI;;;ACDnB,YAAYC,SAAQ;AACpB,YAAYC,WAAU;;;AC4Bf,IAAM,kBAAN,MAAsB;AAAA,EAC3B,YAAoB,cAAmC;AAAnC;AAAA,EAAoC;AAAA;AAAA;AAAA;AAAA,EAKxD,MAAM,WAAW,KAAa,eAA0D;AAEtF,UAAM,WAAW,IAAI,YAAY;AACjC,UAAM,iBAAiC,CAAC;AACxC,UAAM,uBAAiC,CAAC;AACxC,UAAMC,YAAqB,CAAC;AAC5B,UAAM,WAAqB,CAAC;AAG5B,UAAM,eAAe,KAAK,yBAAyB;AAEnD,eAAW,WAAW,cAAc;AAClC,UAAI,QAAQ,MAAM,KAAK,QAAQ,GAAG;AAChC,uBAAe,KAAK,QAAQ,IAAI;AAChC,6BAAqB,KAAK,GAAG,QAAQ,WAAW;AAChD,YAAI,QAAQ,UAAW,UAAS,KAAK,QAAQ,KAAK,WAAW;AAC7D,YAAI,QAAQ,UAAW,CAAAA,UAAS,KAAK,QAAQ,KAAK,WAAW;AAAA,MAC/D;AAAA,IACF;AAEA,UAAM,YAAY,KAAK,mBAAmB,cAAc;AACxD,UAAM,YAAY,KAAK,mBAAmB,SAAS;AAEnD,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA,sBAAsB,CAAC,GAAG,IAAI,IAAI,oBAAoB,CAAC;AAAA,MACvD,UAAU,CAAC,GAAG,IAAI,IAAIA,SAAQ,CAAC;AAAA,MAC/B,UAAU,CAAC,GAAG,IAAI,IAAI,QAAQ,CAAC;AAAA,IACjC;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,2BAML;AACD,WAAO;AAAA,MACL;AAAA,QACE,OAAO;AAAA,QACP,MAAM;AAAA,UACJ,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC;AAAA,UAClB,iBAAiB,EAAE,cAAc,KAAK,UAAU,KAAK,oBAAoB,SAAS;AAAA,QACpF;AAAA,QACA,aAAa,CAAC,gCAAgC,gCAAgC,oCAAoC;AAAA,QAClH,WAAW;AAAA,QACX,WAAW;AAAA,MACb;AAAA,MACA;AAAA,QACE,OAAO;AAAA,QACP,MAAM;AAAA,UACJ,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC;AAAA,UAClB,iBAAiB,EAAE,UAAU,MAAM,oBAAoB,aAAa;AAAA,QACtE;AAAA,QACA,aAAa,CAAC,uCAAuC,4BAA4B,kCAAkC;AAAA,QACnH,WAAW;AAAA,QACX,WAAW;AAAA,MACb;AAAA,MACA;AAAA,QACE,OAAO;AAAA,QACP,MAAM;AAAA,UACJ,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC;AAAA,UAClB,iBAAiB,EAAE,cAAc,KAAK,UAAU,KAAK,oBAAoB,OAAO;AAAA,QAClF;AAAA,QACA,aAAa,CAAC,iCAAiC,+BAA+B;AAAA,QAC9E,WAAW;AAAA,QACX,WAAW;AAAA,MACb;AAAA,MACA;AAAA,QACE,OAAO;AAAA,QACP,MAAM;AAAA,UACJ,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC;AAAA,UAClB,iBAAiB,EAAE,cAAc,IAAI,oBAAoB,OAAO;AAAA,QAClE;AAAA,QACA,aAAa,CAAC,uCAAuC,gCAAgC;AAAA,QACrF,WAAW;AAAA,QACX,WAAW;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,mBAAmB,YAAoC;AAC7D,QAAI,QAAQ;AACZ,eAAW,YAAY,YAAY;AACjC,cAAQ,SAAS,UAAU;AAAA,QACzB,KAAK;AAAO,mBAAS;AAAI;AAAA,QACzB,KAAK;AAAU,mBAAS;AAAI;AAAA,QAC5B,KAAK;AAAQ,mBAAS;AAAI;AAAA,QAC1B,KAAK;AAAY,mBAAS;AAAK;AAAA,MACjC;AAAA,IACF;AACA,WAAO,KAAK,IAAI,OAAO,GAAG;AAAA,EAC5B;AAAA,EAEQ,mBAAmB,OAAuD;AAChF,QAAI,SAAS,GAAI,QAAO;AACxB,QAAI,SAAS,GAAI,QAAO;AACxB,QAAI,SAAS,GAAI,QAAO;AACxB,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,gBAAgB,KAAuB;AAE7C,WAAO,IACJ,MAAM,GAAG,EACT,IAAI,UAAQ,KAAK,KAAK,CAAC,EACvB,OAAO,UAAQ,KAAK,SAAS,CAAC;AAAA,EACnC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,iBAAiB,WAAmB,eAK/C;AACD,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAMA,YAAqB,CAAC;AAC5B,UAAM,WAAqB,CAAC;AAE5B,UAAM,iBAAiB,UAAU,YAAY,EAAE,KAAK;AAGpD,UAAM,gBAAgB,KAAK,yBAAyB,gBAAgB,SAAS;AAC7E,eAAW,KAAK,GAAG,cAAc,UAAU;AAC3C,gBAAY,KAAK,GAAG,cAAc,WAAW;AAC7C,IAAAA,UAAS,KAAK,GAAG,cAAc,QAAQ;AAGvC,UAAM,mBAAmB,KAAK,4BAA4B,gBAAgB,SAAS;AACnF,eAAW,KAAK,GAAG,iBAAiB,UAAU;AAC9C,gBAAY,KAAK,GAAG,iBAAiB,WAAW;AAChD,IAAAA,UAAS,KAAK,GAAG,iBAAiB,QAAQ;AAG1C,UAAM,mBAAmB,MAAM,KAAK,yBAAyB,gBAAgB,WAAW,aAAa;AACrG,eAAW,KAAK,GAAG,iBAAiB,UAAU;AAC9C,gBAAY,KAAK,GAAG,iBAAiB,WAAW;AAChD,IAAAA,UAAS,KAAK,GAAG,iBAAiB,QAAQ;AAG1C,UAAM,kBAAkB,KAAK,2BAA2B,gBAAgB,SAAS;AACjF,eAAW,KAAK,GAAG,gBAAgB,UAAU;AAC7C,gBAAY,KAAK,GAAG,gBAAgB,WAAW;AAC/C,IAAAA,UAAS,KAAK,GAAG,gBAAgB,QAAQ;AAGzC,UAAM,gBAAgB,MAAM,KAAK,yBAAyB,gBAAgB,WAAW,aAAa;AAClG,eAAW,KAAK,GAAG,cAAc,UAAU;AAC3C,gBAAY,KAAK,GAAG,cAAc,WAAW;AAC7C,aAAS,KAAK,GAAG,cAAc,QAAQ;AAEvC,WAAO,EAAE,YAAY,aAAa,UAAAA,WAAU,SAAS;AAAA,EACvD;AAAA;AAAA;AAAA;AAAA,EAKQ,yBAAyB,gBAAwB,mBAIvD;AACA,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAMA,YAAqB,CAAC;AAG5B,QAAI,eAAe,SAAS,aAAa,GAAG;AAC1C,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AAErE,UAAI,eAAe,SAAS,YAAY,KAAK,eAAe,SAAS,UAAU,KAAK,CAAC,eAAe,SAAS,SAAS,GAAG;AACvH,mBAAW,KAAK;AAAA,UACd,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,UAC9C,iBAAiB;AAAA,YACf,cAAc;AAAA;AAAA,YACd,oBAAoB;AAAA,UACtB;AAAA,QACF,CAAC;AACD,oBAAY,KAAK,yEAAyE;AAC1F,oBAAY,KAAK,sDAAsD;AAAA,MACzE;AAEA,UAAI,eAAe,SAAS,aAAa,GAAG;AAC1C,mBAAW,KAAK;AAAA,UACd,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,UAC9C,iBAAiB;AAAA,YACf,cAAc;AAAA;AAAA,YACd,oBAAoB;AAAA,UACtB;AAAA,QACF,CAAC;AACD,oBAAY,KAAK,oDAAoD;AAAA,MACvE;AAEA,UAAI,eAAe,SAAS,gBAAgB,KAAK,eAAe,SAAS,aAAa,GAAG;AACvF,mBAAW,KAAK;AAAA,UACd,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,UAC9C,iBAAiB;AAAA,YACf,cAAc;AAAA;AAAA,YACd,oBAAoB;AAAA,UACtB;AAAA,QACF,CAAC;AACD,oBAAY,KAAK,gEAAgE;AACjF,QAAAA,UAAS,KAAK,uDAAuD;AAAA,MACvE;AAEA,UAAI,eAAe,SAAS,gBAAgB,KAAK,eAAe,SAAS,QAAQ,GAAG;AAClF,mBAAW,KAAK;AAAA,UACd,MAAM;AAAA,UACN,UAAU;AAAA,UACV,aAAa;AAAA,UACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,UAC9C,iBAAiB;AAAA,YACf,cAAc;AAAA;AAAA,YACd,oBAAoB;AAAA,UACtB;AAAA,QACF,CAAC;AACD,oBAAY,KAAK,mDAAmD;AACpE,QAAAA,UAAS,KAAK,sDAAsD;AAAA,MACtE;AAAA,IACF;AAGA,QAAI,eAAe,SAAS,cAAc,KAAK,CAAC,eAAe,SAAS,cAAc,GAAG;AACvF,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,IAAI;AAE5D,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,cAAc;AAAA;AAAA,UACd,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AAED,UAAI,KAAK,cAAc,SAAS,cAAc;AAC5C,oBAAY,KAAK,wDAAwD;AAAA,MAC3E;AACA,MAAAA,UAAS,KAAK,2CAA2C;AAAA,IAC3D;AAEA,WAAO,EAAE,YAAY,aAAa,UAAAA,UAAS;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA,EAKQ,4BAA4B,gBAAwB,mBAI1D;AACA,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAMA,YAAqB,CAAC;AAG5B,QAAI,eAAe,SAAS,YAAY,GAAG;AACzC,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,YAAY;AACpE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,qCAAqC;AACtD,kBAAY,KAAK,6CAA6C;AAC9D,MAAAA,UAAS,KAAK,+BAA+B;AAAA,IAC/C;AAEA,QAAI,eAAe,SAAS,aAAa,GAAG;AAC1C,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,8CAA8C;AAC/D,kBAAY,KAAK,8CAA8C;AAC/D,MAAAA,UAAS,KAAK,sCAAsC;AAAA,IACtD;AAEA,QAAI,eAAe,SAAS,gBAAgB,GAAG;AAC7C,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,gBAAgB;AACxE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,4DAA4D;AAC7E,MAAAA,UAAS,KAAK,yCAAyC;AAAA,IACzD;AAGA,QAAI,eAAe,SAAS,aAAa,KAAK,CAAC,eAAe,SAAS,OAAO,GAAG;AAC/E,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,MAAAA,UAAS,KAAK,2CAA2C;AACzD,kBAAY,KAAK,0CAA0C;AAAA,IAC7D;AAEA,QAAI,eAAe,SAAS,QAAQ,KAAK,CAAC,eAAe,SAAS,OAAO,GAAG;AAC1E,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,QAAQ;AAChE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,MAAAA,UAAS,KAAK,2CAA2C;AACzD,kBAAY,KAAK,wCAAwC;AAAA,IAC3D;AAEA,WAAO,EAAE,YAAY,aAAa,UAAAA,UAAS;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,yBAAyB,gBAAwB,mBAA2B,eAIvF;AACD,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAMA,YAAqB,CAAC;AAE5B,QAAI,CAAC,cAAe,QAAO,EAAE,YAAY,aAAa,UAAAA,UAAS;AAG/D,eAAW,SAAS,eAAe;AACjC,YAAM,YAAY,MAAM,KAAK,YAAY;AAEzC,UAAI,eAAe,SAAS,SAAS,GAAG;AAEtC,YAAI,MAAM,WAAW,KAAS;AAC5B,cAAI,eAAe,SAAS,aAAa,GAAG;AAC1C,uBAAW,KAAK;AAAA,cACd,MAAM;AAAA,cACN,UAAU;AAAA,cACV,aAAa,SAAS,MAAM,IAAI,QAAQ,MAAM,SAAS,eAAe,CAAC;AAAA,cACvE,iBAAiB,CAAC,MAAM,IAAI;AAAA,cAC5B,iBAAiB;AAAA,gBACf,cAAc,KAAK,MAAM,MAAM,WAAW,GAAI;AAAA;AAAA,gBAC9C,oBAAoB;AAAA,cACtB;AAAA,YACF,CAAC;AACD,wBAAY,KAAK,wDAAwD;AACzE,wBAAY,KAAK,6CAA6C;AAAA,UAChE;AAEA,cAAI,eAAe,SAAS,cAAc,GAAG;AAC3C,uBAAW,KAAK;AAAA,cACd,MAAM;AAAA,cACN,UAAU;AAAA,cACV,aAAa,iCAAiC,MAAM,IAAI;AAAA,cACxD,iBAAiB,CAAC,MAAM,IAAI;AAAA,cAC5B,iBAAiB;AAAA,gBACf,cAAc,KAAK,MAAM,MAAM,WAAW,GAAI;AAAA;AAAA,gBAC9C,oBAAoB;AAAA,cACtB;AAAA,YACF,CAAC;AACD,wBAAY,KAAK,sCAAsC;AACvD,YAAAA,UAAS,KAAK,iCAAiC;AAAA,UACjD;AAAA,QACF;AAGA,YAAI,eAAe,SAAS,gBAAgB,KAAK,eAAe,SAAS,OAAO,GAAG;AACjF,qBAAW,KAAK;AAAA,YACd,MAAM;AAAA,YACN,UAAU;AAAA,YACV,aAAa,uDAAuD,MAAM,IAAI;AAAA,YAC9E,iBAAiB,CAAC,MAAM,IAAI;AAAA,YAC5B,iBAAiB;AAAA,cACf,cAAc,KAAK,MAAM,MAAM,WAAW,GAAK;AAAA;AAAA,cAC/C,oBAAoB;AAAA,YACtB;AAAA,UACF,CAAC;AACD,UAAAA,UAAS,KAAK,wDAAwD;AAAA,QACxE;AAAA,MACF;AAAA,IACF;AAEA,WAAO,EAAE,YAAY,aAAa,UAAAA,UAAS;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA,EAKQ,2BAA2B,gBAAwB,mBAIzD;AACA,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAMA,YAAqB,CAAC;AAG5B,QAAI,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,UAAU,GAAG;AACjF,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,yDAAyD;AAC1E,kBAAY,KAAK,2DAA2D;AAC5E,MAAAA,UAAS,KAAK,oDAAoD;AAAA,IACpE;AAGA,QAAI,eAAe,SAAS,gBAAgB,KAAK,eAAe,SAAS,QAAQ,GAAG;AAClF,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,4DAA4D;AAC7E,kBAAY,KAAK,kDAAkD;AACnE,MAAAA,UAAS,KAAK,+CAA+C;AAAA,IAC/D;AAEA,WAAO,EAAE,YAAY,aAAa,UAAAA,UAAS;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,yBAAyB,gBAAwB,mBAA2B,eAIvF;AACD,UAAM,aAA6B,CAAC;AACpC,UAAM,cAAwB,CAAC;AAC/B,UAAM,WAAqB,CAAC;AAG5B,QAAI,eAAe,SAAS,cAAc,KAAK,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,WAAW,GAAG;AAC7H,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,wCAAwC;AACzD,kBAAY,KAAK,+CAA+C;AAChE,eAAS,KAAK,uDAAuD;AAAA,IACvE;AAGA,QAAI,eAAe,SAAS,cAAc,KAAK,eAAe,SAAS,MAAM,GAAG;AAC9E,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,iBAAW,KAAK;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,QACV,aAAa;AAAA,QACb,iBAAiB,CAAC,aAAa,eAAe;AAAA,QAC9C,iBAAiB;AAAA,UACf,UAAU;AAAA;AAAA,UACV,oBAAoB;AAAA,QACtB;AAAA,MACF,CAAC;AACD,kBAAY,KAAK,6CAA6C;AAC9D,kBAAY,KAAK,qDAAqD;AACtE,eAAS,KAAK,qDAAqD;AAAA,IACrE;AAEA,WAAO,EAAE,YAAY,aAAa,SAAS;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA,EAKQ,iBAAiB,WAAmB,cAAqC;AAC/E,UAAM,QAAQ,IAAI,OAAO,GAAG,YAAY,uBAAuB,GAAG;AAClE,UAAM,QAAQ,UAAU,MAAM,KAAK;AACnC,WAAO,QAAQ,MAAM,CAAC,IAAI;AAAA,EAC5B;AACF;;;AC1gBO,IAAM,+BAAN,MAAmC;AAAA,EAGxC,YAAoB,cAAkC;AAAlC;AAClB,SAAK,eAAe,IAAI,gBAAgB,YAAY;AAAA,EACtD;AAAA,EAJQ;AAAA;AAAA;AAAA;AAAA,EASR,MAAM,iBACJ,aACA,eACA,SAK8B;AAE9B,UAAM,WAAW,YAAY,YAAY;AAGzC,UAAM,gBAAgB,KAAK,uBAAuB,aAAa,QAAQ;AACvE,UAAM,mBAAmB,KAAK,0BAA0B,aAAa,QAAQ;AAC7E,UAAM,kBAAkB,KAAK,wBAAwB,aAAa,QAAQ;AAC1E,UAAM,0BAA0B,KAAK,4BAA4B,aAAa,QAAQ;AAEtF,UAAM,oBAAoB,cAAc,OAAO,CAAC,OAAO,SAAS,QAAQ,KAAK,mBAAmB,CAAC;AACjG,UAAM,oBAAuC;AAAA,MAC3C,UAAU,oBAAoB;AAAA,MAC9B,qBAAqB,oBAAoB;AAAA,MACzC,aAAa;AAAA,IACf;AAEA,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,cAAc,CAAC;AAAA,IACjB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,uBAAuB,aAAqB,UAAmC;AACrF,UAAM,QAAyB,CAAC;AAGhC,QAAI,SAAS,SAAS,YAAY,KAAK,SAAS,SAAS,UAAU,KAAK,CAAC,SAAS,SAAS,SAAS,GAAG;AACrG,YAAM,cAAc,YAAY,QAAQ,cAAc,EAAE;AACxD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK,cAAc;AAAA,QACnB,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAGD,YAAM,aAAa,SAAS,MAAM,wBAAwB;AAC1D,YAAM,cAAc,SAAS,MAAM,uBAAuB;AAC1D,UAAI,cAAc,aAAa;AAC7B,cAAM,KAAK;AAAA,UACT,YAAY;AAAA,UACZ,aAAa;AAAA,UACb,KAAK,UAAU,WAAW,CAAC,CAAC,QAAQ,YAAY,CAAC,CAAC,eAAe,YAAY,CAAC,CAAC;AAAA,UAC/E,WAAW;AAAA,UACX,mBAAmB;AAAA,UACnB,aAAa;AAAA,UACb,cAAc,CAAC;AAAA,UACf,mBAAmB,CAAC;AAAA,UACpB,WAAW;AAAA,QACb,CAAC;AAED,cAAM,KAAK;AAAA,UACT,YAAY;AAAA,UACZ,aAAa;AAAA,UACb,KAAK,eAAe,WAAW,CAAC,CAAC,iBAAiB,YAAY,CAAC,CAAC;AAAA,UAChE,WAAW;AAAA,UACX,mBAAmB;AAAA,UACnB,aAAa;AAAA,UACb,cAAc,CAAC;AAAA,UACf,mBAAmB,CAAC;AAAA,UACpB,WAAW;AAAA,QACb,CAAC;AAAA,MACH;AAAA,IACF,WAES,SAAS,SAAS,cAAc,KAAK,CAAC,SAAS,SAAS,cAAc,GAAG;AAChF,YAAM,cAAc,YAAY,QAAQ,kBAAkB,2BAA2B;AACrF,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAAA,IACH,WAES,SAAS,SAAS,gBAAgB,GAAG;AAC5C,YAAM,cAAc,YAAY,QAAQ,UAAU,aAAa;AAC/D,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAED,YAAM,kBAAkB,SAAS,MAAM,2BAA2B;AAClE,YAAM,aAAa,SAAS,MAAM,wBAAwB;AAC1D,UAAI,mBAAmB,YAAY;AACjC,cAAM,KAAK;AAAA,UACT,YAAY;AAAA,UACZ,aAAa;AAAA,UACb,KAAK,eAAe,WAAW,CAAC,CAAC,wBAAwB,gBAAgB,CAAC,CAAC;AAAA,UAC3E,WAAW;AAAA,UACX,mBAAmB;AAAA,UACnB,aAAa;AAAA,UACb,cAAc,CAAC;AAAA,UACf,mBAAmB,CAAC;AAAA,UACpB,WAAW;AAAA,QACb,CAAC;AAAA,MACH;AAAA,IACF,OAEK;AACH,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA,EAEQ,0BAA0B,aAAqB,UAAoC;AACzF,UAAM,gBAAgC,CAAC;AAEvC,QAAI,SAAS,SAAS,cAAc,GAAG;AACrC,YAAM,aAAa,SAAS,MAAM,yBAAyB;AAC3D,UAAI,YAAY;AACd,sBAAc,KAAK;AAAA,UACjB,YAAY;AAAA,UACZ,aAAa;AAAA,UACb,KAAK,wBAAwB,WAAW,CAAC,CAAC;AAAA,QAC5C,CAAC;AAAA,MACH;AAAA,IACF,WAAW,SAAS,SAAS,YAAY,GAAG;AAC1C,YAAM,aAAa,SAAS,MAAM,wBAAwB;AAC1D,YAAM,cAAc,SAAS,MAAM,uBAAuB;AAC1D,UAAI,cAAc,aAAa;AAC7B,sBAAc,KAAK;AAAA,UACjB,YAAY;AAAA,UACZ,aAAa;AAAA,UACb,KAAK,eAAe,WAAW,CAAC,CAAC,0BAA0B,YAAY,CAAC,CAAC;AAAA,QAC3E,CAAC;AAAA,MACH;AAAA,IACF;AAEA,WAAO;AAAA,MACL,aAAa,cAAc,SAAS;AAAA,MACpC;AAAA,MACA,oBAAoB,SAAS,SAAS,MAAM;AAAA,MAC5C,oBAAoB;AAAA,MACpB,gBAAgB;AAAA,IAClB;AAAA,EACF;AAAA,EAEQ,wBAAwB,aAAqB,UAAoC;AACvF,UAAM,SAA2B,CAAC;AAElC,QAAI,SAAS,SAAS,aAAa,GAAG;AACpC,YAAM,aAAa,SAAS,MAAM,wBAAwB;AAC1D,UAAI,YAAY;AACd,eAAO,KAAK;AAAA,UACV,WAAW;AAAA,UACX,aAAa;AAAA,UACb,OAAO,+DAA+D,WAAW,CAAC,CAAC;AAAA,UACnF,gBAAgB;AAAA,UAChB,WAAW;AAAA,QACb,CAAmB;AAAA,MACrB;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA,EAEQ,4BAA4B,aAAqB,UAAoC;AAC3F,UAAM,cAAgC,CAAC;AAEvC,QAAI,SAAS,SAAS,cAAc,GAAG;AACrC,YAAM,aAAa,SAAS,MAAM,yBAAyB;AAC3D,UAAI,YAAY;AACd,oBAAY,KAAK;AAAA,UACf,UAAU;AAAA,UACV,aAAa;AAAA,UACb,OAAO,+DAA+D,WAAW,CAAC,CAAC;AAAA,UACnF,gBAAgB;AAAA,UAChB,WAAW;AAAA,QACb,CAAmB;AAAA,MACrB;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,oBACZ,aACA,gBACA,eACA,SAC0B;AAC1B,UAAM,QAAyB,CAAC;AAChC,UAAM,aAAa,KAAK,gBAAgB,WAAW;AAEnD,aAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,KAAK;AAC1C,YAAM,YAAY,WAAW,CAAC;AAC9B,YAAM,iBAAiB,UAAU,YAAY,EAAE,KAAK;AAGpD,UAAI,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,YAAY,GAAG;AACnF,cAAM,KAAK,GAAG,MAAM,KAAK,iBAAiB,WAAW,aAAa,CAAC;AAAA,MACrE,WAAW,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,aAAa,GAAG;AAC3F,cAAM,KAAK,GAAG,MAAM,KAAK,kBAAkB,WAAW,aAAa,CAAC;AAAA,MACtE,WAAW,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,gBAAgB,GAAG;AAC9F,cAAM,KAAK,GAAG,MAAM,KAAK,qBAAqB,WAAW,aAAa,CAAC;AAAA,MACzE,WAAW,eAAe,SAAS,cAAc,GAAG;AAClD,cAAM,KAAK,GAAG,MAAM,KAAK,mBAAmB,WAAW,aAAa,CAAC;AAAA,MACvE,WAAW,eAAe,SAAS,YAAY,GAAG;AAChD,cAAM,KAAK,GAAG,MAAM,KAAK,iBAAiB,WAAW,aAAa,CAAC;AAAA,MACrE,OAAO;AAEL,cAAM,KAAK,KAAK,kBAAkB,WAAW,IAAI,CAAC,CAAC;AAAA,MACrD;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,iBAAiB,WAAmB,eAA2D;AAC3G,UAAM,QAAyB,CAAC;AAChC,UAAM,iBAAiB,UAAU,YAAY;AAC7C,UAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AAGrE,QAAI,eAAe,SAAS,UAAU,KAAK,CAAC,eAAe,SAAS,SAAS,GAAG;AAC9E,YAAM,aAAa,KAAK,kBAAkB,SAAS;AACnD,YAAM,aAAa,KAAK,kBAAkB,SAAS;AAGnD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,cAAc,UAAU,yBAAyB,SAAS;AAAA,QACvE,KAAK,UAAU,QAAQ,cAAc,EAAE,EAAE,KAAK;AAAA,QAC9C,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB;AAAA,UACjB,uEAAuE,SAAS,wBAAwB,UAAU;AAAA,QACpH;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAGD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,yBAAyB,UAAU;AAAA,QAChD,KAAK,UAAU,SAAS,QAAQ,UAAU,oCAAoC,UAAU;AAAA,QACxF,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC,QAAQ;AAAA,QACvB,mBAAmB;AAAA,UACjB,wBAAwB,SAAS,UAAU,UAAU;AAAA,QACvD;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAGD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,qCAAqC,UAAU;AAAA,QAC5D,KAAK,eAAe,SAAS,iBAAiB,UAAU;AAAA,QACxD,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC,QAAQ;AAAA,QACvB,mBAAmB;AAAA,UACjB,0EAA0E,SAAS,wBAAwB,UAAU;AAAA,QACvH;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAAA,IACH,OAAO;AAEL,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,uBAAuB,SAAS;AAAA,QAC7C,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB;AAAA,UACjB,uEAAuE,SAAS;AAAA,QAClF;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,kBAAkB,WAAmB,eAA2D;AAC5G,UAAM,QAAyB,CAAC;AAChC,UAAM,iBAAiB,UAAU,YAAY;AAC7C,UAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,UAAM,aAAa,KAAK,kBAAkB,SAAS;AAGnD,UAAM,KAAK;AAAA,MACT,YAAY;AAAA,MACZ,aAAa,wDAAwD,UAAU;AAAA,MAC/E,KAAK,gBAAgB,SAAS,IAAI,UAAU,yBAAyB,UAAU,SAAS,SAAS;AAAA,MACjG,WAAW;AAAA,MACX,mBAAmB;AAAA,MACnB,aAAa;AAAA,MACb,cAAc,CAAC;AAAA,MACf,mBAAmB;AAAA,QACjB,wBAAwB,SAAS,IAAI,UAAU;AAAA,MACjD;AAAA,MACA,WAAW;AAAA,IACb,CAAC;AAGD,UAAM,KAAK;AAAA,MACT,YAAY;AAAA,MACZ,aAAa,eAAe,UAAU,eAAe,SAAS;AAAA,MAC9D,KAAK;AAAA,MACL,WAAW;AAAA,MACX,mBAAmB;AAAA,MACnB,aAAa;AAAA;AAAA,MACb,cAAc,CAAC,QAAQ;AAAA,MACvB,mBAAmB;AAAA,QACjB,uEAAuE,SAAS,wBAAwB,UAAU;AAAA,MACpH;AAAA,MACA,WAAW;AAAA,IACb,CAAC;AAED,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,qBAAqB,WAAmB,eAA2D;AAC/G,UAAM,QAAyB,CAAC;AAChC,UAAM,iBAAiB,UAAU,YAAY;AAC7C,UAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AAErE,QAAI,eAAe,SAAS,aAAa,GAAG;AAE1C,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB;AAAA,UACjB;AAAA,QACF;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAGD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,uCAAuC,SAAS;AAAA,QAC7D,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC,QAAQ;AAAA,QACvB,mBAAmB;AAAA,UACjB,iFAAiF,SAAS;AAAA,QAC5F;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAAA,IACH,WAAW,eAAe,SAAS,QAAQ,GAAG;AAE5C,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa;AAAA,QACb,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB;AAAA,UACjB;AAAA,QACF;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAGD,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,kCAAkC,SAAS;AAAA,QACxD,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC,QAAQ;AAAA,QACvB,mBAAmB;AAAA,UACjB,iFAAiF,SAAS;AAAA,QAC5F;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAAA,IACH,OAAO;AAEL,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,2BAA2B,SAAS;AAAA,QACjD,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,mBAAmB,WAAmB,eAA2D;AAC7G,UAAM,QAAyB,CAAC;AAChC,UAAM,iBAAiB,UAAU,YAAY;AAC7C,UAAM,YAAY,KAAK,iBAAiB,gBAAgB,IAAI;AAG5D,QAAI,KAAK,aAAa,SAAS,gBAAgB,CAAC,eAAe,SAAS,cAAc,GAAG;AAEvF,YAAM,cAAc,UAAU,QAAQ,iBAAiB,2BAA2B;AAElF,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,sCAAsC,SAAS;AAAA,QAC5D,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB;AAAA,UACjB,sDAAsD,SAAS;AAAA,QACjE;AAAA,QACA,WAAW;AAAA,MACb,CAAC;AAAA,IACH,OAAO;AAEL,YAAM,KAAK;AAAA,QACT,YAAY;AAAA,QACZ,aAAa,yBAAyB,SAAS;AAAA,QAC/C,KAAK;AAAA,QACL,WAAW;AAAA,QACX,mBAAmB;AAAA,QACnB,aAAa;AAAA,QACb,cAAc,CAAC;AAAA,QACf,mBAAmB,CAAC;AAAA,QACpB,WAAW;AAAA,MACb,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,iBAAiB,WAAmB,eAA2D;AAC3G,UAAM,QAAyB,CAAC;AAChC,UAAM,YAAY,KAAK,iBAAiB,UAAU,YAAY,GAAG,YAAY;AAG7E,UAAM,KAAK;AAAA,MACT,YAAY;AAAA,MACZ,aAAa,0BAA0B,SAAS;AAAA,MAChD,KAAK,gBAAgB,SAAS,WAAW,KAAK,IAAI,CAAC,qBAAqB,SAAS;AAAA,MACjF,WAAW;AAAA,MACX,mBAAmB;AAAA,MACnB,aAAa;AAAA,MACb,cAAc,CAAC;AAAA,MACf,mBAAmB;AAAA,QACjB,wBAAwB,SAAS,WAAW,KAAK,IAAI,CAAC;AAAA,MACxD;AAAA,MACA,WAAW;AAAA,IACb,CAAC;AAGD,UAAM,KAAK;AAAA,MACT,YAAY;AAAA,MACZ,aAAa,cAAc,SAAS;AAAA,MACpC,KAAK;AAAA,MACL,WAAW;AAAA,MACX,mBAAmB;AAAA,MACnB,aAAa;AAAA;AAAA,MACb,cAAc,CAAC,QAAQ;AAAA,MACvB,mBAAmB;AAAA,QACjB,sEAAsE,SAAS;AAAA,MACjF;AAAA,MACA,WAAW;AAAA,IACb,CAAC;AAED,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,kBAAkB,WAAmB,YAAmC;AAC9E,WAAO;AAAA,MACL;AAAA,MACA,aAAa,YAAY,UAAU,UAAU,GAAG,EAAE,CAAC,GAAG,UAAU,SAAS,KAAK,QAAQ,EAAE;AAAA,MACxF,KAAK;AAAA,MACL,WAAW;AAAA,MACX,mBAAmB;AAAA,MACnB,aAAa;AAAA,MACb,cAAc,CAAC;AAAA,MACf,mBAAmB,CAAC;AAAA,MACpB,WAAW;AAAA,IACb;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,uBAAuB,OAAwB,gBAAkD;AACvG,UAAM,gBAAgC,CAAC;AACvC,QAAI,cAAc;AAClB,QAAI,qBAAuE;AAC3E,QAAI,qBAAqB;AAGzB,aAAS,IAAI,MAAM,SAAS,GAAG,KAAK,GAAG,KAAK;AAC1C,YAAM,OAAO,MAAM,CAAC;AAEpB,UAAI,CAAC,KAAK,aAAa;AACrB,sBAAc;AACd,6BAAqB;AACrB;AAAA,MACF;AAEA,UAAI,KAAK,cAAc,UAAU,KAAK,cAAc,YAAY;AAC9D,6BAAqB;AACrB,6BAAqB;AAAA,MACvB;AAGA,YAAM,cAAc,KAAK,oBAAoB,IAAI;AACjD,UAAI,aAAa;AACf,sBAAc,KAAK;AAAA,UACjB,YAAY,cAAc,SAAS;AAAA,UACnC,aAAa,aAAa,KAAK,WAAW;AAAA,UAC1C,KAAK;AAAA,UACL,WAAW,WAAW,KAAK,UAAU;AAAA,QACvC,CAAC;AAAA,MACH;AAAA,IACF;AAEA,UAAM,iBAAiB,cAAc,OAAO,CAAC,OAAO,SAAS,QAAQ,IAAI,CAAC;AAE1E,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,oBAAoB,MAAoC;AAC9D,UAAM,MAAM,KAAK,IAAI,YAAY,EAAE,KAAK;AAExC,QAAI,IAAI,SAAS,aAAa,KAAK,IAAI,SAAS,YAAY,GAAG;AAC7D,YAAM,YAAY,KAAK,iBAAiB,KAAK,aAAa;AAC1D,YAAM,aAAa,KAAK,kBAAkB,KAAK,GAAG;AAClD,aAAO,eAAe,SAAS,gBAAgB,UAAU;AAAA,IAC3D;AAEA,QAAI,IAAI,SAAS,cAAc,GAAG;AAChC,YAAM,YAAY,KAAK,iBAAiB,KAAK,GAAG;AAChD,aAAO,cAAc,SAAS;AAAA,IAChC;AAEA,QAAI,IAAI,SAAS,aAAa,KAAK,IAAI,SAAS,gBAAgB,GAAG;AACjE,YAAM,YAAY,KAAK,iBAAiB,KAAK,aAAa;AAC1D,YAAM,iBAAiB,KAAK,sBAAsB,KAAK,GAAG;AAC1D,aAAO,eAAe,SAAS,oBAAoB,cAAc;AAAA,IACnE;AAGA,WAAO,oCAAoC,KAAK,WAAW;AAAA,EAC7D;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,sBAAsB,aAAqB,eAA4D;AACnH,UAAM,SAA2B,CAAC;AAClC,UAAM,iBAAiB,YAAY,YAAY;AAG/C,QAAI,eAAe,SAAS,aAAa,GAAG;AAC1C,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,aAAO,KAAK;AAAA,QACV,WAAW;AAAA,QACX,aAAa,gBAAgB,SAAS;AAAA,QACtC,OAAO,sEAAsE,SAAS;AAAA,QACtF,gBAAgB;AAAA,QAChB,eAAe;AAAA,QACf,eAAe;AAAA,MACjB,CAAC;AAAA,IACH;AAGA,WAAO,KAAK;AAAA,MACV,WAAW;AAAA,MACX,aAAa;AAAA,MACb,OAAO;AAAA,MACP,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,kBAAkB,CAAC,WAAkB;AAEnC,eAAO,EAAE,SAAS,MAAM,SAAS,kCAAkC;AAAA,MACrE;AAAA,IACF,CAAC;AAGD,WAAO,KAAK;AAAA,MACV,WAAW;AAAA,MACX,aAAa;AAAA,MACb,OAAO;AAAA,MACP,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,kBAAkB,CAAC,WAAkB;AAEnC,eAAO,EAAE,SAAS,MAAM,SAAS,iCAAiC;AAAA,MACpE;AAAA,IACF,CAAC;AAED,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,sBAAsB,aAAqB,eAAmD;AACpG,UAAM,cAAgC,CAAC;AACvC,UAAM,iBAAiB,YAAY,YAAY;AAE/C,QAAI,eAAe,SAAS,aAAa,KAAK,eAAe,SAAS,YAAY,GAAG;AACnF,YAAM,YAAY,KAAK,iBAAiB,gBAAgB,aAAa;AACrE,YAAM,aAAa,KAAK,kBAAkB,WAAW;AAErD,kBAAY,KAAK;AAAA,QACf,UAAU;AAAA,QACV,aAAa,iBAAiB,UAAU,uBAAuB,SAAS;AAAA,QACxE,OAAO,uEAAuE,SAAS,wBAAwB,UAAU;AAAA,QACzH,mBAAmB;AAAA,QACnB,YAAY;AAAA,MACd,CAAC;AAAA,IACH;AAEA,QAAI,eAAe,SAAS,cAAc,GAAG;AAC3C,YAAM,YAAY,KAAK,iBAAiB,WAAW;AAEnD,kBAAY,KAAK;AAAA,QACf,UAAU;AAAA,QACV,aAAa,gBAAgB,SAAS;AAAA,QACtC,OAAO;AAAA,QACP,mBAAmB;AAAA,QACnB,YAAY;AAAA,MACd,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,2BAA2B,OAAwB,gBAAmD;AAC5G,UAAM,gBAAgB,MAAM,OAAO,CAAC,KAAK,SAAS,MAAM,KAAK,mBAAmB,CAAC;AACjF,UAAM,cAAc,MAAM,KAAK,UAAQ,KAAK,cAAc,UAAU,KAAK,cAAc,UAAU;AACjG,UAAM,wBAAwB,eAAe,eAAe,KAAK,SAAO,IAAI,SAAS,UAAU;AAE/F,UAAM,cAAc,eAAe,yBAAyB,gBAAgB;AAC5E,UAAM,kBAAkB;AACxB,UAAM,kBAAkB,KAAK,KAAK,gBAAgB,GAAG;AAErD,UAAM,iBAA2B,CAAC;AAClC,QAAI,uBAAuB;AACzB,qBAAe,KAAK,8DAA8D;AAAA,IACpF;AACA,QAAI,aAAa;AACf,qBAAe,KAAK,iDAAiD;AAAA,IACvE;AACA,QAAI,gBAAgB,KAAK;AACvB,qBAAe,KAAK,+CAA+C;AAAA,IACrE;AAEA,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,oBAAoB,OAAkC;AAC5D,UAAM,eAAe,oBAAI,IAAY;AAErC,UAAM,QAAQ,UAAQ;AACpB,WAAK,aAAa,QAAQ,SAAO,aAAa,IAAI,GAAG,CAAC;AAAA,IACxD,CAAC;AAED,WAAO,MAAM,KAAK,YAAY;AAAA,EAChC;AAAA;AAAA,EAGQ,gBAAgB,KAAuB;AAC7C,WAAO,IAAI,MAAM,GAAG,EAAE,IAAI,UAAQ,KAAK,KAAK,CAAC,EAAE,OAAO,UAAQ,KAAK,SAAS,CAAC;AAAA,EAC/E;AAAA,EAEQ,iBAAiB,WAAmB,cAAqC;AAC/E,UAAM,QAAQ,IAAI,OAAO,GAAG,YAAY,uBAAuB,GAAG;AAClE,UAAM,QAAQ,UAAU,MAAM,KAAK;AACnC,WAAO,QAAQ,MAAM,CAAC,IAAI;AAAA,EAC5B;AAAA,EAEQ,kBAAkB,WAAkC;AAC1D,UAAM,iBAAiB,UAAU,MAAM,0BAA0B;AACjE,UAAM,kBAAkB,UAAU,MAAM,2BAA2B;AACnE,WAAO,iBAAiB,CAAC,KAAK,kBAAkB,CAAC,KAAK;AAAA,EACxD;AAAA,EAEQ,kBAAkB,WAAkC;AAC1D,UAAM,QAAQ,UAAU,MAAM,iCAAiC;AAC/D,WAAO,QAAQ,MAAM,CAAC,IAAI;AAAA,EAC5B;AAAA,EAEQ,iBAAiB,WAAkC;AACzD,UAAM,QAAQ,UAAU,MAAM,4BAA4B;AAC1D,WAAO,QAAQ,MAAM,CAAC,IAAI;AAAA,EAC5B;AAAA,EAEQ,sBAAsB,WAAkC;AAC9D,UAAM,QAAQ,UAAU,MAAM,wBAAwB;AACtD,WAAO,QAAQ,MAAM,CAAC,IAAI;AAAA,EAC5B;AACF;;;ACt2BO,IAAM,oBAAN,MAAwB;AAAA,EACrB,OAAO,IAAI,gBAAgB;AAAA,EAC3B,YAAY,IAAI,6BAA6B,CAAC,CAAQ;AAAA,EACtD,mBAAmB,oBAAI,IAA+B;AAAA;AAAA;AAAA;AAAA,EAK9D,MAAa,QAAQ,WAAsD;AAEzE,UAAM,WAAW,KAAK,iBAAiB,UAAU,EAAE;AACnD,QAAI,KAAK,iBAAiB,IAAI,QAAQ,GAAG;AACvC,YAAM,SAAS,KAAK,iBAAiB,IAAI,QAAQ;AACjD,aAAO;AAAA,QACL,GAAG;AAAA,QACH,UAAU;AAAA;AAAA,MACZ;AAAA,IACF;AAGA,UAAM,CAAC,YAAY,QAAQ,IAAI,MAAM,QAAQ,IAAI;AAAA,MAC/C,KAAK,KAAK,WAAW,UAAU,EAAE;AAAA,MACjC,KAAK,UAAU,iBAAiB,UAAU,EAAE;AAAA,IAC9C,CAAC;AAED,UAAM,WAA8B;AAAA,MAClC,UAAU;AAAA,MACV,UAAU;AAAA,QACR,IAAI,SAAS,cAAc,IAAI,OAAK,EAAE,GAAG,EAAE,KAAK,IAAI;AAAA,QACpD,MAAM,SAAS,iBAAiB,cAAc,IAAI,OAAK,EAAE,GAAG,EAAE,KAAK,IAAI;AAAA,QACvE,iBAAiB,SAAS,gBAAgB,IAAI,OAAK,EAAE,KAAK;AAAA,QAC1D,yBAAyB,SAAS,wBAAwB,IAAI,OAAK,EAAE,KAAK;AAAA,QAC1E,kBAAkB,SAAS,iBAAiB,cAAc,IAAI,OAAK,EAAE,GAAG;AAAA,MAC1E;AAAA,MACA,mBAAmB,SAAS;AAAA,IAC9B;AAGA,SAAK,iBAAiB,IAAI,UAAU,QAAQ;AAE5C,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,iBAAiB,KAAqB;AAE5C,QAAI,OAAO;AACX,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,YAAM,OAAO,IAAI,WAAW,CAAC;AAC7B,cAAS,QAAQ,KAAK,OAAQ;AAC9B,aAAO,OAAO;AAAA,IAChB;AACA,WAAO,KAAK,SAAS,EAAE;AAAA,EACzB;AAAA;AAAA;AAAA;AAAA,EAKO,aAAmB;AACxB,SAAK,iBAAiB,MAAM;AAAA,EAC9B;AACF;;;AN9DA,OAAOC,SAAQ;AACf,OAAOC,WAAU;AACjB,SAAS,iBAAiB;AAC1B,OAAO,QAAQ;AACf,SAAS,aAAa;AAStB,eAAsB,YAAY,SAAsB,eAA6C;AACnG,QAAMC,WAAU,cAAc,qDAAqD;AAEnF,QAAM,cAAc,QAAQ,UAAUD,MAAK,QAAQ,QAAQ,OAAO,IAAI,QAAQ,IAAI;AAClF,QAAM,MAAM,MAAM,cAAc,eAAe,WAAW;AAC1D,QAAM,SAAS,IAAI,aAAa,IAAI,kBAAkB;AAEtD,MAAI,cAA6B;AACjC,QAAM,YAAY;AAAA,IAChB,EAAE,MAAM,UAAU,UAAU,IAAI,eAAe,EAAE;AAAA,IACjD,EAAE,MAAM,WAAW,UAAU,IAAI,gBAAgB,EAAE;AAAA,IACnD,EAAE,MAAM,WAAW,UAAU,IAAI,gBAAgB,EAAE;AAAA,EACrD;AAEA,MAAI,QAAQ,OAAO,QAAQ,QAAQ,QAAQ;AACzC,kBAAc,QAAQ;AAAA,EACxB,OAAO;AACL,eAAW,EAAE,MAAM,UAAAE,UAAS,KAAK,WAAW;AAC1C,YAAM,SAAS,MAAMA,UAAS,OAAO,WAAW;AAChD,UAAI,OAAO,OAAO;AAChB,sBAAc;AACd;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,MAAI,CAAC,aAAa;AAChB,IAAAD,SAAQ,KAAK,sBAAsB;AACnC,YAAQ,IAAI,GAAG,IAAI,uFAAuF,CAAC;AAC3G;AAAA,EACF;AAEA,EAAAA,SAAQ,OAAO,YAAY,YAAY,YAAY,CAAC,gCAAgC;AAEpF,QAAM,WAAW,UAAU,KAAK,OAAK,EAAE,SAAS,WAAW,GAAG;AAC9D,MAAI,CAAC,UAAU;AACb,IAAAA,SAAQ,KAAK,oBAAoB;AACjC;AAAA,EACF;AAEA,QAAM,YAAY,MAAM,SAAS,cAAc,WAAW;AAE1D,QAAM,aAAa,MAAM,sBAAsB,aAAa,WAAW,WAAW;AAElF,MAAI,CAAC,cAAc,CAAC,QAAQ,OAAO;AACjC,IAAAA,SAAQ,QAAQ,2BAA2B;AAC3C,YAAQ,IAAI,GAAG,MAAM,+EAAwE,CAAC;AAC9F,YAAQ,IAAI,GAAG,KAAK,iEAAiE,CAAC;AACtF;AAAA,EACF;AAEA,QAAM,gBAAgB,OAAO,kBAAmB,WAAW,oBAAoB,YAAa;AAC5F,QAAM,wBAAwBD,MAAK,QAAQ,aAAa,aAAa;AAErE,MAAI,YAAY;AACd,IAAAC,SAAQ,OAAO,iDAAiD;AAChE,UAAM,oBAAoB,aAAa,WAAW,uBAAuB,eAAe,aAAa,OAAO;AAAA,EAC9G,OAAO;AACL,IAAAA,SAAQ,OAAO,mDAAmD;AAClE,UAAM,0BAA0B,uBAAuB,eAAe,OAAO;AAAA,EAC/E;AAEA,EAAAA,SAAQ,QAAQ,gBAAgB;AAClC;AAEA,eAAe,sBAAsB,KAAa,QAAa,aAAuC;AACpG,UAAQ,KAAK;AAAA,IACX,KAAK;AACH,aAAO,MAAM,mBAAmB,QAAQ,WAAW;AAAA,IACrD,KAAK;AACH,aAAO,MAAM,oBAAoB,QAAQ,WAAW;AAAA,IACtD,KAAK;AACH,aAAO,MAAM,oBAAoB,QAAQ,WAAW;AAAA,IACtD;AACE,aAAO;AAAA,EACX;AACF;AAEA,eAAe,mBAAmB,QAAa,aAAuC;AACpF,MAAI;AACF,UAAM,aAAaD,MAAK,KAAK,aAAa,UAAU,eAAe;AACnE,UAAM,iBAAiBA,MAAK,KAAK,aAAa,UAAU,YAAY;AAEpE,QAAI,CAAC,MAAMD,IAAG,WAAW,UAAU,EAAG,QAAO;AAC7C,QAAI,CAAC,MAAMA,IAAG,WAAW,cAAc,EAAG,QAAO;AAEjD,QAAI;AACF,YAAM,MAAM,6BAA6B,EAAE,KAAK,YAAY,CAAC;AAC7D,UAAI;AACF,cAAM,EAAE,OAAO,IAAI,MAAM,MAAM,8GAA8G,EAAE,KAAK,YAAY,CAAC;AACjK,eAAO,OAAO,KAAK,EAAE,SAAS;AAAA,MAChC,QAAQ;AACN,eAAO;AAAA,MACT;AAAA,IACF,QAAQ;AACN,YAAM,cAAc,MAAMA,IAAG,KAAK,UAAU;AAC5C,YAAM,iBAAiB,MAAMA,IAAG,QAAQ,cAAc;AACtD,UAAI,eAAe,WAAW,EAAG,QAAO;AACxC,YAAM,gBAAgB,eAAe,OAAO,UAAQ,KAAK,MAAM,UAAU,CAAC;AAC1E,UAAI,cAAc,WAAW,EAAG,QAAO;AACvC,YAAM,kBAAkB,cAAc,KAAK,EAAE,IAAI;AACjD,YAAM,sBAAsBC,MAAK,KAAK,gBAAgB,eAAgB;AACtE,YAAM,iBAAiB,MAAMD,IAAG,KAAK,mBAAmB;AACxD,aAAO,YAAY,QAAQ,eAAe;AAAA,IAC5C;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,KAAK,kCAAkC,KAAK;AACpD,WAAO;AAAA,EACT;AACF;AAEA,eAAe,oBAAoB,QAAa,aAAuC;AAErF,SAAO;AACT;AAEA,eAAe,oBAAoB,QAAa,aAAuC;AACrF,MAAI;AACF,QAAI;AACF,YAAM,EAAE,OAAO,IAAI,MAAM,MAAM,8BAA8B,EAAE,KAAK,YAAY,CAAC;AACjF,aAAO,CAAC,OAAO,SAAS,2BAA2B;AAAA,IACrD,QAAQ;AACN,YAAM,aAAa,CAAC,gBAAgB,cAAc,UAAU;AAC5D,YAAM,gBAAgB,QAAQ,sBAAsB;AACpD,YAAM,oBAAoBC,MAAK,KAAK,aAAa,aAAa;AAE9D,UAAI,CAAC,MAAMD,IAAG,WAAW,iBAAiB,EAAG,QAAO;AAEpD,iBAAW,aAAa,YAAY;AAClC,cAAM,gBAAgBC,MAAK,KAAK,aAAa,SAAS;AACtD,YAAI,MAAMD,IAAG,WAAW,aAAa,GAAG;AACtC,gBAAM,cAAc,MAAMA,IAAG,QAAQ,aAAa;AAClD,gBAAM,UAAU,YAAY,OAAO,OAAK,EAAE,SAAS,KAAK,KAAK,EAAE,SAAS,KAAK,CAAC;AAE9E,qBAAW,cAAc,SAAS;AAChC,kBAAM,aAAaC,MAAK,KAAK,eAAe,UAAU;AACtD,kBAAM,cAAc,MAAMD,IAAG,KAAK,UAAU;AAC5C,kBAAM,iBAAiB,MAAMA,IAAG,QAAQ,iBAAiB;AACzD,gBAAI,eAAe,WAAW,EAAG,QAAO;AACxC,kBAAM,kBAAkB,eAAe,KAAK,EAAE,IAAI;AAClD,kBAAM,sBAAsBC,MAAK,KAAK,mBAAmB,eAAgB;AACzE,kBAAM,iBAAiB,MAAMD,IAAG,KAAK,mBAAmB;AACxD,gBAAI,YAAY,QAAQ,eAAe,OAAO;AAC5C,qBAAO;AAAA,YACT;AAAA,UACF;AAAA,QACF;AAAA,MACF;AACA,aAAO;AAAA,IACT;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,KAAK,mCAAmC,KAAK;AACrD,WAAO;AAAA,EACT;AACF;AAEA,eAAe,oBACb,KACA,QACA,eACA,eACA,aACA,SACe;AACf,QAAM,gBAAgB,eAAe,KAAK,IAAI,CAAC;AAC/C,MAAI,cAAc;AAElB,UAAQ,KAAK;AAAA,IACX,KAAK;AACH,oBAAc,iCAAiC,aAAa;AAC5D;AAAA,IACF,KAAK;AAGH,oBAAc;AACd;AAAA,IACF,KAAK;AACH,YAAM,UAAUC,MAAK,KAAK,eAAe,aAAa;AACtD,oBAAc,kCAAkC,OAAO;AACvD;AAAA,EACJ;AAEA,QAAMC,WAAU,cAAc,WAAW,GAAG,2BAA2B;AACvE,MAAI;AACF,UAAM,EAAE,QAAQ,OAAO,IAAI,MAAM,MAAM,aAAa,EAAE,KAAK,aAAa,OAAO,KAAK,CAAC;AACrF,QAAI,cAAc,OAAO;AACvB,cAAQ,IAAI,MAAM;AAClB,UAAI,OAAQ,SAAQ,MAAM,GAAG,OAAO,MAAM,CAAC;AAAA,IAC7C;AACA,IAAAA,SAAQ,QAAQ,uCAAuC;AACvD,UAAM,0BAA0B,eAAe,eAAe,OAAO;AAAA,EACvE,SAAS,OAAY;AACnB,IAAAA,SAAQ,KAAK,8BAA8B;AAC3C,YAAQ,MAAM,GAAG,IAAI,MAAM,UAAU,MAAM,OAAO,CAAC;AACnD,YAAQ,IAAI,GAAG,OAAO;AAAA,EAA2F,WAAW,EAAE,CAAC;AAAA,EACjI;AACF;AAEA,eAAe,0BACb,eACA,eACA,SACe;AACf,QAAMA,WAAU,cAAc,0CAA0C;AACxE,MAAI,CAAE,MAAMF,IAAG,WAAW,aAAa,GAAI;AACzC,IAAAE,SAAQ,KAAK,mCAAmC,aAAa,EAAE;AAC/D;AAAA,EACF;AAEA,QAAM,QAAQ,MAAMF,IAAG,QAAQ,aAAa;AAC5C,QAAM,iBAAiB,MAAM,OAAO,UAAQ,KAAK,SAAS,MAAM,KAAK,KAAK,SAAS,KAAK,KAAK,KAAK,SAAS,KAAK,CAAC;AAEjH,MAAI,eAAe,WAAW,GAAG;AAC/B,IAAAE,SAAQ,QAAQ,sCAAsC;AACtD;AAAA,EACF;AAEA,EAAAA,SAAQ,OAAO,SAAS,eAAe,MAAM,4CAA4C;AAGzF,QAAM,SAAS,IAAI,kBAAkB;AAGrC,EAAAA,SAAQ,OAAO,4BAA4B;AAC3C,QAAM,YAAY,MAAM,QAAQ;AAAA,IAC9B,eAAe,IAAI,OAAO,SAAS;AACjC,YAAM,WAAWD,MAAK,KAAK,eAAe,IAAI;AAC9C,YAAM,UAAU,MAAMD,IAAG,SAAS,UAAU,OAAO;AACnD,YAAM,MAAM,4BAA4B,OAAO;AAC/C,aAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA,eAAe;AAAA,UACb,MAAM;AAAA,UACN,MAAM;AAAA,UACN,IAAI;AAAA,UACJ,MAAM;AAAA,UACN,WAAW,oBAAI,KAAK;AAAA,UACpB,YAAY,CAAC;AAAA,UACb,UAAU;AAAA,QACZ;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH;AAGA,EAAAE,SAAQ,OAAO,qCAAqC;AACpD,QAAM,WAAW,MAAM,QAAQ;AAAA,IAC7B,UAAU,IAAI,OAAO,EAAE,MAAM,UAAU,SAAS,KAAK,cAAc,MAAM;AACvE,YAAM,WAAW,MAAM,OAAO,QAAQ,aAAa;AACnD,aAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA,YAAY,SAAS,SAAS,OAAO,SAAS,SAAS;AAAA,MACzD;AAAA,IACF,CAAC;AAAA,EACH;AAGA,MAAI,iBAAiB;AACrB,aAAW,YAAY,UAAU;AAC/B,QAAI,SAAS,YAAY;AACvB,YAAM,gBAAgB,CAACE,UAAiB,GAAG,IAAI,KAAKA,KAAI,EAAE;AAC1D,YAAM,gBAAgB,CAACA,UAAiB,GAAG,MAAM,KAAKA,KAAI,EAAE;AAC5D,YAAM,OAAO,UAAU,SAAS,SAAS,SAAS,IAAI,SAAS,SAAS,SAAS,EAAE;AACnF,UAAI,aAAa;AACjB,WAAK,QAAQ,UAAQ;AACnB,cAAM,QAAQ,KAAK,QAAQ,gBAAgB,KAAK,UAAU,gBAAgB,GAAG;AAC7E,sBAAc,MAAM,KAAK,KAAK;AAAA,MAChC,CAAC;AACD,cAAQ,IAAI,GAAG,KAAK;AAAA,mBAAsB,SAAS,IAAI,GAAG,CAAC;AAC3D,cAAQ,IAAI,UAAU;AAEtB,YAAM,UAAU,QAAQ,MAAM,OAAO,MAAMC,SAAQ,EAAE,SAAS,+BAA+B,SAAS,IAAI,IAAI,CAAC;AAE/G,UAAI,SAAS;AACX,YAAI;AACF,gBAAM,aAAa,MAAM,kCAAkC,SAAS,UAAU,SAAS,SAAS,SAAS,IAAI,SAAS,SAAS,SAAS,IAAI;AAC5I,gBAAML,IAAG,UAAU,SAAS,UAAU,YAAY,OAAO;AACzD,kBAAQ,IAAI,GAAG,MAAM,kBAAa,SAAS,IAAI,EAAE,CAAC;AAClD;AAAA,QACF,SAAS,OAAO;AACd,kBAAQ,IAAI,GAAG,OAAO,gDAAsC,SAAS,IAAI,KAAK,KAAK,EAAE,CAAC;AACtF,kBAAQ,IAAI,GAAG,KAAK,kBAAkB,CAAC;AACvC,kBAAQ,IAAI,SAAS,SAAS,SAAS,EAAE;AACzC,cAAI,SAAS,SAAS,SAAS,MAAM;AACnC,oBAAQ,IAAI,GAAG,KAAK,oBAAoB,CAAC;AACzC,oBAAQ,IAAI,SAAS,SAAS,SAAS,IAAI;AAAA,UAC7C;AAAA,QACF;AAAA,MACF,OAAO;AACL,gBAAQ,IAAI,GAAG,KAAK,WAAW,SAAS,IAAI,EAAE,CAAC;AAAA,MACjD;AAAA,IACF;AAAA,EACF;AAEA,MAAI,mBAAmB,KAAK,SAAS,MAAM,OAAK,CAAC,EAAE,UAAU,GAAG;AAC9D,YAAQ,IAAI,GAAG,KAAK,iDAAiD,CAAC;AAAA,EACxE;AACA,EAAAE,SAAQ,QAAQ,iCAAiC;AACnD;AAEA,SAAS,4BAA4B,SAAyB;AAC5D,QAAM,cAAc;AAAA,IAClB;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACA,MAAI,eAAe;AACnB,aAAW,WAAW,aAAa;AACjC,QAAI;AACJ,YAAQ,QAAQ,QAAQ,KAAK,OAAO,OAAO,MAAM;AAC/C,sBAAgB,MAAM,CAAC,IAAI;AAAA,IAC7B;AAAA,EACF;AACA,SAAO,gBAAgB;AACzB;AAEA,eAAe,kCAAkC,UAAkB,OAAe,SAAmC;AACnH,QAAM,UAAU,MAAMF,IAAG,SAAS,UAAU,OAAO;AACnD,MAAI,iBAAiB;AAGrB,QAAM,WAAWC,MAAK,SAAS,QAAQ;AACvC,QAAM,qBAAqB,2BAA2B,UAAU,KAAK;AAGrE,MAAI,CAAC,eAAe,SAAS,qBAAqB,GAAG;AACnD,UAAM,QAAQ,eAAe,MAAM,IAAI;AACvC,QAAI,cAAc;AAGlB,aAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,YAAM,OAAO,MAAM,CAAC,EAAE,KAAK;AAC3B,UAAI,QAAQ,CAAC,KAAK,WAAW,IAAI,KAAK,CAAC,KAAK,WAAW,IAAI,KAAK,CAAC,KAAK,WAAW,GAAG,KAAK,CAAC,KAAK,WAAW,QAAQ,KAAK,CAAC,KAAK,WAAW,QAAQ,GAAG;AACjJ,sBAAc;AACd;AAAA,MACF;AAAA,IACF;AAEA,UAAM,OAAO,aAAa,GAAG,oBAAoB,EAAE;AACnD,qBAAiB,MAAM,KAAK,IAAI;AAAA,EAClC;AAEA,mBAAiB,eAAe,QAAQ,kDAAkD,uBAAuB,MAAM,KAAK,CAAC,KAAK;AAClI,mBAAiB,eAAe,QAAQ,oBAAoB,QAAQ,MAAM,KAAK,CAAC,IAAI;AACpF,mBAAiB,eAAe,QAAQ,yDAAyD,IAAI,MAAM,KAAK,CAAC,GAAG;AAEpH,MAAI,SAAS;AACX,qBAAiB,eAAe,QAAQ,uGAAuG,KAAK,QAAQ,KAAK,CAAC,IAAI;AAAA,EACxK;AAEA,SAAO;AACT;AAEA,SAAS,2BAA2B,UAAkB,aAA6B;AACjF,QAAM,aAAY,oBAAI,KAAK,GAAE,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AACvD,QAAM,aAAa,oBAAoB,WAAW;AAElD,QAAM,SAAS;AAAA;AAAA,WAEN,QAAQ;AAAA,eACJ,SAAS;AAAA;AAAA;AAAA;AAAA;AAMtB,MAAI,WAAW,WAAW,GAAG;AAC3B,WAAO,GAAG,OAAO,MAAM,GAAG,EAAE,CAAC;AAAA;AAAA,kBAEf,WAAW,CAAC,CAAC;AAAA;AAAA,EAE7B,WAAW,WAAW,SAAS,GAAG;AAChC,UAAM,kBAAkB,WAAW,IAAI,QAAM,QAAQ,EAAE,EAAE,EAAE,KAAK,IAAI;AACpE,WAAO,GAAG,OAAO,MAAM,GAAG,EAAE,CAAC;AAAA;AAAA;AAAA,EAG/B,eAAe;AAAA;AAAA,EAEf;AAEA,SAAO;AACT;AAEA,SAAS,oBAAoB,KAAuB;AAClD,QAAM,eAAyB,CAAC;AAChC,QAAM,WAAW,IAAI,YAAY;AAGjC,MAAI,SAAS,SAAS,gBAAgB,KAAK,SAAS,SAAS,WAAW,GAAG;AACzE,iBAAa,KAAK,sDAAsD;AAAA,EAC1E;AACA,MAAI,SAAS,SAAS,cAAc,GAAG;AACrC,iBAAa,KAAK,wCAAwC;AAAA,EAC5D;AACA,MAAI,SAAS,SAAS,QAAQ,KAAK,SAAS,SAAS,MAAM,GAAG;AAC5D,iBAAa,KAAK,mDAAmD;AAAA,EACvE;AACA,MAAI,SAAS,SAAS,aAAa,KAAK,SAAS,SAAS,YAAY,KAAK,CAAC,SAAS,SAAS,UAAU,GAAG;AACzG,iBAAa,KAAK,yDAAyD;AAAA,EAC7E;AACA,MAAI,SAAS,SAAS,qBAAqB,GAAG;AAC5C,iBAAa,KAAK,sDAAsD;AAAA,EAC1E;AACA,MAAI,SAAS,SAAS,cAAc,KAAK,SAAS,SAAS,mBAAmB,GAAG;AAC/E,iBAAa,KAAK,iDAAiD;AAAA,EACrE;AACA,MAAI,SAAS,SAAS,QAAQ,KAAK,SAAS,SAAS,SAAS,GAAG;AAC/D,iBAAa,KAAK,6CAA6C;AAAA,EACjE;AAGA,MAAI,aAAa,WAAW,GAAG;AAC7B,iBAAa,KAAK,kDAAkD;AAAA,EACtE;AAEA,SAAO;AACT;;;AOzbA,SAAS,WAAAK,gBAAe;AAExB,OAAOC,WAAU;AAMjB,eAAsB,YAAY,SAAsB,eAA6C;AACnG,QAAM,IAAIC,SAAQ;AAClB,QAAM,cAAc,QAAQ,UAAUD,MAAK,QAAQ,QAAQ,OAAO,IAAI,QAAQ,IAAI;AAClF,QAAM,MAAM,MAAM,cAAc,eAAe,WAAW;AAC1D,MAAI,cAAc,OAAO;AACvB,YAAQ,IAAI,mCAAmC,IAAI,kBAAkB;AAAA,EACvE;AAEA,IAAE,MAAM,4BAA4B;AAGpC,QAAM,IAAI,QAAQ,CAAAE,aAAW,WAAWA,UAAS,GAAI,CAAC;AAEtD,IAAE,KAAK,wBAAwB;AAE/B,UAAQ,IAAI,iCAA4B;AAC1C;;;ACxBA,SAAS,WAAAC,gBAAuB;AAGhC,OAAOC,SAAQ;AACf,OAAOC,WAAU;AACjB,OAAOC,SAAQ;AACf,SAAS,UAAU,gBAAgB;AACnC,OAAO,WAAW;AAClB,OAAO,cAAc;AAcrB,eAAsB,aAAa,SAAuB,eAA6C;AACrG,QAAM,cAAc,QAAQ,UAAUD,MAAK,QAAQ,QAAQ,OAAO,IAAI,QAAQ,IAAI;AAClF,QAAM,MAAM,MAAM,cAAc,eAAe,WAAW;AAC1D,QAAM,SAAS,IAAI,aAAa,IAAI,kBAAkB;AAEtD,MAAI,cAAc,OAAO;AACvB,YAAQ,IAAI,iCAAiC,IAAI,kBAAkB;AAAA,EACrE;AAEA,QAAME,WAAU,cAAc,2BAA2B;AAGzD,MAAI,aAAwC;AAC5C,MAAI;AACF,YAAQ,IAAI,2BAA2B;AACvC,iBAAa,MAAM,kBAAkB,MAAM;AAC3C,YAAQ,IAAI,uBAAuB;AACnC,IAAAA,SAAQ,OAAO,oCAAoC;AAAA,EACrD,SAAS,OAAO;AACd,YAAQ,IAAI,gCAAgC,KAAK;AACjD,IAAAA,SAAQ,KAAK,+BAA+B;AAC5C,YAAQ,IAAID,IAAG,IAAI,sCAAiC,KAAK,EAAE,CAAC;AAC5D;AAAA,EACF;AAEA,MAAI;AAEF,IAAAC,SAAQ,OAAO,6CAA6C;AAC5D,UAAM,sBAAsB,UAAU;AACtC,YAAQ,IAAI,0BAA0B;AAGtC,IAAAA,SAAQ,OAAO,gCAAgC;AAC/C,UAAM,gBAAgB,OAAO,kBAAkB;AAC/C,UAAM,wBAAwBF,MAAK,QAAQ,aAAa,aAAa;AAErE,UAAM,oBAAoB,MAAM,sBAAsB,YAAY,uBAAuB,QAAQ,SAAS;AAC1G,YAAQ,IAAI,0BAA0B;AAEtC,QAAI,kBAAkB,WAAW,GAAG;AAClC,MAAAE,SAAQ,QAAQ,gCAAgC;AAChD,cAAQ,IAAID,IAAG,MAAM,+BAA0B,CAAC;AAChD;AAAA,IACF;AAEA,IAAAC,SAAQ,KAAK;AAEb,YAAQ,IAAI;AAAA,kBAAc,kBAAkB,MAAM,wBAAwB;AAC1E,sBAAkB,QAAQ,CAAC,WAAW,QAAQ;AAC5C,cAAQ,IAAI,KAAK,MAAM,CAAC,KAAKD,IAAG,KAAK,UAAU,IAAI,CAAC,EAAE;AAAA,IACxD,CAAC;AAED,QAAI,cAAc,QAAQ;AACxB,cAAQ,IAAIA,IAAG,OAAO,2DAAoD,CAAC;AAC3E,iBAAW,aAAa,mBAAmB;AACzC,gBAAQ,IAAI;AAAA,EAAKA,IAAG,KAAK,OAAO,UAAU,IAAI,MAAM,CAAC,EAAE;AACvD,gBAAQ,IAAIA,IAAG,KAAK,UAAU,OAAO,CAAC;AAAA,MACxC;AACA;AAAA,IACF;AAEA,UAAM,UAAU,QAAQ,MAAM,OAAO,MAAME,SAAQ;AAAA,MACjD,SAAS,SAAS,kBAAkB,MAAM;AAAA,IAC5C,CAAC;AACD,YAAQ,IAAI,iBAAiB;AAE7B,QAAI,CAAC,SAAS;AACZ,cAAQ,IAAIF,IAAG,KAAK,sBAAsB,CAAC;AAC3C;AAAA,IACF;AAGA,UAAM,eAAe,cAAc,wBAAwB;AAE3D,eAAW,aAAa,mBAAmB;AACzC,UAAI;AACF,qBAAa,OAAO,YAAY,UAAU,IAAI,KAAK;AAEnD,cAAM,eAAe,YAAY,SAAS;AAC1C,cAAM,uBAAuB,YAAY,SAAS;AAElD,YAAI,cAAc,OAAO;AACvB,kBAAQ,IAAIA,IAAG,MAAM,oBAAe,UAAU,IAAI,EAAE,CAAC;AAAA,QACvD;AAAA,MACF,SAAS,OAAO;AACd,qBAAa,KAAK,mBAAmB,UAAU,IAAI,EAAE;AACrD,gBAAQ,IAAIA,IAAG,IAAI,4BAAuB,KAAK,EAAE,CAAC;AAGlD,YAAI,kBAAkB,QAAQ,SAAS,IAAI,kBAAkB,SAAS,GAAG;AACvE,gBAAM,mBAAmB,MAAME,SAAQ;AAAA,YACrC,SAAS;AAAA,UACX,CAAC;AAED,cAAI,CAAC,kBAAkB;AACrB,oBAAQ,IAAIF,IAAG,OAAO,yCAA+B,CAAC;AACtD;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,iBAAa,QAAQ,qCAAqC;AAC1D,YAAQ,IAAIA,IAAG,MAAM,iDAA4C,CAAC;AAAA,EAEpE,UAAE;AAEA,QAAI,YAAY;AACd,YAAM,wBAAwB,UAAU;AAAA,IAC1C;AAAA,EACF;AACF;AAEA,eAAe,kBAAkB,QAA0C;AACzE,UAAQ,IAAI,sBAAsB;AAClC,QAAM,mBAAmB,OAAO,wBAAwB,OAAO;AAC/D,UAAQ,IAAI,qBAAqB,gBAAgB;AAEjD,MAAI,CAAC,kBAAkB;AACrB,UAAM,IAAI,MAAM,mHAAmH;AAAA,EACrI;AAEA,QAAM,SAAS,iBAAiB,MAAM,GAAG,EAAE,CAAC;AAC5C,UAAQ,IAAI,WAAW,MAAM;AAE7B,UAAQ,QAAQ;AAAA,IACd,KAAK;AACH,cAAQ,IAAI,0BAA0B;AACtC,YAAM,WAAW,IAAI,SAAS,EAAE,iBAAiB,CAAC;AAClD,YAAM,SAAS,QAAQ;AACvB,cAAQ,IAAI,yBAAyB;AACrC,aAAO,EAAE,MAAM,cAAc,QAAQ,SAAS;AAAA,IAEhD,KAAK;AACH,cAAQ,IAAI,qBAAqB;AACjC,YAAM,kBAAkB,MAAM,MAAM,iBAAiB,gBAAgB;AACrE,cAAQ,IAAI,oBAAoB;AAChC,aAAO,EAAE,MAAM,SAAS,QAAQ,gBAAgB;AAAA,IAElD,KAAK;AACH,cAAQ,IAAI,sBAAsB;AAClC,YAAM,aAAa,iBAAiB,UAAU,UAAU,MAAM;AAC9D,YAAM,WAAW,IAAI,SAAS,cAAc,eAAe;AAC3D,cAAQ,IAAI,qBAAqB;AACjC,aAAO,EAAE,MAAM,UAAU,QAAQ,SAAS;AAAA,IAE5C;AACE,YAAM,IAAI,MAAM,8BAA8B,MAAM,EAAE;AAAA,EAC1D;AACF;AAEA,eAAe,sBAAsB,YAA+C;AAClF,QAAM,oBAAoB,MAAM;AAC9B,YAAQ,WAAW,MAAM;AAAA,MACvB,KAAK;AACH,eAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQT,KAAK;AACH,eAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQT,KAAK;AACH,eAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAQX;AAAA,EACF,GAAG;AAEH,QAAM,aAAa,YAAY,gBAAgB;AACjD;AAEA,eAAe,sBAAsB,YAAgC,eAAuB,iBAAyF;AACnL,MAAI,CAAC,MAAMF,IAAG,WAAW,aAAa,GAAG;AACvC,UAAM,IAAI,MAAM,mCAAmC,aAAa,EAAE;AAAA,EACpE;AAEA,QAAM,oBAAoB,MAAM,aAAa,YAAY,sDAAsD;AAC/G,QAAM,eAAe,IAAI,IAAI,kBAAkB,IAAI,CAAC,QAAa,IAAI,IAAI,CAAC;AAE1E,QAAM,WAAW,MAAMA,IAAG,QAAQ,aAAa;AAC/C,QAAM,iBAAiB,SACpB,OAAO,OAAK,EAAE,SAAS,MAAM,KAAK,EAAE,SAAS,KAAK,KAAK,EAAE,SAAS,KAAK,CAAC,EACxE,KAAK;AAER,QAAM,oBAAoB,CAAC;AAE3B,aAAW,QAAQ,gBAAgB;AACjC,UAAM,gBAAgBC,MAAK,MAAM,IAAI,EAAE;AAEvC,QAAI,aAAa,IAAI,aAAa,GAAG;AACnC;AAAA,IACF;AAEA,QAAI,mBAAmB,kBAAkB,iBAAiB;AACxD;AAAA,IACF;AAEA,UAAM,WAAWA,MAAK,KAAK,eAAe,IAAI;AAC9C,QAAI,UAAU,MAAMD,IAAG,SAAS,UAAU,OAAO;AAEjD,QAAI,KAAK,SAAS,KAAK,KAAK,KAAK,SAAS,KAAK,GAAG;AAChD,gBAAUK,6BAA4B,OAAO;AAAA,IAC/C;AAEA,sBAAkB,KAAK;AAAA,MACrB,MAAM;AAAA,MACN,MAAM;AAAA,MACN;AAAA,IACF,CAAC;AAED,QAAI,mBAAmB,kBAAkB,iBAAiB;AACxD;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAEA,eAAe,eAAe,YAAgC,WAA2D;AAEvH,QAAM,aAAa,UAAU,QAC1B,MAAM,GAAG,EACT,IAAI,UAAQ,KAAK,KAAK,CAAC,EACvB,OAAO,UAAQ,KAAK,SAAS,CAAC;AAEjC,aAAW,aAAa,YAAY;AAClC,QAAI,UAAU,KAAK,GAAG;AACpB,YAAM,aAAa,YAAY,SAAS;AAAA,IAC1C;AAAA,EACF;AACF;AAEA,eAAe,uBAAuB,YAAgC,WAA2D;AAE/H,QAAM,WAAW,OAAO,KAAK,UAAU,OAAO,EAAE,SAAS,QAAQ,EAAE,MAAM,GAAG,EAAE;AAE9E,QAAM,eAAe,MAAM;AACzB,YAAQ,WAAW,MAAM;AAAA,MACvB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,IACX;AAAA,EACF,GAAG;AAEH,QAAM,aAAa,YAAY,aAAa,CAAC,UAAU,MAAM,QAAQ,CAAC;AACxE;AAEA,eAAe,aAAa,YAAgC,OAAe,QAAgC;AACzG,UAAQ,WAAW,MAAM;AAAA,IACvB,KAAK;AACH,YAAM,WAAW,MAAM,WAAW,OAAO,MAAM,OAAO,MAAM;AAC5D,aAAO,SAAS;AAAA,IAElB,KAAK;AACH,YAAM,CAAC,WAAW,IAAI,MAAM,WAAW,OAAO,QAAQ,OAAO,MAAM;AACnE,aAAO;AAAA,IAET,KAAK;AACH,YAAM,OAAO,WAAW,OAAO,QAAQ,KAAK;AAC5C,aAAO,KAAK,IAAI,MAAM;AAAA,EAC1B;AACF;AAEA,eAAe,wBAAwB,YAA+C;AACpF,UAAQ,WAAW,MAAM;AAAA,IACvB,KAAK;AACH,YAAM,WAAW,OAAO,IAAI;AAC5B;AAAA,IACF,KAAK;AACH,YAAM,WAAW,OAAO,IAAI;AAC5B;AAAA,IACF,KAAK;AACH,iBAAW,OAAO,MAAM;AACxB;AAAA,EACJ;AACF;AAEA,SAASA,6BAA4B,SAAyB;AAG5D,QAAM,QAAQ,QAAQ,MAAM,gBAAgB;AAE5C,QAAM,eAAe,QAAQ,MAAM,CAAC,EAAE,KAAK,IAAI;AAE/C,MAAI,CAAC,cAAc;AAEjB,QAAI,QAAQ,SAAS,QAAQ,KAAK,QAAQ,SAAS,OAAO,KAAK,QAAQ,SAAS,QAAQ,KAAK,QAAQ,SAAS,QAAQ,KAAK,QAAQ,SAAS,QAAQ,GAAG;AACrJ,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO,gBAAgB;AACzB;;;AC7UA,SAAS,WAAAC,gBAAuB;AAGhC,OAAOC,SAAQ;AACf,OAAOC,WAAU;AACjB,OAAOC,SAAQ;AACf,SAAS,UAAUC,iBAAgB;AACnC,OAAOC,YAAW;AAClB,OAAOC,eAAc;AAqBrB,eAAsB,YAAY,SAAsB,eAA6C;AACnG,QAAM,cAAc,QAAQ,UAAUJ,MAAK,QAAQ,QAAQ,OAAO,IAAI,QAAQ,IAAI;AAClF,QAAM,MAAM,MAAM,cAAc,eAAe,WAAW;AAC1D,QAAM,SAAS,IAAI,aAAa,IAAI,kBAAkB;AAEtD,MAAI,cAAc,OAAO;AACvB,YAAQ,IAAI,2BAA2B,IAAI,kBAAkB;AAAA,EAC/D;AAEA,QAAMK,WAAU,cAAc,2BAA2B;AAGzD,MAAI,aAAwC;AAC5C,MAAI;AACF,iBAAa,MAAMC,mBAAkB,MAAM;AAC3C,IAAAD,SAAQ,OAAO,oCAAoC;AAAA,EACrD,SAAS,OAAO;AACd,IAAAA,SAAQ,KAAK,+BAA+B;AAC5C,YAAQ,IAAIJ,IAAG,IAAI,sCAAiC,KAAK,EAAE,CAAC;AAC5D;AAAA,EACF;AAEA,MAAI;AAEF,IAAAI,SAAQ,OAAO,gCAAgC;AAC/C,UAAM,oBAAoB,MAAM,qBAAqB,UAAU;AAE/D,QAAI,kBAAkB,WAAW,GAAG;AAClC,MAAAA,SAAQ,QAAQ,2BAA2B;AAC3C,cAAQ,IAAIJ,IAAG,MAAM,wCAAmC,CAAC;AACzD;AAAA,IACF;AAEA,IAAAI,SAAQ,KAAK;AAGb,UAAM,uBAAuB,MAAM;AAAA,MACjC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,QAAI,qBAAqB,WAAW,GAAG;AACrC,cAAQ,IAAIJ,IAAG,OAAO,mDAAyC,CAAC;AAChE;AAAA,IACF;AAEA,YAAQ,IAAI;AAAA,oCAAgC,qBAAqB,MAAM,IAAI;AAC3E,yBAAqB,QAAQ,CAAC,WAAW,QAAQ;AAC/C,cAAQ,IAAI,KAAK,MAAM,CAAC,KAAKA,IAAG,KAAK,UAAU,IAAI,CAAC,cAAc,UAAU,UAAU,GAAG;AAAA,IAC3F,CAAC;AAED,QAAI,cAAc,QAAQ;AACxB,cAAQ,IAAIA,IAAG,OAAO,8DAAuD,CAAC;AAC9E,iBAAW,aAAa,sBAAsB;AAC5C,gBAAQ,IAAI;AAAA,EAAKA,IAAG,KAAK,gBAAgB,UAAU,IAAI,MAAM,CAAC,EAAE;AAGhE,cAAM,cAAc,MAAM,kBAAkB,WAAW,QAAQ,WAAW;AAC1E,YAAI,aAAa;AACf,kBAAQ,IAAIA,IAAG,KAAK,WAAW,CAAC;AAAA,QAClC,OAAO;AACL,kBAAQ,IAAIA,IAAG,OAAO,2DAAiD,CAAC;AAAA,QAC1E;AAAA,MACF;AACA;AAAA,IACF;AAGA,UAAM,kBAAkB,qBAAqB;AAAA,MAAO,OAClD,EAAE,KAAK,YAAY,EAAE,SAAS,MAAM,KACpC,EAAE,KAAK,YAAY,EAAE,SAAS,QAAQ;AAAA,IACxC;AAEA,QAAI,gBAAgB,SAAS,GAAG;AAC9B,cAAQ,IAAIA,IAAG,IAAI,8EAAoE,CAAC;AACxF,sBAAgB,QAAQ,OAAK;AAC3B,gBAAQ,IAAI,YAAOA,IAAG,IAAI,EAAE,IAAI,CAAC,EAAE;AAAA,MACrC,CAAC;AAAA,IACH;AAEA,UAAM,UAAU,QAAQ,MAAM,OAAO,MAAMM,SAAQ;AAAA,MACjD,SAAS,gBAAgB,SAAS,IAC9BN,IAAG,IAAI,2FAAiF,IACxF,YAAY,qBAAqB,MAAM;AAAA,IAC7C,CAAC;AAED,QAAI,CAAC,SAAS;AACZ,cAAQ,IAAIA,IAAG,KAAK,qBAAqB,CAAC;AAC1C;AAAA,IACF;AAGA,UAAM,kBAAkB,cAAc,4BAA4B;AAElE,eAAW,aAAa,sBAAsB;AAC5C,UAAI;AACF,wBAAgB,OAAO,gBAAgB,UAAU,IAAI,KAAK;AAE1D,cAAM,cAAc,MAAM,kBAAkB,WAAW,QAAQ,WAAW;AAC1E,YAAI,aAAa;AACf,gBAAM,yBAAyB,YAAY,WAAW;AAAA,QACxD,OAAO;AACL,kBAAQ,IAAIA,IAAG,OAAO,8CAAoC,UAAU,IAAI,eAAe,CAAC;AAAA,QAC1F;AAEA,cAAM,sBAAsB,YAAY,SAAS;AAEjD,YAAI,cAAc,OAAO;AACvB,kBAAQ,IAAIA,IAAG,MAAM,wBAAmB,UAAU,IAAI,EAAE,CAAC;AAAA,QAC3D;AAAA,MACF,SAAS,OAAO;AACd,wBAAgB,KAAK,sBAAsB,UAAU,IAAI,EAAE;AAC3D,gBAAQ,IAAIA,IAAG,IAAI,2BAAsB,KAAK,EAAE,CAAC;AAGjD,YAAI,qBAAqB,QAAQ,SAAS,IAAI,qBAAqB,SAAS,GAAG;AAC7E,gBAAM,mBAAmB,MAAMM,SAAQ;AAAA,YACrC,SAAS;AAAA,UACX,CAAC;AAED,cAAI,CAAC,kBAAkB;AACrB,oBAAQ,IAAIN,IAAG,OAAO,wCAA8B,CAAC;AACrD;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,oBAAgB,QAAQ,yCAAyC;AACjE,YAAQ,IAAIA,IAAG,MAAM,4CAAuC,CAAC;AAAA,EAE/D,UAAE;AAEA,QAAI,YAAY;AACd,YAAMO,yBAAwB,UAAU;AAAA,IAC1C;AAAA,EACF;AACF;AAEA,eAAeF,mBAAkB,QAA0C;AACzE,QAAM,mBAAmB,OAAO,wBAAwB,OAAO;AAE/D,MAAI,CAAC,kBAAkB;AACrB,UAAM,IAAI,MAAM,mHAAmH;AAAA,EACrI;AAEA,QAAM,SAAS,iBAAiB,MAAM,GAAG,EAAE,CAAC;AAE5C,UAAQ,QAAQ;AAAA,IACd,KAAK;AACH,YAAM,WAAW,IAAIJ,UAAS,EAAE,iBAAiB,CAAC;AAClD,YAAM,SAAS,QAAQ;AACvB,aAAO,EAAE,MAAM,cAAc,QAAQ,SAAS;AAAA,IAEhD,KAAK;AACH,YAAM,kBAAkB,MAAMC,OAAM,iBAAiB,gBAAgB;AACrE,aAAO,EAAE,MAAM,SAAS,QAAQ,gBAAgB;AAAA,IAElD,KAAK;AACH,YAAM,aAAa,iBAAiB,UAAU,UAAU,MAAM;AAC9D,YAAM,WAAW,IAAIC,UAAS,cAAc,eAAe;AAC3D,aAAO,EAAE,MAAM,UAAU,QAAQ,SAAS;AAAA,IAE5C;AACE,YAAM,IAAI,MAAM,8BAA8B,MAAM,EAAE;AAAA,EAC1D;AACF;AAEA,eAAe,qBAAqB,YAA6D;AAC/F,MAAI;AACF,UAAM,SAAS,MAAMK;AAAA,MACnB;AAAA,MACA;AAAA,IACF;AACA,WAAO;AAAA,EACT,SAAS,OAAO;AAEd,WAAO,CAAC;AAAA,EACV;AACF;AAEA,eAAe,8BACb,mBACA,SACA,QACA,aAC6B;AAC7B,MAAI,QAAQ,IAAI;AAEd,UAAM,cAAc,kBAAkB,UAAU,OAAK,EAAE,SAAS,QAAQ,EAAE;AAC1E,QAAI,gBAAgB,IAAI;AACtB,YAAM,IAAI,MAAM,cAAc,QAAQ,EAAE,mCAAmC;AAAA,IAC7E;AACA,WAAO,kBAAkB,MAAM,GAAG,WAAW;AAAA,EAC/C;AAEA,QAAM,QAAQ,QAAQ,SAAS;AAE/B,MAAI,SAAS,kBAAkB,QAAQ;AAErC,UAAM,aAAa,QAAQ,MAAM,OAAO,MAAMF,SAAQ;AAAA,MACpD,SAASN,IAAG,OAAO,wCAA8B,kBAAkB,MAAM,wBAAwB;AAAA,IACnG,CAAC;AAED,QAAI,CAAC,YAAY;AACf,aAAO,CAAC;AAAA,IACV;AAEA,WAAO;AAAA,EACT;AAEA,SAAO,kBAAkB,MAAM,GAAG,KAAK;AACzC;AAEA,eAAe,kBAAkB,WAA6B,QAAa,aAA6C;AACtH,QAAM,gBAAgB,OAAO,kBAAkB;AAC/C,QAAM,wBAAwBD,MAAK,QAAQ,aAAa,aAAa;AAGrE,QAAM,QAAQ,MAAMD,IAAG,QAAQ,qBAAqB;AAEpD,aAAW,YAAY,OAAO;AAC5B,QAAIC,MAAK,MAAM,QAAQ,EAAE,SAAS,UAAU,MAAM;AAChD,YAAM,UAAU,MAAMD,IAAG,SAASC,MAAK,KAAK,uBAAuB,QAAQ,GAAG,OAAO;AACrF,aAAO,gCAAgC,OAAO;AAAA,IAChD;AAAA,EACF;AAEA,SAAO;AACT;AAEA,eAAe,yBAAyB,YAAgC,aAAoC;AAC1G,QAAM,aAAa,YAAY,MAAM,GAAG,EAAE,OAAO,OAAK,EAAE,KAAK,MAAM,EAAE;AACrE,aAAW,aAAa,YAAY;AAClC,UAAMS,cAAa,YAAY,SAAS;AAAA,EAC1C;AACF;AAEA,eAAe,sBAAsB,YAAgC,WAA4C;AAC/G,QAAM,eAAe,MAAM;AACzB,YAAQ,WAAW,MAAM;AAAA,MACvB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,IACX;AAAA,EACF,GAAG;AAEH,QAAMA,cAAa,YAAY,aAAa,CAAC,UAAU,EAAE,CAAC;AAC5D;AAEA,eAAeA,cAAa,YAAgC,OAAe,QAAgC;AACzG,UAAQ,WAAW,MAAM;AAAA,IACvB,KAAK;AACH,YAAM,WAAW,MAAM,WAAW,OAAO,MAAM,OAAO,MAAM;AAC5D,aAAO,SAAS;AAAA,IAClB,KAAK;AACH,YAAM,CAAC,WAAW,IAAI,MAAM,WAAW,OAAO,QAAQ,OAAO,MAAM;AACnE,aAAO;AAAA,IACT,KAAK;AACH,YAAM,OAAO,WAAW,OAAO,QAAQ,KAAK;AAC5C,UAAI,MAAM,YAAY,EAAE,KAAK,EAAE,WAAW,QAAQ,GAAG;AACnD,eAAO,KAAK,IAAI,MAAM;AAAA,MACxB,OAAO;AACL,aAAK,IAAI,MAAM;AACf,eAAO,CAAC;AAAA,MACV;AAAA,IACF;AACE,YAAM,IAAI,MAAM,8BAA8B,WAAW,IAAI,EAAE;AAAA,EACnE;AACF;AAEA,eAAeD,yBAAwB,YAA+C;AACpF,UAAQ,WAAW,MAAM;AAAA,IACvB,KAAK;AACH,YAAM,WAAW,OAAO,IAAI;AAC5B;AAAA,IAEF,KAAK;AACH,YAAM,WAAW,OAAO,IAAI;AAC5B;AAAA,IAEF,KAAK;AACH,iBAAW,OAAO,MAAM;AACxB;AAAA,EACJ;AACF;AAEA,SAAS,gCAAgC,SAAyB;AAEhE,QAAM,eAAe;AAAA,IACnB;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,WAAW,cAAc;AAClC,UAAM,QAAQ,QAAQ,MAAM,OAAO;AACnC,QAAI,OAAO;AACT,YAAM,WAAW,MAAM,CAAC;AAGxB,YAAM,cAAc;AAAA,QAClB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,UAAI,eAAe;AACnB,iBAAW,cAAc,aAAa;AACpC,YAAI;AACJ,gBAAQ,WAAW,WAAW,KAAK,QAAQ,OAAO,MAAM;AACtD,0BAAgB,SAAS,CAAC,IAAI;AAAA,QAChC;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO;AACT;;;AnBxVA,IAAM,gBAAgB,OAAO,mBAAiC;AAC5D,MAAI;AACF,UAAM;AAAA,EACR,SAAS,OAAO;AACd,QAAI,SAAS,KAAK,GAAG;AACnB,aAAO,sBAAsB;AAC7B,cAAQ,KAAK,CAAC;AAAA,IAChB;AACA,IAAAE,KAAI,MAAM,iBAAiB,QAAQ,MAAM,UAAU,4BAA4B;AAC/E,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF;AAEA,IAAM,UAAU,IAAI,QAAQ;AAE5B,QACG,KAAK,MAAM,EACX,YAAY,4DAA4D,EACxE,QAAQ,SAAS,iBAAiB,4BAA4B,EAC9D,OAAO,eAAe,wBAAwB,EAC9C,OAAO,uBAAuB,4BAA4B,oBAAoB,EAC9E,OAAO,aAAa,2CAA2C;AAGlE,QACG,QAAQ,MAAM,EACd,YAAY,kDAAkD,EAC9D,OAAO,oBAAoB,+BAA+B,EAC1D,OAAO,qBAAqB,oCAAoC,aAAa,EAC7E,OAAO,kBAAkB,4BAA4B,EACrD,OAAO,4BAA4B,+BAA+B,EAClE,OAAO,aAAa,6DAA6D,EACjF,OAAO,OAAO,YAAY;AACzB,QAAM,qCAA8B;AACpC,QAAM,cAAc,YAAY,SAAS,QAAQ,KAAK,CAAQ,CAAC;AAC/D,QAAM,qDAAgD;AACxD,CAAC;AAGH,QACG,QAAQ,MAAM,EACd,YAAY,uDAAuD,EACnE,OAAO,WAAW,0CAA0C,EAC5D,OAAO,gBAAgB,gDAAgD,MAAM,EAC7E,OAAO,oBAAoB,+BAA+B,EAC1D,OAAO,aAAa,6DAA6D,EACjF,OAAO,OAAO,YAAY;AACzB,QAAM,+BAAwB;AAC9B,QAAM,cAAc,YAAY,SAAS,QAAQ,KAAK,CAAQ,CAAC;AAC/D,QAAM,uBAAkB;AAC1B,CAAC;AAGH,QACG,QAAQ,OAAO,EACf,YAAY,0CAA0C,EACtD,OAAO,sBAAsB,oCAAoC,EACjE,OAAO,mBAAmB,2DAA2D,EACrF,OAAO,oBAAoB,+BAA+B,EAC1D,OAAO,aAAa,6DAA6D,EACjF,OAAO,OAAO,YAAY;AACzB,QAAM,gCAAyB;AAC/B,QAAM,cAAc,aAAa,SAAS,QAAQ,KAAK,CAAQ,CAAC;AAChE,QAAM,wBAAmB;AAC3B,CAAC;AAGH,QACG,QAAQ,MAAM,EACd,YAAY,qCAAqC,EACjD,OAAO,eAAe,oCAAoC,GAAG,EAC7D,OAAO,eAAe,8CAA8C,EACpE,OAAO,oBAAoB,+BAA+B,EAC1D,OAAO,aAAa,6DAA6D,EACjF,OAAO,OAAO,YAAY;AACzB,QAAM,mCAA4B;AAElC,MAAI,QAAQ,OAAO;AACjB,YAAQ,QAAQ,SAAS,QAAQ,OAAO,EAAE;AAAA,EAC5C;AACA,QAAM,cAAc,YAAY,SAAS,QAAQ,KAAK,CAAQ,CAAC;AAC/D,QAAM,2BAAsB;AAC9B,CAAC;AAGH,QACG,QAAQ,MAAM,EACd,YAAY,0BAA0B,EACtC,OAAO,oBAAoB,+BAA+B,EAC1D,OAAO,WAAkB;AAG5B,QAAQ,MAAM;","names":["log","message","resolve","join","path","path","fs","join","warnings","path","fs","fs","resolve","spinner","pkg","confirm","fsExtra","resolve","dirname","fs","path","warnings","fs","path","spinner","detector","text","confirm","spinner","path","spinner","resolve","confirm","fs","path","pc","spinner","confirm","extractSQLFromMigrationFile","confirm","fs","path","pc","PgClient","mysql","Database","spinner","connectToDatabase","confirm","closeDatabaseConnection","executeQuery","log"]}